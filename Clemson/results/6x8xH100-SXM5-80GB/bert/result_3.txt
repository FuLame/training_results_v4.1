+ echo 'Beginning trial 04 of 10'
Beginning trial 04 of 10
+ echo ':::DLPAL /scratch/nnisbet/bert/bert_latest.sif 484644 6 nodeai[01-02,04-07]'
:::DLPAL /scratch/nnisbet/bert/bert_latest.sif 484644 6 nodeai[01-02,04-07]
+ '[' 1 -eq 1 ']'
+ srun --ntasks=6 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on nodeai05.palmetto.clemson.edu
Clearing cache on nodeai07.palmetto.clemson.edu
Clearing cache on nodeai01.palmetto.clemson.edu
Clearing cache on nodeai04.palmetto.clemson.edu
Clearing cache on nodeai06.palmetto.clemson.edu
Clearing cache on nodeai02.palmetto.clemson.edu
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=6 apptainer exec --nv /scratch/nnisbet/bert/bert_latest.sif python -c '
from mlperf_logger import mllogger
mllogger.event(key=mllogger.constants.CACHE_CLEAR, value=True)'
13:4: not a valid test operator: (
13:4: not a valid test operator: 550.54.15
13:4: not a valid test operator: (
13:4: not a valid test operator: 550.54.15
13:4: not a valid test operator: (
13:4: not a valid test operator: 550.54.15
13:4: not a valid test operator: (
13:4: not a valid test operator: 550.54.15
13:4: not a valid test operator: (
13:4: not a valid test operator: 550.54.15
13:4: not a valid test operator: (
13:4: not a valid test operator: 550.54.15
:::MLLOG {"namespace": "", "time_ms": 1723852636576, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1723852636794, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1723852636836, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1723852637589, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1723852637593, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1723852639058, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
++ hostname
+ echo 'Clearing cache on nodeai01.palmetto.clemson.edu'
Clearing cache on nodeai01.palmetto.clemson.edu
+ srun -l --cpus-per-task=13 --mpi=pmix --ntasks-per-node=8 --ntasks=48 apptainer exec --nv --writable-tmpfs -B /project/rcde/mlperf_data/bert/packed_data_hdf5:/workspace/data_phase2,/project/rcde/mlperf_data/bert/phase1/tf2_ckpt/:/workspace/phase1/,/project/rcde/mlperf_data/bert/hdf5/eval_varlength/:/workspace/evaldata,/project/rcde/nnisbet/palmetto-mlperf-benchmarks/Training/results/bert:/results -B /project/rcde/nnisbet/palmetto-mlperf-benchmarks/Training/bert/pytorch/run_pretraining.py:/workspace/bert/run_pretraining.py -B /project/rcde/nnisbet/palmetto-mlperf-benchmarks/Training/bert/pytorch/te_layers.py:/workspace/bert/te_layers.py -B /project/rcde/nnisbet/palmetto-mlperf-benchmarks/Training/bert/pytorch/run_and_time.sh:/workspace/bert/run_and_time.sh /scratch/nnisbet/bert/bert_latest.sif bash -c 'cd /workspace/bert/ ; bash slurm2pytorch ./run_and_time.sh'
32: 13:4: not a valid test operator: (
32: 13:4: not a valid test operator: 550.54.15
34: 13:4: not a valid test operator: (
34: 13:4: not a valid test operator: 550.54.15
38: 13:4: not a valid test operator: (
38: 13:4: not a valid test operator: 550.54.15
22: 13:4: not a valid test operator: (
22: 13:4: not a valid test operator: 550.54.15
33: 13:4: not a valid test operator: (
33: 13:4: not a valid test operator: 550.54.15
16: 13:4: not a valid test operator: (
16: 13:4: not a valid test operator: 550.54.15
23: 13:4: not a valid test operator: (
23: 13:4: not a valid test operator: 550.54.15
20: 13:4: not a valid test operator: (
20: 13:4: not a valid test operator: 550.54.15
36: 13:4: not a valid test operator: (
36: 13:4: not a valid test operator: 550.54.15
39: 13:4: not a valid test operator: (
39: 13:4: not a valid test operator: 550.54.15
19: 13:4: not a valid test operator: (
19: 13:4: not a valid test operator: 550.54.15
21: 13:4: not a valid test operator: (
21: 13:4: not a valid test operator: 550.54.15
17: 13:4: not a valid test operator: (
17: 13:4: not a valid test operator: 550.54.15
35: 13:4: not a valid test operator: (
35: 13:4: not a valid test operator: 550.54.15
37: 13:4: not a valid test operator: (
37: 13:4: not a valid test operator: 550.54.15
18: 13:4: not a valid test operator: (
18: 13:4: not a valid test operator: 550.54.15
15: 13:4: not a valid test operator: (
15: 13:4: not a valid test operator: 550.54.15
13: 13:4: not a valid test operator: (
13: 13:4: not a valid test operator: 550.54.15
11: 13:4: not a valid test operator: (
11: 13:4: not a valid test operator: 550.54.15
14: 13:4: not a valid test operator: (
14: 13:4: not a valid test operator: 550.54.15
12: 13:4: not a valid test operator: (
12: 13:4: not a valid test operator: 550.54.15
10: 13:4: not a valid test operator: (
10: 13:4: not a valid test operator: 550.54.15
 9: 13:4: not a valid test operator: (
 9: 13:4: not a valid test operator: 550.54.15
 8: 13:4: not a valid test operator: (
 8: 13:4: not a valid test operator: 550.54.15
27: 13:4: not a valid test operator: (
27: 13:4: not a valid test operator: 550.54.15
28: 13:4: not a valid test operator: (
28: 13:4: not a valid test operator: 550.54.15
29: 13:4: not a valid test operator: (
29: 13:4: not a valid test operator: 550.54.15
24: 13:4: not a valid test operator: (
24: 13:4: not a valid test operator: 550.54.15
26: 13:4: not a valid test operator: (
26: 13:4: not a valid test operator: 550.54.15
25: 13:4: not a valid test operator: (
25: 13:4: not a valid test operator: 550.54.15
30: 13:4: not a valid test operator: (
30: 13:4: not a valid test operator: 550.54.15
31: 13:4: not a valid test operator: (
31: 13:4: not a valid test operator: 550.54.15
40: 13:4: not a valid test operator: (
40: 13:4: not a valid test operator: 550.54.15
44: 13:4: not a valid test operator: (
44: 13:4: not a valid test operator: 550.54.15
47: 13:4: not a valid test operator: (
47: 13:4: not a valid test operator: 550.54.15
41: 13:4: not a valid test operator: (
41: 13:4: not a valid test operator: 550.54.15
46: 13:4: not a valid test operator: (
46: 13:4: not a valid test operator: 550.54.15
43: 13:4: not a valid test operator: (
43: 13:4: not a valid test operator: 550.54.15
42: 13:4: not a valid test operator: (
42: 13:4: not a valid test operator: 550.54.15
 0: 13:4: not a valid test operator: (
 0: 13:4: not a valid test operator: 550.54.15
 6: 13:4: not a valid test operator: (
 6: 13:4: not a valid test operator: 550.54.15
 4: 13:4: not a valid test operator: (
 4: 13:4: not a valid test operator: 550.54.15
 2: 13:4: not a valid test operator: (
 2: 13:4: not a valid test operator: 550.54.15
 7: 13:4: not a valid test operator: (
 7: 13:4: not a valid test operator: 550.54.15
 3: 13:4: not a valid test operator: (
 3: 13:4: not a valid test operator: 550.54.15
 1: 13:4: not a valid test operator: (
 1: 13:4: not a valid test operator: 550.54.15
45: 13:4: not a valid test operator: (
45: 13:4: not a valid test operator: 550.54.15
 5: 13:4: not a valid test operator: (
 5: 13:4: not a valid test operator: 550.54.15
40: Run vars: id 484644 gpus 8 mparams ''
41: Run vars: id 484644 gpus 8 mparams ''
42: Run vars: id 484644 gpus 8 mparams ''
43: Run vars: id 484644 gpus 8 mparams ''
45: Run vars: id 484644 gpus 8 mparams ''
46: Run vars: id 484644 gpus 8 mparams ''
47: Run vars: id 484644 gpus 8 mparams ''
44: Run vars: id 484644 gpus 8 mparams ''
42: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
44: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
46: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
45: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
43: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
40: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
41: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
47: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
11: Run vars: id 484644 gpus 8 mparams ''
14: Run vars: id 484644 gpus 8 mparams ''
15: Run vars: id 484644 gpus 8 mparams ''
13: Run vars: id 484644 gpus 8 mparams ''
 9: Run vars: id 484644 gpus 8 mparams ''
12: Run vars: id 484644 gpus 8 mparams ''
10: Run vars: id 484644 gpus 8 mparams ''
 8: Run vars: id 484644 gpus 8 mparams ''
 0: Run vars: id 484644 gpus 8 mparams ''
 2: Run vars: id 484644 gpus 8 mparams ''
 4: Run vars: id 484644 gpus 8 mparams ''
 5: Run vars: id 484644 gpus 8 mparams ''
 6: Run vars: id 484644 gpus 8 mparams ''
 1: Run vars: id 484644 gpus 8 mparams ''
 3: Run vars: id 484644 gpus 8 mparams ''
 7: Run vars: id 484644 gpus 8 mparams ''
14: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
11: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
15: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
13: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
10: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
12: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 8: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 9: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 0: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 6: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 2: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 1: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 5: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 4: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 7: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
 3: STARTING TIMING RUN AT 2024-08-16 07:57:23 PM
38: Run vars: id 484644 gpus 8 mparams ''
35: Run vars: id 484644 gpus 8 mparams ''
32: Run vars: id 484644 gpus 8 mparams ''
37: Run vars: id 484644 gpus 8 mparams ''
34: Run vars: id 484644 gpus 8 mparams ''
39: Run vars: id 484644 gpus 8 mparams ''
33: Run vars: id 484644 gpus 8 mparams ''
36: Run vars: id 484644 gpus 8 mparams ''
39: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
35: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
37: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
33: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
34: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
36: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
32: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
38: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
19: Run vars: id 484644 gpus 8 mparams ''
23: Run vars: id 484644 gpus 8 mparams ''
18: Run vars: id 484644 gpus 8 mparams ''
21: Run vars: id 484644 gpus 8 mparams ''
22: Run vars: id 484644 gpus 8 mparams ''
17: Run vars: id 484644 gpus 8 mparams ''
20: Run vars: id 484644 gpus 8 mparams ''
16: Run vars: id 484644 gpus 8 mparams ''
20: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
23: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
21: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
17: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
22: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
19: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
18: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
16: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
29: Run vars: id 484644 gpus 8 mparams ''
30: Run vars: id 484644 gpus 8 mparams ''
28: Run vars: id 484644 gpus 8 mparams ''
26: Run vars: id 484644 gpus 8 mparams ''
25: Run vars: id 484644 gpus 8 mparams ''
27: Run vars: id 484644 gpus 8 mparams ''
24: Run vars: id 484644 gpus 8 mparams ''
31: Run vars: id 484644 gpus 8 mparams ''
28: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
26: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
25: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
29: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
24: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
27: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
30: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
31: STARTING TIMING RUN AT 2024-08-16 07:57:24 PM
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=52
16: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
16:   warnings.warn(msg, DeprecatedFeatureWarning)
23: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
23:   warnings.warn(msg, DeprecatedFeatureWarning)
17: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
17:   warnings.warn(msg, DeprecatedFeatureWarning)
21: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
21:   warnings.warn(msg, DeprecatedFeatureWarning)
22: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
22:   warnings.warn(msg, DeprecatedFeatureWarning)
19: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
19:   warnings.warn(msg, DeprecatedFeatureWarning)
20: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
20:   warnings.warn(msg, DeprecatedFeatureWarning)
18: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
18:   warnings.warn(msg, DeprecatedFeatureWarning)
 3: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 3:   warnings.warn(msg, DeprecatedFeatureWarning)
32: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
32:   warnings.warn(msg, DeprecatedFeatureWarning)
 1: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 1:   warnings.warn(msg, DeprecatedFeatureWarning)
 7: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 7:   warnings.warn(msg, DeprecatedFeatureWarning)
35: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
35:   warnings.warn(msg, DeprecatedFeatureWarning)
39: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
39:   warnings.warn(msg, DeprecatedFeatureWarning)
47: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
47:   warnings.warn(msg, DeprecatedFeatureWarning)
 4: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 4:   warnings.warn(msg, DeprecatedFeatureWarning)
37: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
37:   warnings.warn(msg, DeprecatedFeatureWarning)
36: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
36:   warnings.warn(msg, DeprecatedFeatureWarning)
42: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
42:   warnings.warn(msg, DeprecatedFeatureWarning)
38: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
38:   warnings.warn(msg, DeprecatedFeatureWarning)
 2: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 2:   warnings.warn(msg, DeprecatedFeatureWarning)
 5: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 5:   warnings.warn(msg, DeprecatedFeatureWarning)
34: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
34:   warnings.warn(msg, DeprecatedFeatureWarning)
40: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
40:   warnings.warn(msg, DeprecatedFeatureWarning)
 0: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 0:   warnings.warn(msg, DeprecatedFeatureWarning)
33: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
33:   warnings.warn(msg, DeprecatedFeatureWarning)
 6: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 6:   warnings.warn(msg, DeprecatedFeatureWarning)
46: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
46:   warnings.warn(msg, DeprecatedFeatureWarning)
44: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
44:   warnings.warn(msg, DeprecatedFeatureWarning)
10: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
10:   warnings.warn(msg, DeprecatedFeatureWarning)
45: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
45:   warnings.warn(msg, DeprecatedFeatureWarning)
 9: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 9:   warnings.warn(msg, DeprecatedFeatureWarning)
41: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
41:   warnings.warn(msg, DeprecatedFeatureWarning)
15: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
15:   warnings.warn(msg, DeprecatedFeatureWarning)
 8: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
 8:   warnings.warn(msg, DeprecatedFeatureWarning)
43: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
43:   warnings.warn(msg, DeprecatedFeatureWarning)
11: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
11:   warnings.warn(msg, DeprecatedFeatureWarning)
14: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
14:   warnings.warn(msg, DeprecatedFeatureWarning)
13: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
13:   warnings.warn(msg, DeprecatedFeatureWarning)
12: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
12:   warnings.warn(msg, DeprecatedFeatureWarning)
26: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
26:   warnings.warn(msg, DeprecatedFeatureWarning)
31: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
31:   warnings.warn(msg, DeprecatedFeatureWarning)
29: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
29:   warnings.warn(msg, DeprecatedFeatureWarning)
25: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
25:   warnings.warn(msg, DeprecatedFeatureWarning)
24: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
24:   warnings.warn(msg, DeprecatedFeatureWarning)
30: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
30:   warnings.warn(msg, DeprecatedFeatureWarning)
28: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
28:   warnings.warn(msg, DeprecatedFeatureWarning)
27: /usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
27:   warnings.warn(msg, DeprecatedFeatureWarning)
 0: 2024-08-16 19:57:47.123167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 4: 2024-08-16 19:57:47.123163: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 5: 2024-08-16 19:57:47.123167: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 7: 2024-08-16 19:57:47.123158: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 1: 2024-08-16 19:57:47.123180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 2: 2024-08-16 19:57:47.123172: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 3: 2024-08-16 19:57:47.123218: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 6: 2024-08-16 19:57:47.123173: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
43: 2024-08-16 19:57:47.672506: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
45: 2024-08-16 19:57:47.672506: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
47: 2024-08-16 19:57:47.672511: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
40: 2024-08-16 19:57:47.672527: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
41: 2024-08-16 19:57:47.672512: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
42: 2024-08-16 19:57:47.672517: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
44: 2024-08-16 19:57:47.672523: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
46: 2024-08-16 19:57:47.672525: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
32: 2024-08-16 19:57:47.673095: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
33: 2024-08-16 19:57:47.673084: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
34: 2024-08-16 19:57:47.673107: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
35: 2024-08-16 19:57:47.673091: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
36: 2024-08-16 19:57:47.673086: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
37: 2024-08-16 19:57:47.673068: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
38: 2024-08-16 19:57:47.673113: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
39: 2024-08-16 19:57:47.673073: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 7: 2024-08-16 19:57:47.823271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 0: 2024-08-16 19:57:47.823326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 1: 2024-08-16 19:57:47.823310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 2: 2024-08-16 19:57:47.823330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 3: 2024-08-16 19:57:47.823310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 5: 2024-08-16 19:57:47.823291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 4: 2024-08-16 19:57:47.823347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 6: 2024-08-16 19:57:47.823342: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 8: 2024-08-16 19:57:48.013528: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
10: 2024-08-16 19:57:48.013520: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
14: 2024-08-16 19:57:48.013517: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 9: 2024-08-16 19:57:48.013580: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
11: 2024-08-16 19:57:48.013570: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
12: 2024-08-16 19:57:48.013541: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
13: 2024-08-16 19:57:48.013572: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
15: 2024-08-16 19:57:48.013573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
 1: 2024-08-16 19:57:48.101870: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 2: 2024-08-16 19:57:48.101881: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 0: 2024-08-16 19:57:48.101897: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 3: 2024-08-16 19:57:48.101899: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 4: 2024-08-16 19:57:48.101898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 5: 2024-08-16 19:57:48.101889: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 6: 2024-08-16 19:57:48.101918: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 7: 2024-08-16 19:57:48.101892: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 1: 2024-08-16 19:57:48.203497: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 3: 2024-08-16 19:57:48.203529: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 5: 2024-08-16 19:57:48.203492: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 7: 2024-08-16 19:57:48.203456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 0: 2024-08-16 19:57:48.203614: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 2: 2024-08-16 19:57:48.203602: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 4: 2024-08-16 19:57:48.203695: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 6: 2024-08-16 19:57:48.203678: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
16: 2024-08-16 19:57:48.223158: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
18: 2024-08-16 19:57:48.223165: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
22: 2024-08-16 19:57:48.223161: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
17: 2024-08-16 19:57:48.223219: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
19: 2024-08-16 19:57:48.223214: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
20: 2024-08-16 19:57:48.223176: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
23: 2024-08-16 19:57:48.223216: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
21: 2024-08-16 19:57:48.223246: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
43: 2024-08-16 19:57:48.431233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
45: 2024-08-16 19:57:48.431238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
47: 2024-08-16 19:57:48.431243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
40: 2024-08-16 19:57:48.431281: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
41: 2024-08-16 19:57:48.431268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
42: 2024-08-16 19:57:48.431255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
46: 2024-08-16 19:57:48.431290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
44: 2024-08-16 19:57:48.431310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
37: 2024-08-16 19:57:48.483693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
35: 2024-08-16 19:57:48.483701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
39: 2024-08-16 19:57:48.483738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
33: 2024-08-16 19:57:48.483747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
34: 2024-08-16 19:57:48.483773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
32: 2024-08-16 19:57:48.483822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
38: 2024-08-16 19:57:48.483842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
36: 2024-08-16 19:57:48.483865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
40: 2024-08-16 19:57:48.628318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
41: 2024-08-16 19:57:48.628299: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
42: 2024-08-16 19:57:48.628313: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
43: 2024-08-16 19:57:48.628292: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
44: 2024-08-16 19:57:48.628326: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
45: 2024-08-16 19:57:48.628263: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
46: 2024-08-16 19:57:48.628326: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
47: 2024-08-16 19:57:48.628285: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
40: 2024-08-16 19:57:48.675418: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
41: 2024-08-16 19:57:48.675442: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
42: 2024-08-16 19:57:48.675449: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
43: 2024-08-16 19:57:48.675450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
44: 2024-08-16 19:57:48.675442: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
45: 2024-08-16 19:57:48.675436: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
46: 2024-08-16 19:57:48.675438: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
47: 2024-08-16 19:57:48.675441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
32: 2024-08-16 19:57:48.687155: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
33: 2024-08-16 19:57:48.687143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
34: 2024-08-16 19:57:48.687156: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
35: 2024-08-16 19:57:48.687107: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
36: 2024-08-16 19:57:48.687181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
37: 2024-08-16 19:57:48.687134: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
38: 2024-08-16 19:57:48.687173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
39: 2024-08-16 19:57:48.687120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
32: 2024-08-16 19:57:48.737546: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
33: 2024-08-16 19:57:48.737510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
34: 2024-08-16 19:57:48.737568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
35: 2024-08-16 19:57:48.737486: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
36: 2024-08-16 19:57:48.737559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
37: 2024-08-16 19:57:48.737500: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
38: 2024-08-16 19:57:48.737555: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
39: 2024-08-16 19:57:48.737495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 5: 2024-08-16 19:57:48.787540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 5: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 7: 2024-08-16 19:57:48.787544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 7: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 1: 2024-08-16 19:57:48.787673: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 1: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 3: 2024-08-16 19:57:48.787687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 3: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 0: 2024-08-16 19:57:48.788309: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 0: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 2: 2024-08-16 19:57:48.788304: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 2: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 6: 2024-08-16 19:57:48.788327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 6: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 4: 2024-08-16 19:57:48.788374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 4: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 8: 2024-08-16 19:57:48.807239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
10: 2024-08-16 19:57:48.807244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 9: 2024-08-16 19:57:48.807302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
11: 2024-08-16 19:57:48.807302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
12: 2024-08-16 19:57:48.807306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
13: 2024-08-16 19:57:48.807297: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
14: 2024-08-16 19:57:48.807262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
15: 2024-08-16 19:57:48.807304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
 8: 2024-08-16 19:57:49.077502: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
 9: 2024-08-16 19:57:49.077484: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
10: 2024-08-16 19:57:49.077488: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
11: 2024-08-16 19:57:49.077468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
12: 2024-08-16 19:57:49.077499: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
13: 2024-08-16 19:57:49.077467: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
14: 2024-08-16 19:57:49.077518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
15: 2024-08-16 19:57:49.077463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
15: 2024-08-16 19:57:49.125293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 9: 2024-08-16 19:57:49.125339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
10: 2024-08-16 19:57:49.125324: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
11: 2024-08-16 19:57:49.125338: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
13: 2024-08-16 19:57:49.125337: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
 8: 2024-08-16 19:57:49.125360: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
12: 2024-08-16 19:57:49.125346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
14: 2024-08-16 19:57:49.125347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
23: 2024-08-16 19:57:49.132523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
19: 2024-08-16 19:57:49.132585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
21: 2024-08-16 19:57:49.132730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
22: 2024-08-16 19:57:49.132754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
16: 2024-08-16 19:57:49.132767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
18: 2024-08-16 19:57:49.132775: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
17: 2024-08-16 19:57:49.132825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
20: 2024-08-16 19:57:49.132824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
40: 2024-08-16 19:57:49.145925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
40: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
42: 2024-08-16 19:57:49.145889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
42: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
46: 2024-08-16 19:57:49.145972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
46: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
44: 2024-08-16 19:57:49.146025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
44: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
47: 2024-08-16 19:57:49.146217: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
47: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
45: 2024-08-16 19:57:49.146247: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
45: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
41: 2024-08-16 19:57:49.146307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
41: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
43: 2024-08-16 19:57:49.146298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
43: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
16: 2024-08-16 19:57:49.372172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
17: 2024-08-16 19:57:49.372230: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
18: 2024-08-16 19:57:49.372237: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
21: 2024-08-16 19:57:49.372223: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
22: 2024-08-16 19:57:49.372201: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
23: 2024-08-16 19:57:49.372239: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
19: 2024-08-16 19:57:49.372234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
20: 2024-08-16 19:57:49.372234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
16: 2024-08-16 19:57:49.432216: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
17: 2024-08-16 19:57:49.432244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
18: 2024-08-16 19:57:49.432249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
20: 2024-08-16 19:57:49.432236: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
22: 2024-08-16 19:57:49.432243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
23: 2024-08-16 19:57:49.432230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
19: 2024-08-16 19:57:49.432254: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
21: 2024-08-16 19:57:49.432310: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
39: 2024-08-16 19:57:49.506875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
39: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
37: 2024-08-16 19:57:49.506904: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
37: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
33: 2024-08-16 19:57:49.506959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
33: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
35: 2024-08-16 19:57:49.506954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
35: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
34: 2024-08-16 19:57:49.507163: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
34: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
32: 2024-08-16 19:57:49.507185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
32: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
36: 2024-08-16 19:57:49.507200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
36: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
38: 2024-08-16 19:57:49.507216: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
38: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 8: 2024-08-16 19:57:49.607896: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 8: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
10: 2024-08-16 19:57:49.607942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
10: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
12: 2024-08-16 19:57:49.608015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
12: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
13: 2024-08-16 19:57:49.608215: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
13: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
15: 2024-08-16 19:57:49.608224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
15: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
14: 2024-08-16 19:57:49.608286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
14: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
 9: 2024-08-16 19:57:49.608310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
 9: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
11: 2024-08-16 19:57:49.608312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
11: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
24: 2024-08-16 19:57:49.732442: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
25: 2024-08-16 19:57:49.732458: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
26: 2024-08-16 19:57:49.732443: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
27: 2024-08-16 19:57:49.732459: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
28: 2024-08-16 19:57:49.732454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
29: 2024-08-16 19:57:49.732445: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
30: 2024-08-16 19:57:49.732446: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
31: 2024-08-16 19:57:49.732457: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
23: 2024-08-16 19:57:50.091297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
23: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
16: 2024-08-16 19:57:50.091323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
16: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
18: 2024-08-16 19:57:50.091322: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
18: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
21: 2024-08-16 19:57:50.091328: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
21: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
19: 2024-08-16 19:57:50.091385: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
19: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
20: 2024-08-16 19:57:50.091370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
20: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
22: 2024-08-16 19:57:50.091400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
22: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
17: 2024-08-16 19:57:50.091454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
17: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
29: 2024-08-16 19:57:50.822149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
31: 2024-08-16 19:57:50.822169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
27: 2024-08-16 19:57:50.822228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
25: 2024-08-16 19:57:50.822270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
26: 2024-08-16 19:57:50.822337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
30: 2024-08-16 19:57:50.822466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
28: 2024-08-16 19:57:50.822495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
24: 2024-08-16 19:57:50.823042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
26: 2024-08-16 19:57:51.023981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
29: 2024-08-16 19:57:51.023980: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
24: 2024-08-16 19:57:51.023985: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
25: 2024-08-16 19:57:51.024011: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
27: 2024-08-16 19:57:51.023991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
31: 2024-08-16 19:57:51.023995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
30: 2024-08-16 19:57:51.024159: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
28: 2024-08-16 19:57:51.024168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
25: 2024-08-16 19:57:51.095813: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
30: 2024-08-16 19:57:51.095870: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
24: 2024-08-16 19:57:51.095940: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
27: 2024-08-16 19:57:51.095959: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
28: 2024-08-16 19:57:51.096020: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
31: 2024-08-16 19:57:51.096068: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
26: 2024-08-16 19:57:51.096162: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
29: 2024-08-16 19:57:51.096261: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
29: 2024-08-16 19:57:51.850847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
29: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
25: 2024-08-16 19:57:51.850870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
25: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
28: 2024-08-16 19:57:51.850978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
28: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
30: 2024-08-16 19:57:51.850972: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
30: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
24: 2024-08-16 19:57:51.851023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
24: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
26: 2024-08-16 19:57:51.851330: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
26: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
27: 2024-08-16 19:57:51.851413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
27: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
31: 2024-08-16 19:57:51.851474: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
31: To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
45: :::MLLOG {"namespace": "", "time_ms": 1723852673749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
40: :::MLLOG {"namespace": "", "time_ms": 1723852673749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
41: :::MLLOG {"namespace": "", "time_ms": 1723852673751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
42: :::MLLOG {"namespace": "", "time_ms": 1723852673749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
43: :::MLLOG {"namespace": "", "time_ms": 1723852673753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
46: :::MLLOG {"namespace": "", "time_ms": 1723852673753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
44: :::MLLOG {"namespace": "", "time_ms": 1723852673750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
47: :::MLLOG {"namespace": "", "time_ms": 1723852673749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 2: :::MLLOG {"namespace": "", "time_ms": 1723852674218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 4: :::MLLOG {"namespace": "", "time_ms": 1723852674223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 6: :::MLLOG {"namespace": "", "time_ms": 1723852674224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852674219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 1: :::MLLOG {"namespace": "", "time_ms": 1723852674221, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 3: :::MLLOG {"namespace": "", "time_ms": 1723852674222, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 5: :::MLLOG {"namespace": "", "time_ms": 1723852674218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 7: :::MLLOG {"namespace": "", "time_ms": 1723852674219, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
10: :::MLLOG {"namespace": "", "time_ms": 1723852674477, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 8: :::MLLOG {"namespace": "", "time_ms": 1723852674477, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
13: :::MLLOG {"namespace": "", "time_ms": 1723852674478, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
15: :::MLLOG {"namespace": "", "time_ms": 1723852674479, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
14: :::MLLOG {"namespace": "", "time_ms": 1723852674480, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
 8: device: cuda:0 n_gpu: 48, distributed training: True, 16-bits training: True
 9: :::MLLOG {"namespace": "", "time_ms": 1723852674482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
12: :::MLLOG {"namespace": "", "time_ms": 1723852674482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
11: :::MLLOG {"namespace": "", "time_ms": 1723852674482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
34: :::MLLOG {"namespace": "", "time_ms": 1723852674666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
33: :::MLLOG {"namespace": "", "time_ms": 1723852674670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
35: :::MLLOG {"namespace": "", "time_ms": 1723852674669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
36: :::MLLOG {"namespace": "", "time_ms": 1723852674670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
37: :::MLLOG {"namespace": "", "time_ms": 1723852674667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
38: :::MLLOG {"namespace": "", "time_ms": 1723852674668, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
39: :::MLLOG {"namespace": "", "time_ms": 1723852674666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
32: :::MLLOG {"namespace": "", "time_ms": 1723852674666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
32: device: cuda:0 n_gpu: 48, distributed training: True, 16-bits training: True
40: device: cuda:0 n_gpu: 48, distributed training: True, 16-bits training: True
16: :::MLLOG {"namespace": "", "time_ms": 1723852675233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
17: :::MLLOG {"namespace": "", "time_ms": 1723852675235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
19: :::MLLOG {"namespace": "", "time_ms": 1723852675236, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
18: :::MLLOG {"namespace": "", "time_ms": 1723852675233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
21: :::MLLOG {"namespace": "", "time_ms": 1723852675233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
23: :::MLLOG {"namespace": "", "time_ms": 1723852675233, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
20: :::MLLOG {"namespace": "", "time_ms": 1723852675235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
22: :::MLLOG {"namespace": "", "time_ms": 1723852675234, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
16: device: cuda:0 n_gpu: 48, distributed training: True, 16-bits training: True
46: device: cuda:6 n_gpu: 48, distributed training: True, 16-bits training: True
42: device: cuda:2 n_gpu: 48, distributed training: True, 16-bits training: True
44: device: cuda:4 n_gpu: 48, distributed training: True, 16-bits training: True
43: device: cuda:3 n_gpu: 48, distributed training: True, 16-bits training: True
47: device: cuda:7 n_gpu: 48, distributed training: True, 16-bits training: True
45: device: cuda:5 n_gpu: 48, distributed training: True, 16-bits training: True
41: device: cuda:1 n_gpu: 48, distributed training: True, 16-bits training: True
 1: device: cuda:1 n_gpu: 48, distributed training: True, 16-bits training: True
 5: device: cuda:5 n_gpu: 48, distributed training: True, 16-bits training: True
 3: device: cuda:3 n_gpu: 48, distributed training: True, 16-bits training: True
 4: device: cuda:4 n_gpu: 48, distributed training: True, 16-bits training: True
43: using fp8 FMHA
44: using fp8 FMHA
46: using fp8 FMHA
47: using fp8 FMHA
42: using fp8 FMHA
 7: device: cuda:7 n_gpu: 48, distributed training: True, 16-bits training: True
45: using fp8 FMHA
 6: device: cuda:6 n_gpu: 48, distributed training: True, 16-bits training: True
 2: device: cuda:2 n_gpu: 48, distributed training: True, 16-bits training: True
41: using fp8 FMHA
40: using fp8 FMHA
 3: using fp8 FMHA
 5: using fp8 FMHA
 1: using fp8 FMHA
 4: using fp8 FMHA
 7: using fp8 FMHA
 6: using fp8 FMHA
 2: using fp8 FMHA
 9: device: cuda:1 n_gpu: 48, distributed training: True, 16-bits training: True
14: device: cuda:6 n_gpu: 48, distributed training: True, 16-bits training: True
10: device: cuda:2 n_gpu: 48, distributed training: True, 16-bits training: True
15: device: cuda:7 n_gpu: 48, distributed training: True, 16-bits training: True
13: device: cuda:5 n_gpu: 48, distributed training: True, 16-bits training: True
11: device: cuda:3 n_gpu: 48, distributed training: True, 16-bits training: True
12: device: cuda:4 n_gpu: 48, distributed training: True, 16-bits training: True
35: device: cuda:3 n_gpu: 48, distributed training: True, 16-bits training: True
26: :::MLLOG {"namespace": "", "time_ms": 1723852676707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
24: :::MLLOG {"namespace": "", "time_ms": 1723852676707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
25: :::MLLOG {"namespace": "", "time_ms": 1723852676713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
27: :::MLLOG {"namespace": "", "time_ms": 1723852676712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
28: :::MLLOG {"namespace": "", "time_ms": 1723852676712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
29: :::MLLOG {"namespace": "", "time_ms": 1723852676708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
31: :::MLLOG {"namespace": "", "time_ms": 1723852676707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
30: :::MLLOG {"namespace": "", "time_ms": 1723852676714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1329}}
24: device: cuda:0 n_gpu: 48, distributed training: True, 16-bits training: True
37: device: cuda:5 n_gpu: 48, distributed training: True, 16-bits training: True
39: device: cuda:7 n_gpu: 48, distributed training: True, 16-bits training: True
36: device: cuda:4 n_gpu: 48, distributed training: True, 16-bits training: True
33: device: cuda:1 n_gpu: 48, distributed training: True, 16-bits training: True
38: device: cuda:6 n_gpu: 48, distributed training: True, 16-bits training: True
 8: using fp8 FMHA
 9: using fp8 FMHA
10: using fp8 FMHA
14: using fp8 FMHA
15: using fp8 FMHA
34: device: cuda:2 n_gpu: 48, distributed training: True, 16-bits training: True
11: using fp8 FMHA
13: using fp8 FMHA
12: using fp8 FMHA
32: using fp8 FMHA
37: using fp8 FMHA
39: using fp8 FMHA
36: using fp8 FMHA
35: using fp8 FMHA
38: using fp8 FMHA
33: using fp8 FMHA
34: using fp8 FMHA
19: device: cuda:3 n_gpu: 48, distributed training: True, 16-bits training: True
17: device: cuda:1 n_gpu: 48, distributed training: True, 16-bits training: True
23: device: cuda:7 n_gpu: 48, distributed training: True, 16-bits training: True
18: device: cuda:2 n_gpu: 48, distributed training: True, 16-bits training: True
20: device: cuda:4 n_gpu: 48, distributed training: True, 16-bits training: True
21: device: cuda:5 n_gpu: 48, distributed training: True, 16-bits training: True
22: device: cuda:6 n_gpu: 48, distributed training: True, 16-bits training: True
17: using fp8 FMHA
23: using fp8 FMHA
19: using fp8 FMHA
21: using fp8 FMHA
18: using fp8 FMHA
22: using fp8 FMHA
20: using fp8 FMHA
16: using fp8 FMHA
31: device: cuda:7 n_gpu: 48, distributed training: True, 16-bits training: True
25: device: cuda:1 n_gpu: 48, distributed training: True, 16-bits training: True
30: device: cuda:6 n_gpu: 48, distributed training: True, 16-bits training: True
29: device: cuda:5 n_gpu: 48, distributed training: True, 16-bits training: True
26: device: cuda:2 n_gpu: 48, distributed training: True, 16-bits training: True
28: device: cuda:4 n_gpu: 48, distributed training: True, 16-bits training: True
27: device: cuda:3 n_gpu: 48, distributed training: True, 16-bits training: True
 0: device: cuda:0 n_gpu: 48, distributed training: True, 16-bits training: True
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678797, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678797, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678797, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678797, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678797, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "6xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1336}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678797, "event_type": "POINT_IN_TIME", "key": "seed", "value": 25011, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1338}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678797, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 6912, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1340}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678798, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 72, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1342}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678798, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1344}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678798, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1346}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678798, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 700.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1348}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852678798, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1350}}
 0: parsed args:
 0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=200000, eval_iter_samples=200000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=72, eval_batch_size=16, learning_rate=0.00258, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=700.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=25011, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent
 0: _checkpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_a
 0: g_pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=48, device=device(type='cuda', index=0))
25: using fp8 FMHA
29: using fp8 FMHA
30: using fp8 FMHA
31: using fp8 FMHA
24: using fp8 FMHA
28: using fp8 FMHA
26: using fp8 FMHA
27: using fp8 FMHA
 0: using fp8 FMHA
32: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
32:   self._overflow_buf = torch.cuda.IntTensor([0])
33: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
33:   self._overflow_buf = torch.cuda.IntTensor([0])
34: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
34:   self._overflow_buf = torch.cuda.IntTensor([0])
35: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
35:   self._overflow_buf = torch.cuda.IntTensor([0])
36: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
36:   self._overflow_buf = torch.cuda.IntTensor([0])
37: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
37:   self._overflow_buf = torch.cuda.IntTensor([0])
39: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
39:   self._overflow_buf = torch.cuda.IntTensor([0])
38: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
38:   self._overflow_buf = torch.cuda.IntTensor([0])
40: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
40:   self._overflow_buf = torch.cuda.IntTensor([0])
41: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
41:   self._overflow_buf = torch.cuda.IntTensor([0])
42: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
42:   self._overflow_buf = torch.cuda.IntTensor([0])
43: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
43:   self._overflow_buf = torch.cuda.IntTensor([0])
44: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
44:   self._overflow_buf = torch.cuda.IntTensor([0])
46: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
46:   self._overflow_buf = torch.cuda.IntTensor([0])
47: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
47:   self._overflow_buf = torch.cuda.IntTensor([0])
45: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
45:   self._overflow_buf = torch.cuda.IntTensor([0])
 8: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 8:   self._overflow_buf = torch.cuda.IntTensor([0])
 9: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 9:   self._overflow_buf = torch.cuda.IntTensor([0])
10: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
10:   self._overflow_buf = torch.cuda.IntTensor([0])
11: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
11:   self._overflow_buf = torch.cuda.IntTensor([0])
13: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
13:   self._overflow_buf = torch.cuda.IntTensor([0])
15: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
15:   self._overflow_buf = torch.cuda.IntTensor([0])
14: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
14:   self._overflow_buf = torch.cuda.IntTensor([0])
12: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
12:   self._overflow_buf = torch.cuda.IntTensor([0])
16: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
16:   self._overflow_buf = torch.cuda.IntTensor([0])
18: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
18:   self._overflow_buf = torch.cuda.IntTensor([0])
19: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
19:   self._overflow_buf = torch.cuda.IntTensor([0])
23: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
23:   self._overflow_buf = torch.cuda.IntTensor([0])
17: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
17:   self._overflow_buf = torch.cuda.IntTensor([0])
22: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
22:   self._overflow_buf = torch.cuda.IntTensor([0])
20: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
20:   self._overflow_buf = torch.cuda.IntTensor([0])
21: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
21:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682273, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/embeddings/word_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682273, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/embeddings/position_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682273, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/embeddings/token_type_embeddings"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682273, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/embeddings/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/embeddings/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_0/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682274, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_1/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_2/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682275, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_3/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682276, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_4/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682277, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_5/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_6/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682278, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_7/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682279, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_8/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682280, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_9/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_10/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682281, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_11/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682282, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_12/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682283, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_13/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682284, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_14/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_15/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682285, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_16/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682286, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_17/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_18/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682287, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_19/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682288, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_20/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_21/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682289, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_22/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/self/query/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/self/query/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/self/key/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/self/key/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/self/value/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/self/value/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/attention/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/intermediate/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/intermediate/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/output/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/output/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/output/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/encoder/layer_23/output/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/pooler/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "bert/pooler/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682290, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "cls/predictions/output_bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682291, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "cls/predictions/transform/dense/kernel"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682291, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "cls/predictions/transform/dense/bias"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682291, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "cls/predictions/transform/LayerNorm/gamma"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682291, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "cls/predictions/transform/LayerNorm/beta"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682291, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "cls/seq_relationship/output_weights"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682291, "event_type": "POINT_IN_TIME", "key": "weights_initialization", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 946, "tensor": "cls/seq_relationship/output_bias"}}
 4: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 4:   self._overflow_buf = torch.cuda.IntTensor([0])
 5: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 5:   self._overflow_buf = torch.cuda.IntTensor([0])
 7: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 7:   self._overflow_buf = torch.cuda.IntTensor([0])
 6: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 6:   self._overflow_buf = torch.cuda.IntTensor([0])
 1: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 1:   self._overflow_buf = torch.cuda.IntTensor([0])
 3: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 3:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: :::MLLOG {"namespace": "", "time_ms": 1723852682511, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00258, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 976}}
 2: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 2:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
 0:   self._overflow_buf = torch.cuda.IntTensor([0])
 0: nodeai01:2157660:2157660 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 0: nodeai01:2157660:2157660 [0] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
 0: nodeai01:2157660:2157660 [0] NCCL INFO cudaDriverVersion 12040
 0: NCCL version 2.18.5+cuda12.2
12: nodeai02:2459394:2459394 [4] NCCL INFO cudaDriverVersion 12040
 6: nodeai01:2157661:2157661 [6] NCCL INFO cudaDriverVersion 12040
12: nodeai02:2459394:2459394 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
12: nodeai02:2459394:2459394 [4] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
 6: nodeai01:2157661:2157661 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 6: nodeai01:2157661:2157661 [6] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
11: nodeai02:2459390:2459390 [3] NCCL INFO cudaDriverVersion 12040
11: nodeai02:2459390:2459390 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
11: nodeai02:2459390:2459390 [3] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
10: nodeai02:2459393:2459393 [2] NCCL INFO cudaDriverVersion 12040
10: nodeai02:2459393:2459393 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
10: nodeai02:2459393:2459393 [2] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
 8: nodeai02:2459395:2459395 [0] NCCL INFO cudaDriverVersion 12040
 8: nodeai02:2459395:2459395 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 8: nodeai02:2459395:2459395 [0] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
40: nodeai07:2435734:2435734 [0] NCCL INFO cudaDriverVersion 12040
 4: nodeai01:2157666:2157666 [4] NCCL INFO cudaDriverVersion 12040
 4: nodeai01:2157666:2157666 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 4: nodeai01:2157666:2157666 [4] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
 5: nodeai01:2157665:2157665 [5] NCCL INFO cudaDriverVersion 12040
 5: nodeai01:2157665:2157665 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 5: nodeai01:2157665:2157665 [5] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
40: nodeai07:2435734:2435734 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
40: nodeai07:2435734:2435734 [0] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
18: nodeai04:2417512:2417512 [2] NCCL INFO cudaDriverVersion 12040
17: nodeai04:2417509:2417509 [1] NCCL INFO cudaDriverVersion 12040
18: nodeai04:2417512:2417512 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
17: nodeai04:2417509:2417509 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
18: nodeai04:2417512:2417512 [2] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
17: nodeai04:2417509:2417509 [1] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
19: nodeai04:2417510:2417510 [3] NCCL INFO cudaDriverVersion 12040
19: nodeai04:2417510:2417510 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
19: nodeai04:2417510:2417510 [3] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
 9: nodeai02:2459396:2459396 [1] NCCL INFO cudaDriverVersion 12040
 7: nodeai01:2157664:2157664 [7] NCCL INFO cudaDriverVersion 12040
 9: nodeai02:2459396:2459396 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 9: nodeai02:2459396:2459396 [1] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
15: nodeai02:2459391:2459391 [7] NCCL INFO cudaDriverVersion 12040
 7: nodeai01:2157664:2157664 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
15: nodeai02:2459391:2459391 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 7: nodeai01:2157664:2157664 [7] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
15: nodeai02:2459391:2459391 [7] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
14: nodeai02:2459389:2459389 [6] NCCL INFO cudaDriverVersion 12040
13: nodeai02:2459392:2459392 [5] NCCL INFO cudaDriverVersion 12040
14: nodeai02:2459389:2459389 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 1: nodeai01:2157663:2157663 [1] NCCL INFO cudaDriverVersion 12040
14: nodeai02:2459389:2459389 [6] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
 1: nodeai01:2157663:2157663 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
13: nodeai02:2459392:2459392 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 1: nodeai01:2157663:2157663 [1] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
13: nodeai02:2459392:2459392 [5] NCCL INFO Bootstrap : Using ib0:10.128.200.19<0>
20: nodeai04:2417506:2417506 [4] NCCL INFO cudaDriverVersion 12040
20: nodeai04:2417506:2417506 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
21: nodeai04:2417508:2417508 [5] NCCL INFO cudaDriverVersion 12040
20: nodeai04:2417506:2417506 [4] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
21: nodeai04:2417508:2417508 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
21: nodeai04:2417508:2417508 [5] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
22: nodeai04:2417511:2417511 [6] NCCL INFO cudaDriverVersion 12040
 2: nodeai01:2157662:2157662 [2] NCCL INFO cudaDriverVersion 12040
22: nodeai04:2417511:2417511 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 2: nodeai01:2157662:2157662 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
22: nodeai04:2417511:2417511 [6] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
 2: nodeai01:2157662:2157662 [2] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
38: nodeai06:2504437:2504437 [6] NCCL INFO cudaDriverVersion 12040
23: nodeai04:2417507:2417507 [7] NCCL INFO cudaDriverVersion 12040
23: nodeai04:2417507:2417507 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
34: nodeai06:2504435:2504435 [2] NCCL INFO cudaDriverVersion 12040
38: nodeai06:2504437:2504437 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
38: nodeai06:2504437:2504437 [6] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
16: nodeai04:2417513:2417513 [0] NCCL INFO cudaDriverVersion 12040
23: nodeai04:2417507:2417507 [7] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
47: nodeai07:2435736:2435736 [7] NCCL INFO cudaDriverVersion 12040
34: nodeai06:2504435:2504435 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
47: nodeai07:2435736:2435736 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
16: nodeai04:2417513:2417513 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
16: nodeai04:2417513:2417513 [0] NCCL INFO Bootstrap : Using ib0:10.128.200.35<0>
34: nodeai06:2504435:2504435 [2] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
47: nodeai07:2435736:2435736 [7] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
41: nodeai07:2435735:2435735 [1] NCCL INFO cudaDriverVersion 12040
41: nodeai07:2435735:2435735 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
41: nodeai07:2435735:2435735 [1] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
 3: nodeai01:2157667:2157667 [3] NCCL INFO cudaDriverVersion 12040
 3: nodeai01:2157667:2157667 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 3: nodeai01:2157667:2157667 [3] NCCL INFO Bootstrap : Using ib0:10.128.200.11<0>
33: nodeai06:2504433:2504433 [1] NCCL INFO cudaDriverVersion 12040
33: nodeai06:2504433:2504433 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
33: nodeai06:2504433:2504433 [1] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
35: nodeai06:2504431:2504431 [3] NCCL INFO cudaDriverVersion 12040
35: nodeai06:2504431:2504431 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
35: nodeai06:2504431:2504431 [3] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
46: nodeai07:2435731:2435731 [6] NCCL INFO cudaDriverVersion 12040
46: nodeai07:2435731:2435731 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
46: nodeai07:2435731:2435731 [6] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
42: nodeai07:2435730:2435730 [2] NCCL INFO cudaDriverVersion 12040
42: nodeai07:2435730:2435730 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
42: nodeai07:2435730:2435730 [2] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
36: nodeai06:2504434:2504434 [4] NCCL INFO cudaDriverVersion 12040
39: nodeai06:2504430:2504430 [7] NCCL INFO cudaDriverVersion 12040
43: nodeai07:2435733:2435733 [3] NCCL INFO cudaDriverVersion 12040
36: nodeai06:2504434:2504434 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
39: nodeai06:2504430:2504430 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
43: nodeai07:2435733:2435733 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
36: nodeai06:2504434:2504434 [4] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
39: nodeai06:2504430:2504430 [7] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
43: nodeai07:2435733:2435733 [3] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
44: nodeai07:2435729:2435729 [4] NCCL INFO cudaDriverVersion 12040
44: nodeai07:2435729:2435729 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
44: nodeai07:2435729:2435729 [4] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
45: nodeai07:2435732:2435732 [5] NCCL INFO cudaDriverVersion 12040
45: nodeai07:2435732:2435732 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
45: nodeai07:2435732:2435732 [5] NCCL INFO Bootstrap : Using ib0:10.128.200.59<0>
37: nodeai06:2504432:2504432 [5] NCCL INFO cudaDriverVersion 12040
37: nodeai06:2504432:2504432 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
37: nodeai06:2504432:2504432 [5] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
32: nodeai06:2504436:2504436 [0] NCCL INFO cudaDriverVersion 12040
32: nodeai06:2504436:2504436 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
32: nodeai06:2504436:2504436 [0] NCCL INFO Bootstrap : Using ib0:10.128.200.51<0>
40: nodeai07:2435734:2436919 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
40: nodeai07:2435734:2436919 [0] NCCL INFO P2P plugin IBext
40: nodeai07:2435734:2436919 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
41: nodeai07:2435735:2436921 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
41: nodeai07:2435735:2436921 [1] NCCL INFO P2P plugin IBext
41: nodeai07:2435735:2436921 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 5: nodeai01:2157665:2158846 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 5: nodeai01:2157665:2158846 [5] NCCL INFO P2P plugin IBext
 5: nodeai01:2157665:2158846 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
42: nodeai07:2435730:2436923 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
42: nodeai07:2435730:2436923 [2] NCCL INFO P2P plugin IBext
42: nodeai07:2435730:2436923 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 0: nodeai01:2157660:2158843 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 0: nodeai01:2157660:2158843 [0] NCCL INFO P2P plugin IBext
 0: nodeai01:2157660:2158843 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
45: nodeai07:2435732:2436926 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
45: nodeai07:2435732:2436926 [5] NCCL INFO P2P plugin IBext
45: nodeai07:2435732:2436926 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
47: nodeai07:2435736:2436920 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
47: nodeai07:2435736:2436920 [7] NCCL INFO P2P plugin IBext
47: nodeai07:2435736:2436920 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 7: nodeai01:2157664:2158847 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 7: nodeai01:2157664:2158847 [7] NCCL INFO P2P plugin IBext
 7: nodeai01:2157664:2158847 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 6: nodeai01:2157661:2158844 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 6: nodeai01:2157661:2158844 [6] NCCL INFO P2P plugin IBext
 6: nodeai01:2157661:2158844 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
43: nodeai07:2435733:2436924 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
43: nodeai07:2435733:2436924 [3] NCCL INFO P2P plugin IBext
43: nodeai07:2435733:2436924 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
46: nodeai07:2435731:2436922 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
46: nodeai07:2435731:2436922 [6] NCCL INFO P2P plugin IBext
46: nodeai07:2435731:2436922 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 3: nodeai01:2157667:2158850 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 3: nodeai01:2157667:2158850 [3] NCCL INFO P2P plugin IBext
 3: nodeai01:2157667:2158850 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 4: nodeai01:2157666:2158845 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 4: nodeai01:2157666:2158845 [4] NCCL INFO P2P plugin IBext
 4: nodeai01:2157666:2158845 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 1: nodeai01:2157663:2158848 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 1: nodeai01:2157663:2158848 [1] NCCL INFO P2P plugin IBext
 1: nodeai01:2157663:2158848 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
44: nodeai07:2435729:2436925 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
44: nodeai07:2435729:2436925 [4] NCCL INFO P2P plugin IBext
44: nodeai07:2435729:2436925 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 2: nodeai01:2157662:2158849 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 2: nodeai01:2157662:2158849 [2] NCCL INFO P2P plugin IBext
 2: nodeai01:2157662:2158849 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
43: nodeai07:2435733:2436924 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
43: nodeai07:2435733:2436924 [3] NCCL INFO Using network IBext
 7: nodeai01:2157664:2158847 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 7: nodeai01:2157664:2158847 [7] NCCL INFO Using network IBext
44: nodeai07:2435729:2436925 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
44: nodeai07:2435729:2436925 [4] NCCL INFO Using network IBext
 4: nodeai01:2157666:2158845 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 4: nodeai01:2157666:2158845 [4] NCCL INFO Using network IBext
 5: nodeai01:2157665:2158846 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 5: nodeai01:2157665:2158846 [5] NCCL INFO Using network IBext
47: nodeai07:2435736:2436920 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
47: nodeai07:2435736:2436920 [7] NCCL INFO Using network IBext
 0: nodeai01:2157660:2158843 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 0: nodeai01:2157660:2158843 [0] NCCL INFO Using network IBext
45: nodeai07:2435732:2436926 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
45: nodeai07:2435732:2436926 [5] NCCL INFO Using network IBext
46: nodeai07:2435731:2436922 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
46: nodeai07:2435731:2436922 [6] NCCL INFO Using network IBext
 6: nodeai01:2157661:2158844 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 6: nodeai01:2157661:2158844 [6] NCCL INFO Using network IBext
 1: nodeai01:2157663:2158848 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 1: nodeai01:2157663:2158848 [1] NCCL INFO Using network IBext
 2: nodeai01:2157662:2158849 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 2: nodeai01:2157662:2158849 [2] NCCL INFO Using network IBext
41: nodeai07:2435735:2436921 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
41: nodeai07:2435735:2436921 [1] NCCL INFO Using network IBext
 3: nodeai01:2157667:2158850 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.11<0>
 3: nodeai01:2157667:2158850 [3] NCCL INFO Using network IBext
16: nodeai04:2417513:2418698 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
16: nodeai04:2417513:2418698 [0] NCCL INFO P2P plugin IBext
42: nodeai07:2435730:2436923 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
42: nodeai07:2435730:2436923 [2] NCCL INFO Using network IBext
19: nodeai04:2417510:2418691 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
19: nodeai04:2417510:2418691 [3] NCCL INFO P2P plugin IBext
19: nodeai04:2417510:2418691 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
23: nodeai04:2417507:2418697 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
23: nodeai04:2417507:2418697 [7] NCCL INFO P2P plugin IBext
23: nodeai04:2417507:2418697 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
16: nodeai04:2417513:2418698 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
20: nodeai04:2417506:2418694 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
20: nodeai04:2417506:2418694 [4] NCCL INFO P2P plugin IBext
20: nodeai04:2417506:2418694 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
22: nodeai04:2417511:2418696 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
22: nodeai04:2417511:2418696 [6] NCCL INFO P2P plugin IBext
22: nodeai04:2417511:2418696 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
40: nodeai07:2435734:2436919 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.59<0>
40: nodeai07:2435734:2436919 [0] NCCL INFO Using network IBext
21: nodeai04:2417508:2418695 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
21: nodeai04:2417508:2418695 [5] NCCL INFO P2P plugin IBext
21: nodeai04:2417508:2418695 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
18: nodeai04:2417512:2418693 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
18: nodeai04:2417512:2418693 [2] NCCL INFO P2P plugin IBext
18: nodeai04:2417512:2418693 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
17: nodeai04:2417509:2418692 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
17: nodeai04:2417509:2418692 [1] NCCL INFO P2P plugin IBext
17: nodeai04:2417509:2418692 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
23: nodeai04:2417507:2418697 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
23: nodeai04:2417507:2418697 [7] NCCL INFO Using network IBext
20: nodeai04:2417506:2418694 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
20: nodeai04:2417506:2418694 [4] NCCL INFO Using network IBext
35: nodeai06:2504431:2505625 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
35: nodeai06:2504431:2505625 [3] NCCL INFO P2P plugin IBext
35: nodeai06:2504431:2505625 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
38: nodeai06:2504437:2505621 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
38: nodeai06:2504437:2505621 [6] NCCL INFO P2P plugin IBext
38: nodeai06:2504437:2505621 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
32: nodeai06:2504436:2505627 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
32: nodeai06:2504436:2505627 [0] NCCL INFO P2P plugin IBext
32: nodeai06:2504436:2505627 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
33: nodeai06:2504433:2505622 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
33: nodeai06:2504433:2505622 [1] NCCL INFO P2P plugin IBext
33: nodeai06:2504433:2505622 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
16: nodeai04:2417513:2418698 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
16: nodeai04:2417513:2418698 [0] NCCL INFO Using network IBext
36: nodeai06:2504434:2505624 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
36: nodeai06:2504434:2505624 [4] NCCL INFO P2P plugin IBext
36: nodeai06:2504434:2505624 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
19: nodeai04:2417510:2418691 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
19: nodeai04:2417510:2418691 [3] NCCL INFO Using network IBext
39: nodeai06:2504430:2505623 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
39: nodeai06:2504430:2505623 [7] NCCL INFO P2P plugin IBext
39: nodeai06:2504430:2505623 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
21: nodeai04:2417508:2418695 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
21: nodeai04:2417508:2418695 [5] NCCL INFO Using network IBext
37: nodeai06:2504432:2505626 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
37: nodeai06:2504432:2505626 [5] NCCL INFO P2P plugin IBext
37: nodeai06:2504432:2505626 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
34: nodeai06:2504435:2505620 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
34: nodeai06:2504435:2505620 [2] NCCL INFO P2P plugin IBext
34: nodeai06:2504435:2505620 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
18: nodeai04:2417512:2418693 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
18: nodeai04:2417512:2418693 [2] NCCL INFO Using network IBext
17: nodeai04:2417509:2418692 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
17: nodeai04:2417509:2418692 [1] NCCL INFO Using network IBext
22: nodeai04:2417511:2418696 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.35<0>
22: nodeai04:2417511:2418696 [6] NCCL INFO Using network IBext
35: nodeai06:2504431:2505625 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
35: nodeai06:2504431:2505625 [3] NCCL INFO Using network IBext
37: nodeai06:2504432:2505626 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
37: nodeai06:2504432:2505626 [5] NCCL INFO Using network IBext
36: nodeai06:2504434:2505624 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
36: nodeai06:2504434:2505624 [4] NCCL INFO Using network IBext
32: nodeai06:2504436:2505627 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
32: nodeai06:2504436:2505627 [0] NCCL INFO Using network IBext
38: nodeai06:2504437:2505621 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
38: nodeai06:2504437:2505621 [6] NCCL INFO Using network IBext
34: nodeai06:2504435:2505620 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
34: nodeai06:2504435:2505620 [2] NCCL INFO Using network IBext
33: nodeai06:2504433:2505622 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
33: nodeai06:2504433:2505622 [1] NCCL INFO Using network IBext
39: nodeai06:2504430:2505623 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.51<0>
39: nodeai06:2504430:2505623 [7] NCCL INFO Using network IBext
13: nodeai02:2459392:2460582 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
13: nodeai02:2459392:2460582 [5] NCCL INFO P2P plugin IBext
13: nodeai02:2459392:2460582 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
12: nodeai02:2459394:2460576 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
12: nodeai02:2459394:2460576 [4] NCCL INFO P2P plugin IBext
12: nodeai02:2459394:2460576 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
15: nodeai02:2459391:2460581 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
15: nodeai02:2459391:2460581 [7] NCCL INFO P2P plugin IBext
15: nodeai02:2459391:2460581 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
14: nodeai02:2459389:2460583 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
14: nodeai02:2459389:2460583 [6] NCCL INFO P2P plugin IBext
14: nodeai02:2459389:2460583 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
10: nodeai02:2459393:2460578 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
10: nodeai02:2459393:2460578 [2] NCCL INFO P2P plugin IBext
10: nodeai02:2459393:2460578 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 8: nodeai02:2459395:2460579 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 8: nodeai02:2459395:2460579 [0] NCCL INFO P2P plugin IBext
 8: nodeai02:2459395:2460579 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
 9: nodeai02:2459396:2460580 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
 9: nodeai02:2459396:2460580 [1] NCCL INFO P2P plugin IBext
 9: nodeai02:2459396:2460580 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
11: nodeai02:2459390:2460577 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
11: nodeai02:2459390:2460577 [3] NCCL INFO P2P plugin IBext
11: nodeai02:2459390:2460577 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
12: nodeai02:2459394:2460576 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
12: nodeai02:2459394:2460576 [4] NCCL INFO Using network IBext
15: nodeai02:2459391:2460581 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
15: nodeai02:2459391:2460581 [7] NCCL INFO Using network IBext
 8: nodeai02:2459395:2460579 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
 8: nodeai02:2459395:2460579 [0] NCCL INFO Using network IBext
10: nodeai02:2459393:2460578 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
10: nodeai02:2459393:2460578 [2] NCCL INFO Using network IBext
13: nodeai02:2459392:2460582 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
13: nodeai02:2459392:2460582 [5] NCCL INFO Using network IBext
14: nodeai02:2459389:2460583 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
14: nodeai02:2459389:2460583 [6] NCCL INFO Using network IBext
 9: nodeai02:2459396:2460580 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
 9: nodeai02:2459396:2460580 [1] NCCL INFO Using network IBext
11: nodeai02:2459390:2460577 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.19<0>
11: nodeai02:2459390:2460577 [3] NCCL INFO Using network IBext
29: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
29:   self._overflow_buf = torch.cuda.IntTensor([0])
31: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
31:   self._overflow_buf = torch.cuda.IntTensor([0])
24: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
24:   self._overflow_buf = torch.cuda.IntTensor([0])
25: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
25:   self._overflow_buf = torch.cuda.IntTensor([0])
26: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
26:   self._overflow_buf = torch.cuda.IntTensor([0])
27: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
27:   self._overflow_buf = torch.cuda.IntTensor([0])
28: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
28:   self._overflow_buf = torch.cuda.IntTensor([0])
30: /usr/local/lib/python3.10/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py:119: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)
30:   self._overflow_buf = torch.cuda.IntTensor([0])
30: nodeai05:1971448:1971448 [6] NCCL INFO cudaDriverVersion 12040
30: nodeai05:1971448:1971448 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
30: nodeai05:1971448:1971448 [6] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
25: nodeai05:1971445:1971445 [1] NCCL INFO cudaDriverVersion 12040
25: nodeai05:1971445:1971445 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
24: nodeai05:1971447:1971447 [0] NCCL INFO cudaDriverVersion 12040
25: nodeai05:1971445:1971445 [1] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
24: nodeai05:1971447:1971447 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
24: nodeai05:1971447:1971447 [0] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
26: nodeai05:1971443:1971443 [2] NCCL INFO cudaDriverVersion 12040
26: nodeai05:1971443:1971443 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
26: nodeai05:1971443:1971443 [2] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
28: nodeai05:1971442:1971442 [4] NCCL INFO cudaDriverVersion 12040
28: nodeai05:1971442:1971442 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
28: nodeai05:1971442:1971442 [4] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
29: nodeai05:1971444:1971444 [5] NCCL INFO cudaDriverVersion 12040
29: nodeai05:1971444:1971444 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
29: nodeai05:1971444:1971444 [5] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
31: nodeai05:1971449:1971449 [7] NCCL INFO cudaDriverVersion 12040
31: nodeai05:1971449:1971449 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
31: nodeai05:1971449:1971449 [7] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
27: nodeai05:1971446:1971446 [3] NCCL INFO cudaDriverVersion 12040
27: nodeai05:1971446:1971446 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
27: nodeai05:1971446:1971446 [3] NCCL INFO Bootstrap : Using ib0:10.128.200.43<0>
31: nodeai05:1971449:1972636 [7] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
31: nodeai05:1971449:1972636 [7] NCCL INFO P2P plugin IBext
31: nodeai05:1971449:1972636 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
26: nodeai05:1971443:1972633 [2] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
26: nodeai05:1971443:1972633 [2] NCCL INFO P2P plugin IBext
26: nodeai05:1971443:1972633 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
27: nodeai05:1971446:1972637 [3] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
27: nodeai05:1971446:1972637 [3] NCCL INFO P2P plugin IBext
27: nodeai05:1971446:1972637 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
24: nodeai05:1971447:1972632 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
24: nodeai05:1971447:1972632 [0] NCCL INFO P2P plugin IBext
24: nodeai05:1971447:1972632 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
28: nodeai05:1971442:1972634 [4] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
28: nodeai05:1971442:1972634 [4] NCCL INFO P2P plugin IBext
28: nodeai05:1971442:1972634 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
29: nodeai05:1971444:1972635 [5] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
29: nodeai05:1971444:1972635 [5] NCCL INFO P2P plugin IBext
29: nodeai05:1971444:1972635 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
30: nodeai05:1971448:1972630 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
30: nodeai05:1971448:1972630 [6] NCCL INFO P2P plugin IBext
30: nodeai05:1971448:1972630 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
25: nodeai05:1971445:1972631 [1] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
25: nodeai05:1971445:1972631 [1] NCCL INFO P2P plugin IBext
25: nodeai05:1971445:1972631 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib3,ib4,ib5,ib6,ib7,ib8,ib9
31: nodeai05:1971449:1972636 [7] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
31: nodeai05:1971449:1972636 [7] NCCL INFO Using network IBext
24: nodeai05:1971447:1972632 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
24: nodeai05:1971447:1972632 [0] NCCL INFO Using network IBext
27: nodeai05:1971446:1972637 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
27: nodeai05:1971446:1972637 [3] NCCL INFO Using network IBext
25: nodeai05:1971445:1972631 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
25: nodeai05:1971445:1972631 [1] NCCL INFO Using network IBext
29: nodeai05:1971444:1972635 [5] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
29: nodeai05:1971444:1972635 [5] NCCL INFO Using network IBext
26: nodeai05:1971443:1972633 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
26: nodeai05:1971443:1972633 [2] NCCL INFO Using network IBext
30: nodeai05:1971448:1972630 [6] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
30: nodeai05:1971448:1972630 [6] NCCL INFO Using network IBext
28: nodeai05:1971442:1972634 [4] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_4:1/IB/SHARP [3]mlx5_5:1/IB/SHARP [4]mlx5_6:1/IB/SHARP [5]mlx5_9:1/IB/SHARP [6]mlx5_10:1/IB/SHARP [7]mlx5_11:1/IB/SHARP [RO]; OOB ib0:10.128.200.43<0>
28: nodeai05:1971442:1972634 [4] NCCL INFO Using network IBext
 0: nodeai01:2157660:2158843 [0] NCCL INFO comm 0x5609149c7900 rank 0 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init START
 2: nodeai01:2157662:2158849 [2] NCCL INFO comm 0x556a1ae3d180 rank 2 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init START
 1: nodeai01:2157663:2158848 [1] NCCL INFO comm 0x55cb52b80bc0 rank 1 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init START
 3: nodeai01:2157667:2158850 [3] NCCL INFO comm 0x5571536aaac0 rank 3 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init START
 4: nodeai01:2157666:2158845 [4] NCCL INFO comm 0x556b87fd8840 rank 4 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init START
47: nodeai07:2435736:2436920 [7] NCCL INFO comm 0x564e1f98c4c0 rank 47 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init START
 5: nodeai01:2157665:2158846 [5] NCCL INFO comm 0x55f9facf8dc0 rank 5 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init START
 6: nodeai01:2157661:2158844 [6] NCCL INFO comm 0x55aa516d7b00 rank 6 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init START
46: nodeai07:2435731:2436922 [6] NCCL INFO comm 0x561627d2c000 rank 46 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init START
 7: nodeai01:2157664:2158847 [7] NCCL INFO comm 0x561beaa3d7c0 rank 7 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init START
 8: nodeai02:2459395:2460579 [0] NCCL INFO comm 0x55adf155d980 rank 8 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init START
10: nodeai02:2459393:2460578 [2] NCCL INFO comm 0x556f37828780 rank 10 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init START
 9: nodeai02:2459396:2460580 [1] NCCL INFO comm 0x559b064037c0 rank 9 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init START
11: nodeai02:2459390:2460577 [3] NCCL INFO comm 0x564a9a78c300 rank 11 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init START
12: nodeai02:2459394:2460576 [4] NCCL INFO comm 0x55d806bc8ac0 rank 12 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init START
13: nodeai02:2459392:2460582 [5] NCCL INFO comm 0x55d980822880 rank 13 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init START
14: nodeai02:2459389:2460583 [6] NCCL INFO comm 0x55ec378e1840 rank 14 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init START
15: nodeai02:2459391:2460581 [7] NCCL INFO comm 0x5619f3e1f040 rank 15 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init START
41: nodeai07:2435735:2436921 [1] NCCL INFO comm 0x55cf5455bb80 rank 41 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init START
42: nodeai07:2435730:2436923 [2] NCCL INFO comm 0x55c42d8bcb80 rank 42 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init START
43: nodeai07:2435733:2436924 [3] NCCL INFO comm 0x559b9d0385c0 rank 43 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init START
44: nodeai07:2435729:2436925 [4] NCCL INFO comm 0x564b194ce040 rank 44 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init START
45: nodeai07:2435732:2436926 [5] NCCL INFO comm 0x56314f520540 rank 45 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init START
16: nodeai04:2417513:2418698 [0] NCCL INFO comm 0x55ea13bc9dc0 rank 16 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init START
17: nodeai04:2417509:2418692 [1] NCCL INFO comm 0x55c6a815d6c0 rank 17 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init START
18: nodeai04:2417512:2418693 [2] NCCL INFO comm 0x564c4f3cf680 rank 18 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init START
19: nodeai04:2417510:2418691 [3] NCCL INFO comm 0x55e304d4de40 rank 19 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init START
20: nodeai04:2417506:2418694 [4] NCCL INFO comm 0x5615b35ac500 rank 20 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init START
40: nodeai07:2435734:2436919 [0] NCCL INFO comm 0x5581fe0eb6c0 rank 40 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init START
21: nodeai04:2417508:2418695 [5] NCCL INFO comm 0x558756c399c0 rank 21 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init START
22: nodeai04:2417511:2418696 [6] NCCL INFO comm 0x560f6bdcbc80 rank 22 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init START
29: nodeai05:1971444:1972635 [5] NCCL INFO comm 0x56523b50c3c0 rank 29 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init START
23: nodeai04:2417507:2418697 [7] NCCL INFO comm 0x55a36f03a600 rank 23 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init START
30: nodeai05:1971448:1972630 [6] NCCL INFO comm 0x55754205dbc0 rank 30 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init START
31: nodeai05:1971449:1972636 [7] NCCL INFO comm 0x55a01ffbc340 rank 31 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init START
28: nodeai05:1971442:1972634 [4] NCCL INFO comm 0x560ddfef6e80 rank 28 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init START
24: nodeai05:1971447:1972632 [0] NCCL INFO comm 0x5643eedcebc0 rank 24 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init START
25: nodeai05:1971445:1972631 [1] NCCL INFO comm 0x55c3e3c1bc80 rank 25 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init START
26: nodeai05:1971443:1972633 [2] NCCL INFO comm 0x55be6917b200 rank 26 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init START
27: nodeai05:1971446:1972637 [3] NCCL INFO comm 0x55bd9df48280 rank 27 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init START
32: nodeai06:2504436:2505627 [0] NCCL INFO comm 0x56050cd8ad80 rank 32 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init START
33: nodeai06:2504433:2505622 [1] NCCL INFO comm 0x5601eae71980 rank 33 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init START
34: nodeai06:2504435:2505620 [2] NCCL INFO comm 0x5630f97acd40 rank 34 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init START
35: nodeai06:2504431:2505625 [3] NCCL INFO comm 0x55cbe345b740 rank 35 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init START
36: nodeai06:2504434:2505624 [4] NCCL INFO comm 0x5619adeb56c0 rank 36 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init START
37: nodeai06:2504432:2505626 [5] NCCL INFO comm 0x5622d3b13780 rank 37 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init START
38: nodeai06:2504437:2505621 [6] NCCL INFO comm 0x55f04b397a80 rank 38 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init START
39: nodeai06:2504430:2505623 [7] NCCL INFO comm 0x5654f999ea00 rank 39 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init START
10: nodeai02:2459393:2460578 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
10: nodeai02:2459393:2460578 [2] NCCL INFO NVLS multicast support is available on dev 2
 8: nodeai02:2459395:2460579 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
 8: nodeai02:2459395:2460579 [0] NCCL INFO NVLS multicast support is available on dev 0
11: nodeai02:2459390:2460577 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
11: nodeai02:2459390:2460577 [3] NCCL INFO NVLS multicast support is available on dev 3
14: nodeai02:2459389:2460583 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
14: nodeai02:2459389:2460583 [6] NCCL INFO NVLS multicast support is available on dev 6
 9: nodeai02:2459396:2460580 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
 9: nodeai02:2459396:2460580 [1] NCCL INFO NVLS multicast support is available on dev 1
12: nodeai02:2459394:2460576 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
12: nodeai02:2459394:2460576 [4] NCCL INFO NVLS multicast support is available on dev 4
15: nodeai02:2459391:2460581 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
15: nodeai02:2459391:2460581 [7] NCCL INFO NVLS multicast support is available on dev 7
13: nodeai02:2459392:2460582 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
13: nodeai02:2459392:2460582 [5] NCCL INFO NVLS multicast support is available on dev 5
32: nodeai06:2504436:2505627 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
32: nodeai06:2504436:2505627 [0] NCCL INFO NVLS multicast support is available on dev 0
33: nodeai06:2504433:2505622 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
33: nodeai06:2504433:2505622 [1] NCCL INFO NVLS multicast support is available on dev 1
34: nodeai06:2504435:2505620 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
34: nodeai06:2504435:2505620 [2] NCCL INFO NVLS multicast support is available on dev 2
35: nodeai06:2504431:2505625 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
35: nodeai06:2504431:2505625 [3] NCCL INFO NVLS multicast support is available on dev 3
39: nodeai06:2504430:2505623 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
39: nodeai06:2504430:2505623 [7] NCCL INFO NVLS multicast support is available on dev 7
37: nodeai06:2504432:2505626 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
37: nodeai06:2504432:2505626 [5] NCCL INFO NVLS multicast support is available on dev 5
38: nodeai06:2504437:2505621 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
38: nodeai06:2504437:2505621 [6] NCCL INFO NVLS multicast support is available on dev 6
36: nodeai06:2504434:2505624 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
36: nodeai06:2504434:2505624 [4] NCCL INFO NVLS multicast support is available on dev 4
40: nodeai07:2435734:2436919 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
40: nodeai07:2435734:2436919 [0] NCCL INFO NVLS multicast support is available on dev 0
43: nodeai07:2435733:2436924 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
43: nodeai07:2435733:2436924 [3] NCCL INFO NVLS multicast support is available on dev 3
47: nodeai07:2435736:2436920 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
47: nodeai07:2435736:2436920 [7] NCCL INFO NVLS multicast support is available on dev 7
46: nodeai07:2435731:2436922 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
46: nodeai07:2435731:2436922 [6] NCCL INFO NVLS multicast support is available on dev 6
45: nodeai07:2435732:2436926 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
45: nodeai07:2435732:2436926 [5] NCCL INFO NVLS multicast support is available on dev 5
44: nodeai07:2435729:2436925 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
44: nodeai07:2435729:2436925 [4] NCCL INFO NVLS multicast support is available on dev 4
41: nodeai07:2435735:2436921 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
41: nodeai07:2435735:2436921 [1] NCCL INFO NVLS multicast support is available on dev 1
42: nodeai07:2435730:2436923 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
42: nodeai07:2435730:2436923 [2] NCCL INFO NVLS multicast support is available on dev 2
 5: nodeai01:2157665:2158846 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS multicast support is available on dev 5
 6: nodeai01:2157661:2158844 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS multicast support is available on dev 6
 7: nodeai01:2157664:2158847 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS multicast support is available on dev 7
 2: nodeai01:2157662:2158849 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS multicast support is available on dev 2
 3: nodeai01:2157667:2158850 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS multicast support is available on dev 3
 1: nodeai01:2157663:2158848 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS multicast support is available on dev 1
 0: nodeai01:2157660:2158843 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS multicast support is available on dev 0
 4: nodeai01:2157666:2158845 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS multicast support is available on dev 4
26: nodeai05:1971443:1972633 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
26: nodeai05:1971443:1972633 [2] NCCL INFO NVLS multicast support is available on dev 2
24: nodeai05:1971447:1972632 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
24: nodeai05:1971447:1972632 [0] NCCL INFO NVLS multicast support is available on dev 0
28: nodeai05:1971442:1972634 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
28: nodeai05:1971442:1972634 [4] NCCL INFO NVLS multicast support is available on dev 4
29: nodeai05:1971444:1972635 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
29: nodeai05:1971444:1972635 [5] NCCL INFO NVLS multicast support is available on dev 5
31: nodeai05:1971449:1972636 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
31: nodeai05:1971449:1972636 [7] NCCL INFO NVLS multicast support is available on dev 7
25: nodeai05:1971445:1972631 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
25: nodeai05:1971445:1972631 [1] NCCL INFO NVLS multicast support is available on dev 1
27: nodeai05:1971446:1972637 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
27: nodeai05:1971446:1972637 [3] NCCL INFO NVLS multicast support is available on dev 3
30: nodeai05:1971448:1972630 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
30: nodeai05:1971448:1972630 [6] NCCL INFO NVLS multicast support is available on dev 6
20: nodeai04:2417506:2418694 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
20: nodeai04:2417506:2418694 [4] NCCL INFO NVLS multicast support is available on dev 4
21: nodeai04:2417508:2418695 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
21: nodeai04:2417508:2418695 [5] NCCL INFO NVLS multicast support is available on dev 5
18: nodeai04:2417512:2418693 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
18: nodeai04:2417512:2418693 [2] NCCL INFO NVLS multicast support is available on dev 2
19: nodeai04:2417510:2418691 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
19: nodeai04:2417510:2418691 [3] NCCL INFO NVLS multicast support is available on dev 3
23: nodeai04:2417507:2418697 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
23: nodeai04:2417507:2418697 [7] NCCL INFO NVLS multicast support is available on dev 7
22: nodeai04:2417511:2418696 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
22: nodeai04:2417511:2418696 [6] NCCL INFO NVLS multicast support is available on dev 6
17: nodeai04:2417509:2418692 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
17: nodeai04:2417509:2418692 [1] NCCL INFO NVLS multicast support is available on dev 1
16: nodeai04:2417513:2418698 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
16: nodeai04:2417513:2418698 [0] NCCL INFO NVLS multicast support is available on dev 0
16: nodeai04:2417513:2418698 [0] NCCL INFO Trees [0] 17/8/24->16->32 [1] -1/-1/-1->16->23 [2] 17/-1/-1->16->23 [3] 17/-1/-1->16->23 [4] 17/-1/-1->16->23 [5] 17/-1/-1->16->23 [6] 17/-1/-1->16->23 [7] 17/-1/-1->16->23 [8] 17/-1/-1->16->24 [9] -1/-1/-1->16->23 [10] 17/-1/-1->16->23 [11] 17/-1/-1->16->23 [12] 17/-1/-1->16->23 [13] 17/-1/-1->16->23 [14] 17/-1/-1->16->23 [15] 17/-1/-1->16->23
16: nodeai04:2417513:2418698 [0] NCCL INFO P2P Chunksize set to 131072
17: nodeai04:2417509:2418692 [1] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/9/25->17->33 [2] -1/-1/-1->17->16 [3] 18/-1/-1->17->16 [4] 18/-1/-1->17->16 [5] 18/-1/-1->17->16 [6] 18/-1/-1->17->16 [7] 18/-1/-1->17->16 [8] 18/-1/-1->17->16 [9] 18/-1/-1->17->25 [10] -1/-1/-1->17->16 [11] 18/-1/-1->17->16 [12] 18/-1/-1->17->16 [13] 18/-1/-1->17->16 [14] 18/-1/-1->17->16 [15] 18/-1/-1->17->16
17: nodeai04:2417509:2418692 [1] NCCL INFO P2P Chunksize set to 131072
18: nodeai04:2417512:2418693 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/10/26->18->34 [3] -1/-1/-1->18->17 [4] 19/-1/-1->18->17 [5] 19/-1/-1->18->17 [6] 19/-1/-1->18->17 [7] 19/-1/-1->18->17 [8] 19/-1/-1->18->17 [9] 19/-1/-1->18->17 [10] 19/-1/-1->18->26 [11] -1/-1/-1->18->17 [12] 19/-1/-1->18->17 [13] 19/-1/-1->18->17 [14] 19/-1/-1->18->17 [15] 19/-1/-1->18->17
18: nodeai04:2417512:2418693 [2] NCCL INFO P2P Chunksize set to 131072
19: nodeai04:2417510:2418691 [3] NCCL INFO Trees [0] 20/-1/-1->19->18 [1] 20/-1/-1->19->18 [2] 20/-1/-1->19->18 [3] 20/11/27->19->35 [4] -1/-1/-1->19->18 [5] 20/-1/-1->19->18 [6] 20/-1/-1->19->18 [7] 20/-1/-1->19->18 [8] 20/-1/-1->19->18 [9] 20/-1/-1->19->18 [10] 20/-1/-1->19->18 [11] 20/-1/-1->19->27 [12] -1/-1/-1->19->18 [13] 20/-1/-1->19->18 [14] 20/-1/-1->19->18 [15] 20/-1/-1->19->18
19: nodeai04:2417510:2418691 [3] NCCL INFO P2P Chunksize set to 131072
23: nodeai04:2417507:2418697 [7] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] 16/-1/-1->23->22 [2] 16/-1/-1->23->22 [3] 16/-1/-1->23->22 [4] 16/-1/-1->23->22 [5] 16/-1/-1->23->22 [6] 16/-1/-1->23->22 [7] 16/15/31->23->39 [8] -1/-1/-1->23->22 [9] 16/-1/-1->23->22 [10] 16/-1/-1->23->22 [11] 16/-1/-1->23->22 [12] 16/-1/-1->23->22 [13] 16/-1/-1->23->22 [14] 16/-1/-1->23->22 [15] 16/-1/-1->23->31
23: nodeai04:2417507:2418697 [7] NCCL INFO P2P Chunksize set to 131072
20: nodeai04:2417506:2418694 [4] NCCL INFO Trees [0] 21/-1/-1->20->19 [1] 21/-1/-1->20->19 [2] 21/-1/-1->20->19 [3] 21/-1/-1->20->19 [4] 21/12/28->20->36 [5] -1/-1/-1->20->19 [6] 21/-1/-1->20->19 [7] 21/-1/-1->20->19 [8] 21/-1/-1->20->19 [9] 21/-1/-1->20->19 [10] 21/-1/-1->20->19 [11] 21/-1/-1->20->19 [12] 21/-1/-1->20->28 [13] -1/-1/-1->20->19 [14] 21/-1/-1->20->19 [15] 21/-1/-1->20->19
20: nodeai04:2417506:2418694 [4] NCCL INFO P2P Chunksize set to 131072
21: nodeai04:2417508:2418695 [5] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] 22/-1/-1->21->20 [3] 22/-1/-1->21->20 [4] 22/-1/-1->21->20 [5] 22/13/29->21->37 [6] -1/-1/-1->21->20 [7] 22/-1/-1->21->20 [8] 22/-1/-1->21->20 [9] 22/-1/-1->21->20 [10] 22/-1/-1->21->20 [11] 22/-1/-1->21->20 [12] 22/-1/-1->21->20 [13] 22/-1/-1->21->29 [14] -1/-1/-1->21->20 [15] 22/-1/-1->21->20
21: nodeai04:2417508:2418695 [5] NCCL INFO P2P Chunksize set to 131072
22: nodeai04:2417511:2418696 [6] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->21 [3] 23/-1/-1->22->21 [4] 23/-1/-1->22->21 [5] 23/-1/-1->22->21 [6] 23/14/30->22->38 [7] -1/-1/-1->22->21 [8] 23/-1/-1->22->21 [9] 23/-1/-1->22->21 [10] 23/-1/-1->22->21 [11] 23/-1/-1->22->21 [12] 23/-1/-1->22->21 [13] 23/-1/-1->22->21 [14] 23/-1/-1->22->30 [15] -1/-1/-1->22->21
22: nodeai04:2417511:2418696 [6] NCCL INFO P2P Chunksize set to 131072
15: nodeai02:2459391:2460581 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] 8/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->23 [8] -1/-1/-1->15->14 [9] 8/-1/-1->15->14 [10] 8/-1/-1->15->14 [11] 8/-1/-1->15->14 [12] 8/-1/-1->15->14 [13] 8/-1/-1->15->14 [14] 8/-1/-1->15->14 [15] 8/31/7->15->47
15: nodeai02:2459391:2460581 [7] NCCL INFO P2P Chunksize set to 131072
14: nodeai02:2459389:2460583 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->22 [7] -1/-1/-1->14->13 [8] 15/-1/-1->14->13 [9] 15/-1/-1->14->13 [10] 15/-1/-1->14->13 [11] 15/-1/-1->14->13 [12] 15/-1/-1->14->13 [13] 15/-1/-1->14->13 [14] 15/30/6->14->46 [15] -1/-1/-1->14->13
14: nodeai02:2459389:2460583 [6] NCCL INFO P2P Chunksize set to 131072
10: nodeai02:2459393:2460578 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->18 [3] -1/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9 [8] 11/-1/-1->10->9 [9] 11/-1/-1->10->9 [10] 11/26/2->10->42 [11] -1/-1/-1->10->9 [12] 11/-1/-1->10->9 [13] 11/-1/-1->10->9 [14] 11/-1/-1->10->9 [15] 11/-1/-1->10->9
10: nodeai02:2459393:2460578 [2] NCCL INFO P2P Chunksize set to 131072
11: nodeai02:2459390:2460577 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->19 [4] -1/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] 12/-1/-1->11->10 [7] 12/-1/-1->11->10 [8] 12/-1/-1->11->10 [9] 12/-1/-1->11->10 [10] 12/-1/-1->11->10 [11] 12/27/3->11->43 [12] -1/-1/-1->11->10 [13] 12/-1/-1->11->10 [14] 12/-1/-1->11->10 [15] 12/-1/-1->11->10
11: nodeai02:2459390:2460577 [3] NCCL INFO P2P Chunksize set to 131072
12: nodeai02:2459394:2460576 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->20 [5] -1/-1/-1->12->11 [6] 13/-1/-1->12->11 [7] 13/-1/-1->12->11 [8] 13/-1/-1->12->11 [9] 13/-1/-1->12->11 [10] 13/-1/-1->12->11 [11] 13/-1/-1->12->11 [12] 13/28/4->12->44 [13] -1/-1/-1->12->11 [14] 13/-1/-1->12->11 [15] 13/-1/-1->12->11
12: nodeai02:2459394:2460576 [4] NCCL INFO P2P Chunksize set to 131072
13: nodeai02:2459392:2460582 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->21 [6] -1/-1/-1->13->12 [7] 14/-1/-1->13->12 [8] 14/-1/-1->13->12 [9] 14/-1/-1->13->12 [10] 14/-1/-1->13->12 [11] 14/-1/-1->13->12 [12] 14/-1/-1->13->12 [13] 14/29/5->13->45 [14] -1/-1/-1->13->12 [15] 14/-1/-1->13->12
13: nodeai02:2459392:2460582 [5] NCCL INFO P2P Chunksize set to 131072
 9: nodeai02:2459396:2460580 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->17 [2] -1/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8 [8] 10/-1/-1->9->8 [9] 10/25/1->9->41 [10] -1/-1/-1->9->8 [11] 10/-1/-1->9->8 [12] 10/-1/-1->9->8 [13] 10/-1/-1->9->8 [14] 10/-1/-1->9->8 [15] 10/-1/-1->9->8
 9: nodeai02:2459396:2460580 [1] NCCL INFO P2P Chunksize set to 131072
24: nodeai05:1971447:1972632 [0] NCCL INFO Trees [0] 25/-1/-1->24->16 [1] -1/-1/-1->24->31 [2] 25/-1/-1->24->31 [3] 25/-1/-1->24->31 [4] 25/-1/-1->24->31 [5] 25/-1/-1->24->31 [6] 25/-1/-1->24->31 [7] 25/-1/-1->24->31 [8] 25/32/16->24->8 [9] -1/-1/-1->24->31 [10] 25/-1/-1->24->31 [11] 25/-1/-1->24->31 [12] 25/-1/-1->24->31 [13] 25/-1/-1->24->31 [14] 25/-1/-1->24->31 [15] 25/-1/-1->24->31
24: nodeai05:1971447:1972632 [0] NCCL INFO P2P Chunksize set to 131072
 8: nodeai02:2459395:2460579 [0] NCCL INFO Trees [0] 9/-1/-1->8->16 [1] -1/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/-1/-1->8->15 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15 [8] 9/24/0->8->40 [9] -1/-1/-1->8->15 [10] 9/-1/-1->8->15 [11] 9/-1/-1->8->15 [12] 9/-1/-1->8->15 [13] 9/-1/-1->8->15 [14] 9/-1/-1->8->15 [15] 9/-1/-1->8->15
25: nodeai05:1971445:1972631 [1] NCCL INFO Trees [0] 26/-1/-1->25->24 [1] 26/-1/-1->25->17 [2] -1/-1/-1->25->24 [3] 26/-1/-1->25->24 [4] 26/-1/-1->25->24 [5] 26/-1/-1->25->24 [6] 26/-1/-1->25->24 [7] 26/-1/-1->25->24 [8] 26/-1/-1->25->24 [9] 26/33/17->25->9 [10] -1/-1/-1->25->24 [11] 26/-1/-1->25->24 [12] 26/-1/-1->25->24 [13] 26/-1/-1->25->24 [14] 26/-1/-1->25->24 [15] 26/-1/-1->25->24
25: nodeai05:1971445:1972631 [1] NCCL INFO P2P Chunksize set to 131072
 8: nodeai02:2459395:2460579 [0] NCCL INFO P2P Chunksize set to 131072
26: nodeai05:1971443:1972633 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] 27/-1/-1->26->25 [2] 27/-1/-1->26->18 [3] -1/-1/-1->26->25 [4] 27/-1/-1->26->25 [5] 27/-1/-1->26->25 [6] 27/-1/-1->26->25 [7] 27/-1/-1->26->25 [8] 27/-1/-1->26->25 [9] 27/-1/-1->26->25 [10] 27/34/18->26->10 [11] -1/-1/-1->26->25 [12] 27/-1/-1->26->25 [13] 27/-1/-1->26->25 [14] 27/-1/-1->26->25 [15] 27/-1/-1->26->25
26: nodeai05:1971443:1972633 [2] NCCL INFO P2P Chunksize set to 131072
27: nodeai05:1971446:1972637 [3] NCCL INFO Trees [0] 28/-1/-1->27->26 [1] 28/-1/-1->27->26 [2] 28/-1/-1->27->26 [3] 28/-1/-1->27->19 [4] -1/-1/-1->27->26 [5] 28/-1/-1->27->26 [6] 28/-1/-1->27->26 [7] 28/-1/-1->27->26 [8] 28/-1/-1->27->26 [9] 28/-1/-1->27->26 [10] 28/-1/-1->27->26 [11] 28/35/19->27->11 [12] -1/-1/-1->27->26 [13] 28/-1/-1->27->26 [14] 28/-1/-1->27->26 [15] 28/-1/-1->27->26
27: nodeai05:1971446:1972637 [3] NCCL INFO P2P Chunksize set to 131072
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
28: nodeai05:1971442:1972634 [4] NCCL INFO Trees [0] 29/-1/-1->28->27 [1] 29/-1/-1->28->27 [2] 29/-1/-1->28->27 [3] 29/-1/-1->28->27 [4] 29/-1/-1->28->20 [5] -1/-1/-1->28->27 [6] 29/-1/-1->28->27 [7] 29/-1/-1->28->27 [8] 29/-1/-1->28->27 [9] 29/-1/-1->28->27 [10] 29/-1/-1->28->27 [11] 29/-1/-1->28->27 [12] 29/36/20->28->12 [13] -1/-1/-1->28->27 [14] 29/-1/-1->28->27 [15] 29/-1/-1->28->27
28: nodeai05:1971442:1972634 [4] NCCL INFO P2P Chunksize set to 131072
29: nodeai05:1971444:1972635 [5] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] 30/-1/-1->29->28 [3] 30/-1/-1->29->28 [4] 30/-1/-1->29->28 [5] 30/-1/-1->29->21 [6] -1/-1/-1->29->28 [7] 30/-1/-1->29->28 [8] 30/-1/-1->29->28 [9] 30/-1/-1->29->28 [10] 30/-1/-1->29->28 [11] 30/-1/-1->29->28 [12] 30/-1/-1->29->28 [13] 30/37/21->29->13 [14] -1/-1/-1->29->28 [15] 30/-1/-1->29->28
29: nodeai05:1971444:1972635 [5] NCCL INFO P2P Chunksize set to 131072
30: nodeai05:1971448:1972630 [6] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->29 [3] 31/-1/-1->30->29 [4] 31/-1/-1->30->29 [5] 31/-1/-1->30->29 [6] 31/-1/-1->30->22 [7] -1/-1/-1->30->29 [8] 31/-1/-1->30->29 [9] 31/-1/-1->30->29 [10] 31/-1/-1->30->29 [11] 31/-1/-1->30->29 [12] 31/-1/-1->30->29 [13] 31/-1/-1->30->29 [14] 31/38/22->30->14 [15] -1/-1/-1->30->29
30: nodeai05:1971448:1972630 [6] NCCL INFO P2P Chunksize set to 131072
31: nodeai05:1971449:1972636 [7] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] 24/-1/-1->31->30 [2] 24/-1/-1->31->30 [3] 24/-1/-1->31->30 [4] 24/-1/-1->31->30 [5] 24/-1/-1->31->30 [6] 24/-1/-1->31->30 [7] 24/-1/-1->31->23 [8] -1/-1/-1->31->30 [9] 24/-1/-1->31->30 [10] 24/-1/-1->31->30 [11] 24/-1/-1->31->30 [12] 24/-1/-1->31->30 [13] 24/-1/-1->31->30 [14] 24/-1/-1->31->30 [15] 24/39/23->31->15
31: nodeai05:1971449:1972636 [7] NCCL INFO P2P Chunksize set to 131072
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
32: nodeai06:2504436:2505627 [0] NCCL INFO Trees [0] 33/16/40->32->0 [1] -1/-1/-1->32->39 [2] 33/-1/-1->32->39 [3] 33/-1/-1->32->39 [4] 33/-1/-1->32->39 [5] 33/-1/-1->32->39 [6] 33/-1/-1->32->39 [7] 33/-1/-1->32->39 [8] 33/-1/-1->32->24 [9] -1/-1/-1->32->39 [10] 33/-1/-1->32->39 [11] 33/-1/-1->32->39 [12] 33/-1/-1->32->39 [13] 33/-1/-1->32->39 [14] 33/-1/-1->32->39 [15] 33/-1/-1->32->39
32: nodeai06:2504436:2505627 [0] NCCL INFO P2P Chunksize set to 131072
40: nodeai07:2435734:2436919 [0] NCCL INFO Trees [0] 41/-1/-1->40->32 [1] -1/-1/-1->40->47 [2] 41/-1/-1->40->47 [3] 41/-1/-1->40->47 [4] 41/-1/-1->40->47 [5] 41/-1/-1->40->47 [6] 41/-1/-1->40->47 [7] 41/-1/-1->40->47 [8] 41/8/-1->40->-1 [9] -1/-1/-1->40->47 [10] 41/-1/-1->40->47 [11] 41/-1/-1->40->47 [12] 41/-1/-1->40->47 [13] 41/-1/-1->40->47 [14] 41/-1/-1->40->47 [15] 41/-1/-1->40->47
40: nodeai07:2435734:2436919 [0] NCCL INFO P2P Chunksize set to 131072
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 00/16 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 01/16 :    0   3   2   9  15  14  13  12   8  11  10  17  23  22  21  20  16  19  18  25
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 02/16 :    0   3  10  15  14  13  12   9   8  11  18  23  22  21  20  17  16  19  26  31
33: nodeai06:2504433:2505622 [1] NCCL INFO Trees [0] 34/-1/-1->33->32 [1] 34/17/41->33->1 [2] -1/-1/-1->33->32 [3] 34/-1/-1->33->32 [4] 34/-1/-1->33->32 [5] 34/-1/-1->33->32 [6] 34/-1/-1->33->32 [7] 34/-1/-1->33->32 [8] 34/-1/-1->33->32 [9] 34/-1/-1->33->25 [10] -1/-1/-1->33->32 [11] 34/-1/-1->33->32 [12] 34/-1/-1->33->32 [13] 34/-1/-1->33->32 [14] 34/-1/-1->33->32 [15] 34/-1/-1->33->32
33: nodeai06:2504433:2505622 [1] NCCL INFO P2P Chunksize set to 131072
41: nodeai07:2435735:2436921 [1] NCCL INFO Trees [0] 42/-1/-1->41->40 [1] 42/-1/-1->41->33 [2] -1/-1/-1->41->40 [3] 42/-1/-1->41->40 [4] 42/-1/-1->41->40 [5] 42/-1/-1->41->40 [6] 42/-1/-1->41->40 [7] 42/-1/-1->41->40 [8] 42/-1/-1->41->40 [9] 42/9/-1->41->-1 [10] -1/-1/-1->41->40 [11] 42/-1/-1->41->40 [12] 42/-1/-1->41->40 [13] 42/-1/-1->41->40 [14] 42/-1/-1->41->40 [15] 42/-1/-1->41->40
41: nodeai07:2435735:2436921 [1] NCCL INFO P2P Chunksize set to 131072
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 03/16 :    0  11  15  14  13  12  10   9   8  19  23  22  21  20  18  17  16  27  31  30
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 04/16 :    0   7   6   5  12  11  10   9   8  15  14  13  20  19  18  17  16  23  22  21
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 05/16 :    0   4   7   6  13  11  10   9   8  12  15  14  21  19  18  17  16  20  23  22
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 06/16 :    0   5   4   7  14  11  10   9   8  13  12  15  22  19  18  17  16  21  20  23
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 07/16 :    0   6   5   4  15  11  10   9   8  14  13  12  23  19  18  17  16  22  21  20
34: nodeai06:2504435:2505620 [2] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 35/-1/-1->34->33 [2] 35/18/42->34->2 [3] -1/-1/-1->34->33 [4] 35/-1/-1->34->33 [5] 35/-1/-1->34->33 [6] 35/-1/-1->34->33 [7] 35/-1/-1->34->33 [8] 35/-1/-1->34->33 [9] 35/-1/-1->34->33 [10] 35/-1/-1->34->26 [11] -1/-1/-1->34->33 [12] 35/-1/-1->34->33 [13] 35/-1/-1->34->33 [14] 35/-1/-1->34->33 [15] 35/-1/-1->34->33
34: nodeai06:2504435:2505620 [2] NCCL INFO P2P Chunksize set to 131072
42: nodeai07:2435730:2436923 [2] NCCL INFO Trees [0] 43/-1/-1->42->41 [1] 43/-1/-1->42->41 [2] 43/-1/-1->42->34 [3] -1/-1/-1->42->41 [4] 43/-1/-1->42->41 [5] 43/-1/-1->42->41 [6] 43/-1/-1->42->41 [7] 43/-1/-1->42->41 [8] 43/-1/-1->42->41 [9] 43/-1/-1->42->41 [10] 43/10/-1->42->-1 [11] -1/-1/-1->42->41 [12] 43/-1/-1->42->41 [13] 43/-1/-1->42->41 [14] 43/-1/-1->42->41 [15] 43/-1/-1->42->41
42: nodeai07:2435730:2436923 [2] NCCL INFO P2P Chunksize set to 131072
 1: nodeai01:2157663:2158848 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/33/-1->1->-1 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->9 [10] -1/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
 1: nodeai01:2157663:2158848 [1] NCCL INFO P2P Chunksize set to 131072
35: nodeai06:2504431:2505625 [3] NCCL INFO Trees [0] 36/-1/-1->35->34 [1] 36/-1/-1->35->34 [2] 36/-1/-1->35->34 [3] 36/19/43->35->3 [4] -1/-1/-1->35->34 [5] 36/-1/-1->35->34 [6] 36/-1/-1->35->34 [7] 36/-1/-1->35->34 [8] 36/-1/-1->35->34 [9] 36/-1/-1->35->34 [10] 36/-1/-1->35->34 [11] 36/-1/-1->35->27 [12] -1/-1/-1->35->34 [13] 36/-1/-1->35->34 [14] 36/-1/-1->35->34 [15] 36/-1/-1->35->34
35: nodeai06:2504431:2505625 [3] NCCL INFO P2P Chunksize set to 131072
47: nodeai07:2435736:2436920 [7] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] 40/-1/-1->47->46 [2] 40/-1/-1->47->46 [3] 40/-1/-1->47->46 [4] 40/-1/-1->47->46 [5] 40/-1/-1->47->46 [6] 40/-1/-1->47->46 [7] 40/-1/-1->47->39 [8] -1/-1/-1->47->46 [9] 40/-1/-1->47->46 [10] 40/-1/-1->47->46 [11] 40/-1/-1->47->46 [12] 40/-1/-1->47->46 [13] 40/-1/-1->47->46 [14] 40/-1/-1->47->46 [15] 40/15/-1->47->-1
47: nodeai07:2435736:2436920 [7] NCCL INFO P2P Chunksize set to 131072
 2: nodeai01:2157662:2158849 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/34/-1->2->-1 [3] -1/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->10 [11] -1/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
 2: nodeai01:2157662:2158849 [2] NCCL INFO P2P Chunksize set to 131072
36: nodeai06:2504434:2505624 [4] NCCL INFO Trees [0] 37/-1/-1->36->35 [1] 37/-1/-1->36->35 [2] 37/-1/-1->36->35 [3] 37/-1/-1->36->35 [4] 37/20/44->36->4 [5] -1/-1/-1->36->35 [6] 37/-1/-1->36->35 [7] 37/-1/-1->36->35 [8] 37/-1/-1->36->35 [9] 37/-1/-1->36->35 [10] 37/-1/-1->36->35 [11] 37/-1/-1->36->35 [12] 37/-1/-1->36->28 [13] -1/-1/-1->36->35 [14] 37/-1/-1->36->35 [15] 37/-1/-1->36->35
36: nodeai06:2504434:2505624 [4] NCCL INFO P2P Chunksize set to 131072
43: nodeai07:2435733:2436924 [3] NCCL INFO Trees [0] 44/-1/-1->43->42 [1] 44/-1/-1->43->42 [2] 44/-1/-1->43->42 [3] 44/-1/-1->43->35 [4] -1/-1/-1->43->42 [5] 44/-1/-1->43->42 [6] 44/-1/-1->43->42 [7] 44/-1/-1->43->42 [8] 44/-1/-1->43->42 [9] 44/-1/-1->43->42 [10] 44/-1/-1->43->42 [11] 44/11/-1->43->-1 [12] -1/-1/-1->43->42 [13] 44/-1/-1->43->42 [14] 44/-1/-1->43->42 [15] 44/-1/-1->43->42
43: nodeai07:2435733:2436924 [3] NCCL INFO P2P Chunksize set to 131072
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 3: nodeai01:2157667:2158850 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/35/-1->3->-1 [4] -1/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->11 [12] -1/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
 3: nodeai01:2157667:2158850 [3] NCCL INFO P2P Chunksize set to 131072
37: nodeai06:2504432:2505626 [5] NCCL INFO Trees [0] 38/-1/-1->37->36 [1] 38/-1/-1->37->36 [2] 38/-1/-1->37->36 [3] 38/-1/-1->37->36 [4] 38/-1/-1->37->36 [5] 38/21/45->37->5 [6] -1/-1/-1->37->36 [7] 38/-1/-1->37->36 [8] 38/-1/-1->37->36 [9] 38/-1/-1->37->36 [10] 38/-1/-1->37->36 [11] 38/-1/-1->37->36 [12] 38/-1/-1->37->36 [13] 38/-1/-1->37->29 [14] -1/-1/-1->37->36 [15] 38/-1/-1->37->36
37: nodeai06:2504432:2505626 [5] NCCL INFO P2P Chunksize set to 131072
44: nodeai07:2435729:2436925 [4] NCCL INFO Trees [0] 45/-1/-1->44->43 [1] 45/-1/-1->44->43 [2] 45/-1/-1->44->43 [3] 45/-1/-1->44->43 [4] 45/-1/-1->44->36 [5] -1/-1/-1->44->43 [6] 45/-1/-1->44->43 [7] 45/-1/-1->44->43 [8] 45/-1/-1->44->43 [9] 45/-1/-1->44->43 [10] 45/-1/-1->44->43 [11] 45/-1/-1->44->43 [12] 45/12/-1->44->-1 [13] -1/-1/-1->44->43 [14] 45/-1/-1->44->43 [15] 45/-1/-1->44->43
44: nodeai07:2435729:2436925 [4] NCCL INFO P2P Chunksize set to 131072
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 4: nodeai01:2157666:2158845 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/36/-1->4->-1 [5] -1/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->12 [13] -1/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
 4: nodeai01:2157666:2158845 [4] NCCL INFO P2P Chunksize set to 131072
38: nodeai06:2504437:2505621 [6] NCCL INFO Trees [0] 39/-1/-1->38->37 [1] 39/-1/-1->38->37 [2] 39/-1/-1->38->37 [3] 39/-1/-1->38->37 [4] 39/-1/-1->38->37 [5] 39/-1/-1->38->37 [6] 39/22/46->38->6 [7] -1/-1/-1->38->37 [8] 39/-1/-1->38->37 [9] 39/-1/-1->38->37 [10] 39/-1/-1->38->37 [11] 39/-1/-1->38->37 [12] 39/-1/-1->38->37 [13] 39/-1/-1->38->37 [14] 39/-1/-1->38->30 [15] -1/-1/-1->38->37
38: nodeai06:2504437:2505621 [6] NCCL INFO P2P Chunksize set to 131072
45: nodeai07:2435732:2436926 [5] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 46/-1/-1->45->44 [2] 46/-1/-1->45->44 [3] 46/-1/-1->45->44 [4] 46/-1/-1->45->44 [5] 46/-1/-1->45->37 [6] -1/-1/-1->45->44 [7] 46/-1/-1->45->44 [8] 46/-1/-1->45->44 [9] 46/-1/-1->45->44 [10] 46/-1/-1->45->44 [11] 46/-1/-1->45->44 [12] 46/-1/-1->45->44 [13] 46/13/-1->45->-1 [14] -1/-1/-1->45->44 [15] 46/-1/-1->45->44
45: nodeai07:2435732:2436926 [5] NCCL INFO P2P Chunksize set to 131072
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 5: nodeai01:2157665:2158846 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/37/-1->5->-1 [6] -1/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->13 [14] -1/-1/-1->5->4 [15] 6/-1/-1->5->4
 5: nodeai01:2157665:2158846 [5] NCCL INFO P2P Chunksize set to 131072
39: nodeai06:2504430:2505623 [7] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] 32/-1/-1->39->38 [2] 32/-1/-1->39->38 [3] 32/-1/-1->39->38 [4] 32/-1/-1->39->38 [5] 32/-1/-1->39->38 [6] 32/-1/-1->39->38 [7] 32/23/47->39->7 [8] -1/-1/-1->39->38 [9] 32/-1/-1->39->38 [10] 32/-1/-1->39->38 [11] 32/-1/-1->39->38 [12] 32/-1/-1->39->38 [13] 32/-1/-1->39->38 [14] 32/-1/-1->39->38 [15] 32/-1/-1->39->31
39: nodeai06:2504430:2505623 [7] NCCL INFO P2P Chunksize set to 131072
46: nodeai07:2435731:2436922 [6] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 47/-1/-1->46->45 [2] 47/-1/-1->46->45 [3] 47/-1/-1->46->45 [4] 47/-1/-1->46->45 [5] 47/-1/-1->46->45 [6] 47/-1/-1->46->38 [7] -1/-1/-1->46->45 [8] 47/-1/-1->46->45 [9] 47/-1/-1->46->45 [10] 47/-1/-1->46->45 [11] 47/-1/-1->46->45 [12] 47/-1/-1->46->45 [13] 47/-1/-1->46->45 [14] 47/14/-1->46->-1 [15] -1/-1/-1->46->45
46: nodeai07:2435731:2436922 [6] NCCL INFO P2P Chunksize set to 131072
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 6: nodeai01:2157661:2158844 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/38/-1->6->-1 [7] -1/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->14 [15] -1/-1/-1->6->5
 6: nodeai01:2157661:2158844 [6] NCCL INFO P2P Chunksize set to 131072
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 7: nodeai01:2157664:2158847 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] 0/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/39/-1->7->-1 [8] -1/-1/-1->7->6 [9] 0/-1/-1->7->6 [10] 0/-1/-1->7->6 [11] 0/-1/-1->7->6 [12] 0/-1/-1->7->6 [13] 0/-1/-1->7->6 [14] 0/-1/-1->7->6 [15] 0/-1/-1->7->15
 7: nodeai01:2157664:2158847 [7] NCCL INFO P2P Chunksize set to 131072
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 08/16 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 09/16 :    0   3   2   9  15  14  13  12   8  11  10  17  23  22  21  20  16  19  18  25
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 10/16 :    0   3  10  15  14  13  12   9   8  11  18  23  22  21  20  17  16  19  26  31
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 11/16 :    0  11  15  14  13  12  10   9   8  19  23  22  21  20  18  17  16  27  31  30
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 12/16 :    0   7   6   5  12  11  10   9   8  15  14  13  20  19  18  17  16  23  22  21
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 13/16 :    0   4   7   6  13  11  10   9   8  12  15  14  21  19  18  17  16  20  23  22
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 14/16 :    0   5   4   7  14  11  10   9   8  13  12  15  22  19  18  17  16  21  20  23
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 15/16 :    0   6   5   4  15  11  10   9   8  14  13  12  23  19  18  17  16  22  21  20
 0: nodeai01:2157660:2158843 [0] NCCL INFO Trees [0] 1/32/-1->0->-1 [1] -1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->7 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7 [8] 1/-1/-1->0->8 [9] -1/-1/-1->0->7 [10] 1/-1/-1->0->7 [11] 1/-1/-1->0->7 [12] 1/-1/-1->0->7 [13] 1/-1/-1->0->7 [14] 1/-1/-1->0->7 [15] 1/-1/-1->0->7
 0: nodeai01:2157660:2158843 [0] NCCL INFO P2P Chunksize set to 131072
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 15[7] -> 16[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 15[7] -> 16[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 17[1] -> 18[2] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 00/0 : 29[5] -> 30[6] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 08/0 : 14[6] -> 15[7] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 22[6] -> 23[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 00/0 : 23[7] -> 24[0] [receive] via NET/IBext/0/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 00/0 : 42[2] -> 43[3] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 23[7] -> 24[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 00/0 : 43[3] -> 44[4] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 08/0 : 13[5] -> 14[6] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 34[2] -> 35[3] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 00/0 : 47[7] -> 0[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 00/0 : 15[7] -> 16[0] [send] via NET/IBext/0(8)/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 20[4] -> 21[5] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 17[1] -> 18[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 08/0 : 47[7] -> 0[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 08/0 : 12[4] -> 13[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [receive] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 19[3] -> 20[4] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 00/0 : 39[7] -> 40[0] [receive] via NET/IBext/0/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 00/0 : 28[4] -> 29[5] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 00/0 : 30[6] -> 31[7] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 00/0 : 31[7] -> 32[0] [send] via NET/IBext/0(24)/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 18[2] -> 19[3] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 08/0 : 15[7] -> 16[0] [send] via NET/IBext/0(8)/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 21[5] -> 22[6] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 7[7] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 00/0 : 45[5] -> 46[6] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 31[7] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 08/0 : 39[7] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 08/0 : 25[1] -> 26[2] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 33[1] -> 34[2] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 16[0] -> 17[1] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 00/0 : 46[6] -> 47[7] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 00/0 : 27[3] -> 28[4] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 00/0 : 44[4] -> 45[5] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 08/0 : 29[5] -> 30[6] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 31[7] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[1] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 35[3] -> 36[4] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 37[5] -> 38[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 08/0 : 42[2] -> 43[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 36[4] -> 37[5] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 08/0 : 11[3] -> 12[4] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 08/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 00/0 : 41[1] -> 42[2] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 34[2] -> 35[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 38[6] -> 39[7] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 22[6] -> 23[7] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 08/0 : 28[4] -> 29[5] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 08/0 : 26[2] -> 27[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 20[4] -> 21[5] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 08/0 : 10[2] -> 11[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 08/0 : 30[6] -> 31[7] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 08/0 : 9[1] -> 10[2] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 19[3] -> 20[4] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 21[5] -> 22[6] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 33[1] -> 34[2] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 08/0 : 27[3] -> 28[4] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 08/0 : 46[6] -> 47[7] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 35[3] -> 36[4] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 08/0 : 44[4] -> 45[5] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 24[0] -> 25[1] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 23[7] -> 24[0] [send] via NET/IBext/0(16)/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 37[5] -> 38[6] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 08/0 : 31[7] -> 32[0] [send] via NET/IBext/0(24)/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 8[0] -> 9[1] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 36[4] -> 37[5] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 08/0 : 45[5] -> 46[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 23[7] -> 24[0] [send] via NET/IBext/0(16)/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 12[4] -> 15[7] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 17[1] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 01/0 : 16[0] -> 19[3] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 08/0 : 41[1] -> 42[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 18[2] -> 23[7] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 38[6] -> 39[7] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 05/0 : 20[4] -> 23[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 08/0 : 40[0] -> 41[1] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 02/0 : 26[2] -> 31[7] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 00/0 : 47[7] -> 0[0] [send] via NET/IBext/0(40)/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 09/0 : 17[1] -> 23[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 32[0] -> 33[1] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 16[0] -> 19[3] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 02/0 : 10[2] -> 15[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 28[4] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 25[1] -> 31[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 24[0] -> 27[3] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 34[2] -> 39[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 05/0 : 44[4] -> 47[7] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 08/0 : 47[7] -> 0[0] [send] via NET/IBext/0(40)/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 02/0 : 42[2] -> 47[7] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [send] via NET/IBext/0(0)/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 06/0 : 12[4] -> 15[7] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 18[2] -> 23[7] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 9[1] -> 15[7] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 26[2] -> 31[7] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 05/0 : 36[4] -> 39[7] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 02/0 : 2[2] -> 7[7] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 08/0 : 7[7] -> 8[0] [send] via NET/IBext/0(0)/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 01/0 : 1[1] -> 7[7] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 34[2] -> 39[7] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 39[7] -> 40[0] [send] via NET/IBext/0(32)/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 01/0 : 41[1] -> 47[7] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 10[2] -> 15[7] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 39[7] -> 40[0] [send] via NET/IBext/0(32)/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 20[4] -> 23[7] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 33[1] -> 39[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 01/0 : 32[0] -> 35[3] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 06/0 : 44[4] -> 47[7] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 10/0 : 2[2] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 06/0 : 28[4] -> 31[7] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 10/0 : 42[2] -> 47[7] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 09/0 : 1[1] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 8[0] -> 11[3] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 05/0 : 4[4] -> 7[7] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 36[4] -> 39[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 25[1] -> 31[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 02/0 : 24[0] -> 27[3] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 09/0 : 16[0] -> 19[3] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 01/0 : 40[0] -> 43[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 12[4] -> 15[7] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 9[1] -> 15[7] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 09/0 : 41[1] -> 47[7] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 13/0 : 20[4] -> 23[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 28[4] -> 31[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 14/0 : 12[4] -> 15[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 13/0 : 44[4] -> 47[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 32[0] -> 35[3] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 09/0 : 33[1] -> 39[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 24[0] -> 27[3] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 16[0] -> 19[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 13/0 : 36[4] -> 39[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 02/0 : 40[0] -> 43[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 20[4] -> 23[7] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 06/0 : 4[4] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 14/0 : 28[4] -> 31[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 8[0] -> 11[3] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 10/0 : 24[0] -> 27[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 10/0 : 8[0] -> 11[3] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 09/0 : 32[0] -> 35[3] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 14/0 : 44[4] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 09/0 : 40[0] -> 43[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 36[4] -> 39[7] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 13/0 : 4[4] -> 7[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[4] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 27[3] -> 31[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[4] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 19[3] -> 23[7] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 32[0] -> 35[3] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 10/0 : 40[0] -> 43[3] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 11[3] -> 15[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 14/0 : 4[4] -> 7[7] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 27[3] -> 31[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 13/0 : 16[0] -> 20[4] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 35[3] -> 39[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 24[0] -> 28[4] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 05/0 : 32[0] -> 36[4] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 11/0 : 19[3] -> 23[7] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 03/0 : 43[3] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 05/0 : 40[0] -> 44[4] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[4] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 11[3] -> 15[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 8[0] -> 12[4] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 11/0 : 35[3] -> 39[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 13/0 : 32[0] -> 36[4] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 13/0 : 0[0] -> 4[4] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 11/0 : 3[3] -> 7[7] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 11/0 : 43[3] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 13/0 : 40[0] -> 44[4] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 06/0 : 24[0] -> 29[5] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 16[0] -> 21[5] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 06/0 : 8[0] -> 13[5] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 14/0 : 24[0] -> 29[5] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 16[0] -> 21[5] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 14/0 : 8[0] -> 13[5] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 06/0 : 40[0] -> 45[5] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 32[0] -> 37[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 06/0 : 0[0] -> 5[5] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 24[0] -> 30[6] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 14/0 : 40[0] -> 45[5] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 32[0] -> 37[5] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 07/0 : 16[0] -> 22[6] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 8[0] -> 14[6] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 14/0 : 0[0] -> 5[5] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 24[0] -> 30[6] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 15/0 : 16[0] -> 22[6] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 8[0] -> 14[6] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 07/0 : 40[0] -> 46[6] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 07/0 : 32[0] -> 38[6] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 07/0 : 0[0] -> 6[6] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 15/0 : 32[0] -> 38[6] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 15/0 : 40[0] -> 46[6] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 04/0 : 24[0] -> 31[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 16[0] -> 23[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 04/0 : 8[0] -> 15[7] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 15/0 : 0[0] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 12/0 : 24[0] -> 31[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 16[0] -> 23[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 32[0] -> 39[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 12/0 : 8[0] -> 15[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 04/0 : 40[0] -> 47[7] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 32[0] -> 39[7] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 12/0 : 0[0] -> 7[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 12/0 : 40[0] -> 47[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 04/0 : 21[5] -> 28[4] [receive] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 19[3] -> 26[2] [send] via NET/IBext/2(18)/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 13[5] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 11[3] -> 18[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 02/0 : 11[3] -> 18[2] [send] via NET/IBext/2(10)/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 02/0 : 27[3] -> 34[2] [send] via NET/IBext/2(26)/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 13[5] -> 20[4] [receive] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [receive] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 21[5] -> 28[4] [receive] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 19[3] -> 26[2] [send] via NET/IBext/2(18)/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 02/0 : 19[3] -> 26[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 11[3] -> 18[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 10/0 : 11[3] -> 18[2] [send] via NET/IBext/2(10)/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 10/0 : 27[3] -> 34[2] [send] via NET/IBext/2(26)/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 19[3] -> 26[2] [receive] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [receive] via NET/IBext/4/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 02/0 : 43[3] -> 2[2] [send] via NET/IBext/2(42)/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [receive] via NET/IBext/4/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 02/0 : 35[3] -> 42[2] [receive] via NET/IBext/2/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 04/0 : 37[5] -> 44[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 35[3] -> 42[2] [send] via NET/IBext/2(34)/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 10/0 : 43[3] -> 2[2] [send] via NET/IBext/2(42)/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 29[5] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 27[3] -> 34[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 10/0 : 35[3] -> 42[2] [receive] via NET/IBext/2/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [send] via NET/IBext/2(2)/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 12/0 : 37[5] -> 44[4] [receive] via NET/IBext/4/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 02/0 : 43[3] -> 2[2] [receive] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 35[3] -> 42[2] [send] via NET/IBext/2(34)/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 29[5] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 04/0 : 45[5] -> 4[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 27[3] -> 34[2] [receive] via NET/IBext/2/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [send] via NET/IBext/2(2)/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 10/0 : 43[3] -> 2[2] [receive] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 22[6] -> 29[5] [receive] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 12/0 : 45[5] -> 4[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 22[6] -> 29[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 14[6] -> 21[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 13/0 : 14[6] -> 21[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 04/0 : 29[5] -> 36[4] [send] via NET/IBext/4(28)/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 21[5] -> 28[4] [send] via NET/IBext/4(20)/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 12/0 : 29[5] -> 36[4] [send] via NET/IBext/4(28)/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 04/0 : 13[5] -> 20[4] [send] via NET/IBext/4(12)/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 21[5] -> 28[4] [send] via NET/IBext/4(20)/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 05/0 : 38[6] -> 45[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 12/0 : 13[5] -> 20[4] [send] via NET/IBext/4(12)/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 30[6] -> 37[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 13/0 : 38[6] -> 45[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 13/0 : 30[6] -> 37[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 06/0 : 23[7] -> 30[6] [receive] via NET/IBext/6/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 04/0 : 45[5] -> 4[4] [send] via NET/IBext/4(44)/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 06/0 : 31[7] -> 38[6] [send] via NET/IBext/6(30)/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 37[5] -> 44[4] [send] via NET/IBext/4(36)/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 23[7] -> 30[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 10[2] -> 17[1] [receive] via NET/IBext/1/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 12/0 : 45[5] -> 4[4] [send] via NET/IBext/4(44)/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 05/0 : 46[6] -> 5[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 15[7] -> 22[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 01/0 : 18[2] -> 25[1] [send] via NET/IBext/1(17)/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 18[2] -> 25[1] [receive] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 37[5] -> 44[4] [send] via NET/IBext/4(36)/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [receive] via NET/IBext/1/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 14/0 : 31[7] -> 38[6] [send] via NET/IBext/6(30)/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 26[2] -> 33[1] [send] via NET/IBext/1(25)/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 23[7] -> 30[6] [send] via NET/IBext/6(22)/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 10[2] -> 17[1] [send] via NET/IBext/1(9)/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 09/0 : 10[2] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 13/0 : 46[6] -> 5[5] [receive] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 09/0 : 18[2] -> 25[1] [send] via NET/IBext/1(17)/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 06/0 : 15[7] -> 22[6] [send] via NET/IBext/6(14)/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 18[2] -> 25[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 26[2] -> 33[1] [send] via NET/IBext/1(25)/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 15[7] -> 22[6] [receive] via NET/IBext/6/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 10[2] -> 17[1] [send] via NET/IBext/1(9)/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 30[6] -> 37[5] [send] via NET/IBext/5(29)/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 23[7] -> 30[6] [send] via NET/IBext/6(22)/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 14/0 : 15[7] -> 22[6] [send] via NET/IBext/6(14)/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 01/0 : 34[2] -> 41[1] [receive] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [send] via NET/IBext/4(4)/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 30[6] -> 37[5] [send] via NET/IBext/5(29)/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 01/0 : 42[2] -> 1[1] [send] via NET/IBext/1(41)/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 05/0 : 22[6] -> 29[5] [send] via NET/IBext/5(21)/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 09/0 : 34[2] -> 41[1] [receive] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [send] via NET/IBext/4(4)/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 09/0 : 42[2] -> 1[1] [send] via NET/IBext/1(41)/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 26[2] -> 33[1] [receive] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 14[6] -> 21[5] [send] via NET/IBext/5(13)/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 01/0 : 34[2] -> 41[1] [send] via NET/IBext/1(33)/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 01/0 : 42[2] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 06/0 : 47[7] -> 6[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 14[6] -> 21[5] [send] via NET/IBext/5(13)/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [send] via NET/IBext/1(1)/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 09/0 : 26[2] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [send] via NET/IBext/6(6)/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 13/0 : 22[6] -> 29[5] [send] via NET/IBext/5(21)/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 09/0 : 34[2] -> 41[1] [send] via NET/IBext/1(33)/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 09/0 : 42[2] -> 1[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 31[7] -> 38[6] [receive] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 14/0 : 47[7] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [send] via NET/IBext/1(1)/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 39[7] -> 46[6] [send] via NET/IBext/6(38)/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [send] via NET/IBext/6(6)/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 06/0 : 39[7] -> 46[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 31[7] -> 38[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 39[7] -> 46[6] [send] via NET/IBext/6(38)/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 06/0 : 47[7] -> 6[6] [send] via NET/IBext/6(46)/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [send] via NET/IBext/5(5)/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 14/0 : 39[7] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 14/0 : 47[7] -> 6[6] [send] via NET/IBext/6(46)/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [send] via NET/IBext/5(5)/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 05/0 : 38[6] -> 45[5] [send] via NET/IBext/5(37)/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 29[5] -> 27[3] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 22[6] -> 19[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 13/0 : 38[6] -> 45[5] [send] via NET/IBext/5(37)/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 05/0 : 46[6] -> 5[5] [send] via NET/IBext/5(45)/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 13/0 : 46[6] -> 5[5] [send] via NET/IBext/5(45)/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 06/0 : 30[6] -> 27[3] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 29[5] -> 27[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 13[5] -> 11[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 06/0 : 6[6] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 22[6] -> 19[3] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 37[5] -> 35[3] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 06/0 : 14[6] -> 11[3] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 21[5] -> 19[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 30[6] -> 27[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 13[5] -> 11[3] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 13/0 : 37[5] -> 35[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 14/0 : 6[6] -> 3[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 05/0 : 45[5] -> 43[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 38[6] -> 35[3] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 13/0 : 21[5] -> 19[3] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 14[6] -> 11[3] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 05/0 : 5[5] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 06/0 : 46[6] -> 43[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 38[6] -> 35[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 13/0 : 45[5] -> 43[3] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 13/0 : 5[5] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 28[4] -> 39[7] [send] via NET/IBext/7(31)/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 20[4] -> 31[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 07/0 : 36[4] -> 47[7] [send] via NET/IBext/7(39)/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 14/0 : 46[6] -> 43[3] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 28[4] -> 39[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 28[4] -> 39[7] [send] via NET/IBext/7(31)/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 20[4] -> 31[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 15/0 : 36[4] -> 47[7] [send] via NET/IBext/7(39)/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 15/0 : 28[4] -> 39[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 24[0] -> 35[3] [send] via NET/IBext/3(27)/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 16[0] -> 27[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 03/0 : 40[0] -> 3[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 24[0] -> 35[3] [send] via NET/IBext/3(27)/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 03/0 : 0[0] -> 11[3] [send] via NET/IBext/3(3)/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 16[0] -> 27[3] [receive] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 07/0 : 44[4] -> 7[7] [send] via NET/IBext/7(47)/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 11/0 : 40[0] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 11/0 : 0[0] -> 11[3] [send] via NET/IBext/3(3)/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 07/0 : 36[4] -> 47[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 8[0] -> 19[3] [send] via NET/IBext/3(11)/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 03/0 : 16[0] -> 27[3] [send] via NET/IBext/3(19)/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 8[0] -> 19[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 0[0] -> 11[3] [receive] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 15/0 : 44[4] -> 7[7] [send] via NET/IBext/7(47)/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 15/0 : 36[4] -> 47[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 11/0 : 16[0] -> 27[3] [send] via NET/IBext/3(19)/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 8[0] -> 19[3] [send] via NET/IBext/3(11)/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 0[0] -> 11[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 11/0 : 8[0] -> 19[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 07/0 : 20[4] -> 31[7] [send] via NET/IBext/7(23)/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 12[4] -> 23[7] [receive] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 15/0 : 20[4] -> 31[7] [send] via NET/IBext/7(23)/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 15/0 : 12[4] -> 23[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 4[4] -> 15[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 24[0] -> 35[3] [receive] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 03/0 : 32[0] -> 43[3] [send] via NET/IBext/3(35)/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 12[4] -> 23[7] [send] via NET/IBext/7(15)/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 4[4] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 07/0 : 4[4] -> 15[7] [send] via NET/IBext/7(7)/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 28[4] -> 24[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 39[7] -> 35[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 07/0 : 47[7] -> 43[3] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 11/0 : 24[0] -> 35[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 01/0 : 20[4] -> 16[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 01/0 : 36[4] -> 32[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 31[7] -> 27[3] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 11/0 : 32[0] -> 43[3] [send] via NET/IBext/3(35)/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 12[4] -> 23[7] [send] via NET/IBext/7(15)/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 07/0 : 44[4] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 15/0 : 4[4] -> 15[7] [send] via NET/IBext/7(7)/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 15/0 : 44[4] -> 7[7] [receive] via NET/IBext/7/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 03/0 : 32[0] -> 43[3] [receive] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 03/0 : 40[0] -> 3[3] [send] via NET/IBext/3(43)/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 11/0 : 32[0] -> 43[3] [receive] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 11/0 : 40[0] -> 3[3] [send] via NET/IBext/3(43)/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 23[7] -> 19[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 12[4] -> 8[0] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 15/0 : 47[7] -> 43[3] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 15/0 : 39[7] -> 35[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 09/0 : 20[4] -> 16[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 09/0 : 36[4] -> 32[0] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 15[7] -> 11[3] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 28[4] -> 24[0] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 01/0 : 4[4] -> 0[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 31[7] -> 27[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 12[4] -> 8[0] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 15/0 : 23[7] -> 19[3] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 09/0 : 4[4] -> 0[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 07/0 : 7[7] -> 3[3] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 01/0 : 44[4] -> 40[0] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 15[7] -> 11[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 20[4] -> 17[1] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 01/0 : 39[7] -> 38[6] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 31[7] -> 30[6] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 02/0 : 28[4] -> 25[1] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 02/0 : 12[4] -> 9[1] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 02/0 : 4[4] -> 1[1] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 09/0 : 44[4] -> 40[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 15/0 : 7[7] -> 3[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 36[4] -> 33[1] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 01/0 : 23[7] -> 22[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 39[7] -> 38[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 20[4] -> 17[1] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 02/0 : 31[7] -> 30[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 01/0 : 47[7] -> 46[6] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 10/0 : 12[4] -> 9[1] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 02/0 : 44[4] -> 41[1] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 10/0 : 28[4] -> 25[1] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 10/0 : 4[4] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 36[4] -> 33[1] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 31[7] -> 30[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 02/0 : 15[7] -> 14[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 23[7] -> 22[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 03/0 : 39[7] -> 38[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 02/0 : 47[7] -> 46[6] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 03/0 : 20[4] -> 18[2] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 12[4] -> 10[2] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 10/0 : 44[4] -> 41[1] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 28[4] -> 26[2] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 03/0 : 36[4] -> 34[2] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 03/0 : 4[4] -> 2[2] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 04/0 : 31[7] -> 30[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 03/0 : 23[7] -> 22[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 03/0 : 47[7] -> 46[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 15[7] -> 14[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 39[7] -> 38[6] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 11/0 : 20[4] -> 18[2] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 04/0 : 47[7] -> 46[6] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 12[4] -> 10[2] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 23[7] -> 22[6] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 03/0 : 44[4] -> 42[2] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 28[4] -> 26[2] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 11/0 : 4[4] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 11/0 : 36[4] -> 34[2] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 31[7] -> 30[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 04/0 : 15[7] -> 14[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 05/0 : 39[7] -> 38[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 05/0 : 47[7] -> 46[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 05/0 : 23[7] -> 22[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 15[7] -> 14[6] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 31[7] -> 30[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 09/0 : 47[7] -> 46[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 09/0 : 39[7] -> 38[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 10/0 : 47[7] -> 46[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 09/0 : 23[7] -> 22[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 11/0 : 47[7] -> 46[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 15[7] -> 14[6] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 10/0 : 31[7] -> 30[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 39[7] -> 38[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 12/0 : 47[7] -> 46[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 23[7] -> 22[6] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 01/0 : 35[3] -> 34[2] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 13/0 : 47[7] -> 46[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 10/0 : 15[7] -> 14[6] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 31[7] -> 30[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 11/0 : 39[7] -> 38[6] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 27[3] -> 26[2] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 04/0 : 35[3] -> 34[2] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 11/0 : 23[7] -> 22[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 15[7] -> 14[6] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 01/0 : 19[3] -> 18[2] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 12/0 : 31[7] -> 30[6] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 39[7] -> 38[6] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 04/0 : 27[3] -> 26[2] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 05/0 : 35[3] -> 34[2] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 23[7] -> 22[6] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 01/0 : 46[6] -> 45[5] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 02/0 : 41[1] -> 40[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 12/0 : 15[7] -> 14[6] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 02/0 : 46[6] -> 45[5] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 04/0 : 19[3] -> 18[2] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 03/0 : 41[1] -> 40[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 31[7] -> 30[6] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 13/0 : 39[7] -> 38[6] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 03/0 : 46[6] -> 45[5] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 27[3] -> 26[2] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 35[3] -> 34[2] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 04/0 : 41[1] -> 40[0] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 04/0 : 46[6] -> 45[5] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 13/0 : 23[7] -> 22[6] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 05/0 : 41[1] -> 40[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 07/0 : 46[6] -> 45[5] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 15[7] -> 14[6] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 06/0 : 41[1] -> 40[0] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 05/0 : 19[3] -> 18[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 09/0 : 46[6] -> 45[5] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 07/0 : 41[1] -> 40[0] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 06/0 : 27[3] -> 26[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 10/0 : 46[6] -> 45[5] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 07/0 : 35[3] -> 34[2] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 10/0 : 41[1] -> 40[0] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 11/0 : 46[6] -> 45[5] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 11/0 : 41[1] -> 40[0] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 12/0 : 46[6] -> 45[5] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 19[3] -> 18[2] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 12/0 : 41[1] -> 40[0] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 27[3] -> 26[2] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 11[3] -> 10[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 15/0 : 46[6] -> 45[5] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 09/0 : 35[3] -> 34[2] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 13/0 : 41[1] -> 40[0] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 14/0 : 41[1] -> 40[0] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 12/0 : 11[3] -> 10[2] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 07/0 : 19[3] -> 18[2] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 02/0 : 17[1] -> 16[0] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 27[3] -> 26[2] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 01/0 : 38[6] -> 37[5] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 12/0 : 35[3] -> 34[2] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 11[3] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 03/0 : 17[1] -> 16[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 09/0 : 19[3] -> 18[2] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 30[6] -> 29[5] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 02/0 : 33[1] -> 32[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 02/0 : 14[6] -> 13[5] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 12/0 : 27[3] -> 26[2] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 38[6] -> 37[5] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 01/0 : 22[6] -> 21[5] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 13/0 : 35[3] -> 34[2] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 14/0 : 11[3] -> 10[2] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 02/0 : 30[6] -> 29[5] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 17[1] -> 16[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 12/0 : 19[3] -> 18[2] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 14[6] -> 13[5] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 02/0 : 13[5] -> 12[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 01/0 : 37[5] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 03/0 : 38[6] -> 37[5] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 27[3] -> 26[2] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 11[3] -> 10[2] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 04/0 : 28[4] -> 27[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 20[4] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 03/0 : 18[2] -> 17[1] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 03/0 : 33[1] -> 32[0] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 35[3] -> 34[2] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 04/0 : 25[1] -> 24[0] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 22[6] -> 21[5] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 03/0 : 26[2] -> 25[1] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 36[4] -> 35[3] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 03/0 : 34[2] -> 33[1] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 30[6] -> 29[5] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 05/0 : 17[1] -> 16[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 13/0 : 19[3] -> 18[2] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 04/0 : 14[6] -> 13[5] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 29[5] -> 28[4] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 13[5] -> 12[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 37[5] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 38[6] -> 37[5] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 20[4] -> 19[3] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 14/0 : 27[3] -> 26[2] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 33[1] -> 32[0] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 28[4] -> 27[3] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 25[1] -> 24[0] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 15/0 : 35[3] -> 34[2] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 18[2] -> 17[1] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 06/0 : 10[2] -> 9[1] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 03/0 : 22[6] -> 21[5] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 01/0 : 21[5] -> 20[4] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 07/0 : 14[6] -> 13[5] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 36[4] -> 35[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 04/0 : 30[6] -> 29[5] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 06/0 : 13[5] -> 12[4] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 17[1] -> 16[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 19[3] -> 18[2] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 10/0 : 9[1] -> 8[0] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 34[2] -> 33[1] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 04/0 : 26[2] -> 25[1] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 02/0 : 29[5] -> 28[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 03/0 : 37[5] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 07/0 : 38[6] -> 37[5] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 10[2] -> 9[1] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 05/0 : 33[1] -> 32[0] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 27[3] -> 26[2] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 06/0 : 25[1] -> 24[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 14[6] -> 13[5] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 05/0 : 18[2] -> 17[1] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 13[5] -> 12[4] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 9[1] -> 8[0] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 22[6] -> 21[5] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 21[5] -> 20[4] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 07/0 : 30[6] -> 29[5] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 07/0 : 17[1] -> 16[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 15/0 : 19[3] -> 18[2] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 11/0 : 10[2] -> 9[1] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 26[2] -> 25[1] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 05/0 : 34[2] -> 33[1] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 29[5] -> 28[4] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 10/0 : 14[6] -> 13[5] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 18[2] -> 17[1] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 06/0 : 37[5] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 09/0 : 38[6] -> 37[5] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 33[1] -> 32[0] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 13[5] -> 12[4] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 12/0 : 9[1] -> 8[0] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 34[2] -> 33[1] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 25[1] -> 24[0] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 07/0 : 22[6] -> 21[5] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 30[6] -> 29[5] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 03/0 : 21[5] -> 20[4] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 12/0 : 10[2] -> 9[1] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 06/0 : 29[5] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 06/0 : 26[2] -> 25[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 10/0 : 17[1] -> 16[0] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 14[6] -> 13[5] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 10/0 : 13[5] -> 12[4] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 07/0 : 37[5] -> 36[4] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 07/0 : 33[1] -> 32[0] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 38[6] -> 37[5] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 07/0 : 18[2] -> 17[1] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 9[1] -> 8[0] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 07/0 : 34[2] -> 33[1] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 10/0 : 30[6] -> 29[5] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 10[2] -> 9[1] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 09/0 : 22[6] -> 21[5] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 10/0 : 25[1] -> 24[0] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 06/0 : 21[5] -> 20[4] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 29[5] -> 28[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 09/0 : 37[5] -> 36[4] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 12/0 : 14[6] -> 13[5] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 26[2] -> 25[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 11/0 : 17[1] -> 16[0] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 10/0 : 33[1] -> 32[0] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 13[5] -> 12[4] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 11/0 : 18[2] -> 17[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 11/0 : 38[6] -> 37[5] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 14/0 : 9[1] -> 8[0] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 11/0 : 34[2] -> 33[1] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 30[6] -> 29[5] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 14/0 : 10[2] -> 9[1] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 22[6] -> 21[5] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 07/0 : 21[5] -> 20[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 37[5] -> 36[4] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 25[1] -> 24[0] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 29[5] -> 28[4] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 11/0 : 33[1] -> 32[0] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 11/0 : 26[2] -> 25[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 17[1] -> 16[0] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 18[2] -> 17[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 15/0 : 14[6] -> 13[5] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 12/0 : 30[6] -> 29[5] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 38[6] -> 37[5] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 15/0 : 41[1] -> 40[0] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 34[2] -> 33[1] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 11/0 : 22[6] -> 21[5] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 14/0 : 13[5] -> 12[4] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 09/0 : 21[5] -> 20[4] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 11/0 : 37[5] -> 36[4] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 9[1] -> 8[0] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 12/0 : 25[1] -> 24[0] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 33[1] -> 32[0] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 10/0 : 29[5] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 12/0 : 26[2] -> 25[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 13/0 : 17[1] -> 16[0] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 15/0 : 30[6] -> 29[5] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 13/0 : 34[2] -> 33[1] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 15/0 : 38[6] -> 37[5] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 10[2] -> 9[1] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 22[6] -> 21[5] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 21[5] -> 20[4] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 13/0 : 18[2] -> 17[1] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 14/0 : 37[5] -> 36[4] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 25[1] -> 24[0] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 17[1] -> 16[0] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 13/0 : 33[1] -> 32[0] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 29[5] -> 28[4] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 26[2] -> 25[1] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 34[2] -> 33[1] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 13[5] -> 12[4] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 15/0 : 37[5] -> 36[4] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 33[1] -> 32[0] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 14/0 : 25[1] -> 24[0] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 11/0 : 21[5] -> 20[4] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 15/0 : 22[6] -> 21[5] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 18[2] -> 17[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 15/0 : 17[1] -> 16[0] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 14/0 : 21[5] -> 20[4] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 15/0 : 18[2] -> 17[1] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 11/0 : 44[4] -> 42[2] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 14/0 : 29[5] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 14/0 : 26[2] -> 25[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 25[1] -> 24[0] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 29[5] -> 28[4] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 15/0 : 34[2] -> 33[1] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 15/0 : 33[1] -> 32[0] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 26[2] -> 25[1] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 01/0 : 43[3] -> 42[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 15/0 : 21[5] -> 20[4] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 04/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 05/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 06/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 07/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 09/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 12/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 13/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 14/0 : 43[3] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 15/0 : 43[3] -> 42[2] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 03/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 01/0 : 45[5] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 04/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 02/0 : 45[5] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 05/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 03/0 : 45[5] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 06/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 06/0 : 45[5] -> 44[4] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 04/0 : 44[4] -> 43[3] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 07/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 07/0 : 45[5] -> 44[4] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 12/0 : 44[4] -> 43[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 04/0 : 12[4] -> 11[3] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 11/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 09/0 : 45[5] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 12/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 10/0 : 45[5] -> 44[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 12[4] -> 11[3] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 13/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 11/0 : 45[5] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 14/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 14/0 : 45[5] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 15/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 15/0 : 45[5] -> 44[4] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Connected all rings
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 04/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 05/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 06/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 07/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 10/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 11/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 12/0 : 40[0] -> 41[1] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Connected all rings
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 13/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 14/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 15/0 : 40[0] -> 41[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Connected all rings
21: nodeai04:2417508:2418695 [5] NCCL INFO Connected all rings
19: nodeai04:2417510:2418691 [3] NCCL INFO Connected all rings
45: nodeai07:2435732:2436926 [5] NCCL INFO Connected all rings
22: nodeai04:2417511:2418696 [6] NCCL INFO Connected all rings
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 01/0 : 42[2] -> 43[3] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 02/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 01/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 04/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 02/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 05/0 : 42[2] -> 43[3] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 06/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 03/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 07/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 04/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 09/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 05/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 10/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 07/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 12/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 09/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 13/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 10/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 14/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 11/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 15/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 12/0 : 45[5] -> 46[6] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 13/0 : 45[5] -> 46[6] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 15/0 : 45[5] -> 46[6] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Connected all rings
38: nodeai06:2504437:2505621 [6] NCCL INFO Connected all rings
16: nodeai04:2417513:2418698 [0] NCCL INFO Connected all rings
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[1] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[1] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 16[0] -> 17[1] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 01/0 : 21[5] -> 22[6] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 05/0 : 16[0] -> 17[1] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 21[5] -> 22[6] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 16[0] -> 17[1] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 03/0 : 21[5] -> 22[6] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 17[1] -> 18[2] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 07/0 : 16[0] -> 17[1] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 21[5] -> 22[6] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 16[0] -> 17[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 03/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 21[5] -> 22[6] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 11/0 : 16[0] -> 17[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 07/0 : 21[5] -> 22[6] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 16[0] -> 17[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 05/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 09/0 : 21[5] -> 22[6] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 13/0 : 16[0] -> 17[1] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 21[5] -> 22[6] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 01/0 : 22[6] -> 23[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Connected all rings
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 16[0] -> 17[1] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 01/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 07/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 11/0 : 21[5] -> 22[6] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 22[6] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 15/0 : 16[0] -> 17[1] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 09/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 21[5] -> 22[6] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 03/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 19[3] -> 20[4] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Connected all rings
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 11/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 13/0 : 21[5] -> 22[6] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 05/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 15/0 : 21[5] -> 22[6] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 05/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 13/0 : 17[1] -> 18[2] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 07/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 17[1] -> 18[2] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 09/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 09/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 15/0 : 17[1] -> 18[2] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 19[3] -> 20[4] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 11/0 : 22[6] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 01/0 : 16[0] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 11/0 : 19[3] -> 20[4] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 16[0] -> 23[7] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 13/0 : 19[3] -> 20[4] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 03/0 : 16[0] -> 23[7] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 13/0 : 22[6] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 19[3] -> 20[4] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 22[6] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 05/0 : 16[0] -> 23[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 15/0 : 19[3] -> 20[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Connected all rings
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 16[0] -> 23[7] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Connected all rings
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 07/0 : 16[0] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 09/0 : 16[0] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 16[0] -> 23[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 01/0 : 44[4] -> 45[5] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 11/0 : 16[0] -> 23[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 02/0 : 44[4] -> 45[5] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 13/0 : 16[0] -> 23[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 03/0 : 44[4] -> 45[5] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 16[0] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 15/0 : 16[0] -> 23[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 04/0 : 44[4] -> 45[5] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Connected all rings
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 06/0 : 44[4] -> 45[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 07/0 : 44[4] -> 45[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 09/0 : 44[4] -> 45[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 10/0 : 44[4] -> 45[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 11/0 : 44[4] -> 45[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 12/0 : 44[4] -> 45[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 14/0 : 44[4] -> 45[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 15/0 : 44[4] -> 45[5] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Connected all rings
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 01/0 : 41[1] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 01/0 : 43[3] -> 44[4] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 02/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 03/0 : 41[1] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 03/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 04/0 : 41[1] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 01/0 : 46[6] -> 47[7] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 05/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 05/0 : 41[1] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 02/0 : 46[6] -> 47[7] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Connected all rings
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 06/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 06/0 : 41[1] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 03/0 : 46[6] -> 47[7] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Connected all rings
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 07/0 : 43[3] -> 44[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Connected all rings
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 07/0 : 41[1] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 04/0 : 46[6] -> 47[7] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 09/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 09/0 : 41[1] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 05/0 : 46[6] -> 47[7] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 10/0 : 43[3] -> 44[4] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Connected all rings
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 11/0 : 41[1] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 11/0 : 43[3] -> 44[4] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Connected all rings
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 06/0 : 46[6] -> 47[7] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 12/0 : 41[1] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 13/0 : 43[3] -> 44[4] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 09/0 : 46[6] -> 47[7] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 13/0 : 41[1] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 14/0 : 43[3] -> 44[4] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 10/0 : 46[6] -> 47[7] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 14/0 : 41[1] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 15/0 : 43[3] -> 44[4] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 11/0 : 46[6] -> 47[7] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 15/0 : 41[1] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 12/0 : 46[6] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 01/0 : 40[0] -> 47[7] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 04/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 13/0 : 46[6] -> 47[7] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 03/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 02/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 01/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 14/0 : 46[6] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 02/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 03/0 : 40[0] -> 47[7] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 05/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 06/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 05/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 06/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 07/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 09/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 10/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 11/0 : 40[0] -> 47[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Connected all rings
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 13/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 14/0 : 40[0] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 15/0 : 40[0] -> 47[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 04/0 : 24[0] -> 25[1] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 00/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 07/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 24[0] -> 25[1] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Connected all rings
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 06/0 : 24[0] -> 25[1] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Connected all rings
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 24[0] -> 25[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 25[1] -> 26[2] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 10/0 : 24[0] -> 25[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 25[1] -> 26[2] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 24[0] -> 25[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 04/0 : 25[1] -> 26[2] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 12/0 : 24[0] -> 25[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 25[1] -> 26[2] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 24[0] -> 25[1] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 06/0 : 25[1] -> 26[2] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 29[5] -> 30[6] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 14/0 : 24[0] -> 25[1] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 02/0 : 30[6] -> 31[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Connected all rings
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 25[1] -> 26[2] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 02/0 : 29[5] -> 30[6] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 24[0] -> 25[1] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Connected all rings
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 30[6] -> 31[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[1] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 25[1] -> 26[2] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 29[5] -> 30[6] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 04/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 25[1] -> 26[2] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 32[0] -> 33[1] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 04/0 : 29[5] -> 30[6] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 12/0 : 25[1] -> 26[2] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 26[2] -> 27[3] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Connected all rings
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 05/0 : 32[0] -> 33[1] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 06/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 25[1] -> 26[2] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 14/0 : 25[1] -> 26[2] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 27[3] -> 28[4] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 32[0] -> 33[1] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Connected all rings
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 04/0 : 26[2] -> 27[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 10/0 : 30[6] -> 31[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 28[4] -> 29[5] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 25[1] -> 26[2] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 02/0 : 27[3] -> 28[4] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Connected all rings
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 07/0 : 32[0] -> 33[1] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 10/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 26[2] -> 27[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 30[6] -> 31[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 02/0 : 28[4] -> 29[5] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 24[0] -> 31[7] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 06/0 : 26[2] -> 27[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 12/0 : 30[6] -> 31[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 28[4] -> 29[5] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 32[0] -> 33[1] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 02/0 : 24[0] -> 31[7] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 12/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 26[2] -> 27[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 30[6] -> 31[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 04/0 : 28[4] -> 29[5] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 06/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 24[0] -> 31[7] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 11/0 : 32[0] -> 33[1] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 01/0 : 37[5] -> 38[6] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 26[2] -> 27[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 30[6] -> 31[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 06/0 : 28[4] -> 29[5] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 24[0] -> 31[7] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 29[5] -> 30[6] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 26[2] -> 27[3] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 32[0] -> 33[1] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 37[5] -> 38[6] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 28[4] -> 29[5] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 06/0 : 24[0] -> 31[7] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 27[3] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 12/0 : 26[2] -> 27[3] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 28[4] -> 29[5] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 06/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 13/0 : 32[0] -> 33[1] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 03/0 : 37[5] -> 38[6] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 10/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 24[0] -> 31[7] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 26[2] -> 27[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 01/0 : 38[6] -> 39[7] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 10/0 : 28[4] -> 29[5] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Connected all rings
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 24[0] -> 31[7] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 33[1] -> 34[2] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 27[3] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 14/0 : 26[2] -> 27[3] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 32[0] -> 33[1] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 28[4] -> 29[5] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 37[5] -> 38[6] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 38[6] -> 39[7] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 10/0 : 24[0] -> 31[7] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 27[3] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 26[2] -> 27[3] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 28[4] -> 29[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 03/0 : 33[1] -> 34[2] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 15/0 : 32[0] -> 33[1] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 37[5] -> 38[6] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 24[0] -> 31[7] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 14/0 : 27[3] -> 28[4] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 14/0 : 28[4] -> 29[5] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 03/0 : 38[6] -> 39[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 24[0] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 33[1] -> 34[2] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 28[4] -> 29[5] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 07/0 : 37[5] -> 38[6] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 14/0 : 24[0] -> 31[7] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 38[6] -> 39[7] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Connected all rings
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 01/0 : 36[4] -> 37[5] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 05/0 : 33[1] -> 34[2] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 24[0] -> 31[7] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 04/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 09/0 : 37[5] -> 38[6] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 05/0 : 38[6] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 36[4] -> 37[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 33[1] -> 34[2] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Connected all rings
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 37[5] -> 38[6] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 38[6] -> 39[7] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 07/0 : 33[1] -> 34[2] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 03/0 : 36[4] -> 37[5] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Connected all rings
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 01/0 : 34[2] -> 35[3] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 11/0 : 37[5] -> 38[6] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 09/0 : 38[6] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 36[4] -> 37[5] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 09/0 : 33[1] -> 34[2] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 34[2] -> 35[3] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 37[5] -> 38[6] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 38[6] -> 39[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 36[4] -> 37[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 11/0 : 33[1] -> 34[2] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 01/0 : 35[3] -> 36[4] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 34[2] -> 35[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 01/0 : 20[4] -> 21[5] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 13/0 : 37[5] -> 38[6] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 11/0 : 38[6] -> 39[7] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 20[4] -> 21[5] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 07/0 : 36[4] -> 37[5] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Connected all rings
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 33[1] -> 34[2] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 03/0 : 20[4] -> 21[5] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 35[3] -> 36[4] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 05/0 : 34[2] -> 35[3] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 15/0 : 37[5] -> 38[6] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 20[4] -> 21[5] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 38[6] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 09/0 : 36[4] -> 37[5] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 20[4] -> 21[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 13/0 : 33[1] -> 34[2] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 35[3] -> 36[4] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 07/0 : 20[4] -> 21[5] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 34[2] -> 35[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 09/0 : 20[4] -> 21[5] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 13/0 : 38[6] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 36[4] -> 37[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 33[1] -> 34[2] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 05/0 : 35[3] -> 36[4] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 20[4] -> 21[5] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 07/0 : 34[2] -> 35[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 38[6] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 11/0 : 36[4] -> 37[5] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 15/0 : 33[1] -> 34[2] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 11/0 : 20[4] -> 21[5] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 35[3] -> 36[4] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 09/0 : 34[2] -> 35[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 20[4] -> 21[5] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 36[4] -> 37[5] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 07/0 : 35[3] -> 36[4] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 01/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 34[2] -> 35[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 36[4] -> 37[5] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 20[4] -> 21[5] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 09/0 : 35[3] -> 36[4] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 34[2] -> 35[3] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 15/0 : 36[4] -> 37[5] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Connected all rings
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 14/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 35[3] -> 36[4] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 15/0 : 20[4] -> 21[5] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 03/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 13/0 : 34[2] -> 35[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 11/0 : 35[3] -> 36[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 13/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 05/0 : 32[0] -> 39[7] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 34[2] -> 35[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 13/0 : 35[3] -> 36[4] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 15/0 : 34[2] -> 35[3] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 13/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 35[3] -> 36[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Connected all rings
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 13/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 07/0 : 32[0] -> 39[7] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 15/0 : 35[3] -> 36[4] via P2P/IPC
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 09/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 09/0 : 32[0] -> 39[7] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 11/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 11/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 13/0 : 32[0] -> 39[7] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 09/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 32[0] -> 39[7] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 15/0 : 32[0] -> 39[7] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 15/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 11/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 12/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[3] via P2P/IPC
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 10/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 18[2] -> 19[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 15/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 18[2] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 05/0 : 18[2] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 18[2] -> 19[3] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 07/0 : 18[2] -> 19[3] via P2P/IPC
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 08/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 15/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 09/0 : 18[2] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 18[2] -> 19[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 10/0 : 8[0] -> 9[1] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 18[2] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 13/0 : 18[2] -> 19[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 8[0] -> 9[1] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 18[2] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 15/0 : 18[2] -> 19[3] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 09/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 11/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 12/0 : 8[0] -> 9[1] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 8[0] -> 9[1] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 14/0 : 8[0] -> 9[1] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 8[0] -> 9[1] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 9[1] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 06/0 : 9[1] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 9[1] -> 10[2] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 9[1] -> 10[2] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 02/0 : 13[5] -> 14[6] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 9[1] -> 10[2] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 13[5] -> 14[6] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 12/0 : 9[1] -> 10[2] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 04/0 : 13[5] -> 14[6] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 9[1] -> 10[2] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 13[5] -> 14[6] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 14/0 : 9[1] -> 10[2] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 13[5] -> 14[6] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 9[1] -> 10[2] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 13[5] -> 14[6] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 10/0 : 13[5] -> 14[6] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 02/0 : 8[0] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 13[5] -> 14[6] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 12/0 : 13[5] -> 14[6] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 8[0] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 13[5] -> 14[6] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 12/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 06/0 : 8[0] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 13[5] -> 14[6] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 8[0] -> 15[7] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 14/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 8[0] -> 15[7] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 10/0 : 8[0] -> 15[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 8[0] -> 15[7] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 8[0] -> 15[7] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 14/0 : 8[0] -> 15[7] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 8[0] -> 15[7] via P2P/IPC
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Connected all rings
 6: nodeai01:2157661:2158844 [6] NCCL INFO Connected all rings
 1: nodeai01:2157663:2158848 [1] NCCL INFO Connected all rings
12: nodeai02:2459394:2460576 [4] NCCL INFO Connected all rings
11: nodeai02:2459390:2460577 [3] NCCL INFO Connected all rings
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 02/0 : 12[4] -> 13[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 12[4] -> 13[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 04/0 : 12[4] -> 13[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 06/0 : 12[4] -> 13[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 12[4] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 12[4] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 02/0 : 11[3] -> 12[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 10/0 : 12[4] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 11[3] -> 12[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 12[4] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 11[3] -> 12[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 12[4] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 06/0 : 11[3] -> 12[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 14/0 : 12[4] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 11[3] -> 12[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 12[4] -> 13[5] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 11[3] -> 12[4] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 10/0 : 11[3] -> 12[4] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 11[3] -> 12[4] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 11[3] -> 12[4] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Connected all rings
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 14/0 : 11[3] -> 12[4] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 11[3] -> 12[4] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 04/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 02/0 : 14[6] -> 15[7] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 04/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 06/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 10/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 12/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 14[6] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 14[6] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Connected all rings
 5: nodeai01:2157665:2158846 [5] NCCL INFO Connected all rings
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Connected all rings
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 06/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Connected all rings
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Connected all rings
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Connected all rings
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 02/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 05/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 06/0 : 0[0] -> 7[7] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 07/0 : 0[0] -> 7[7] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 09/0 : 0[0] -> 7[7] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 10/0 : 0[0] -> 7[7] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 11/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 13/0 : 0[0] -> 7[7] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 14/0 : 0[0] -> 7[7] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 02/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 15/0 : 0[0] -> 7[7] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 01/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 02/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 01/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 06/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 10/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 03/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 06/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 09/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 03/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 04/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 05/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 05/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 04/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 02/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 02/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 01/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 14/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 11/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 09/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 10/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 09/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 09/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 01/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 13/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 12/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 06/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 11/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 06/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 03/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 05/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 13/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 06/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 06/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 14/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 04/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 04/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 00/0 : 42[2] -> 41[1] via P2P/IPC
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 34[2] -> 33[1] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 18[2] -> 17[1] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 33[1] -> 32[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 11/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 07/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 13/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 01/0 : 34[2] -> 33[1] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 03/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 33[1] -> 32[0] via P2P/IPC
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 01/0 : 18[2] -> 17[1] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 04/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 01/0 : 42[2] -> 41[1] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 18[2] -> 17[1] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 13/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 06/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 34[2] -> 33[1] via P2P/IPC
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 05/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 07/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 08/0 : 10[2] -> 9[1] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 09/0 : 18[2] -> 17[1] via P2P/IPC
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 09/0 : 34[2] -> 33[1] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 08/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 15/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 10[2] -> 9[1] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 00/0 : 41[1] -> 40[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 08/0 : 42[2] -> 41[1] via P2P/IPC
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 11/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 04/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 00/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 17[1] -> 16[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 00/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 07/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 38[6] -> 37[5] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 04/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 00/0 : 46[6] -> 45[5] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 15/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 12/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 09/0 : 42[2] -> 41[1] via P2P/IPC
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 17[1] -> 16[0] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 08/0 : 41[1] -> 40[0] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 15/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 05/0 : 38[6] -> 37[5] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 01/0 : 47[7] -> 40[0] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 01/0 : 39[7] -> 32[0] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 08/0 : 9[1] -> 8[0] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 05/0 : 46[6] -> 45[5] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 38[6] -> 37[5] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 39[7] -> 32[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 31[7] -> 24[0] via P2P/IPC
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 15[7] -> 8[0] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 01/0 : 23[7] -> 16[0] via P2P/IPC
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 08/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 37[5] -> 36[4] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 00/0 : 44[4] -> 43[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 02/0 : 47[7] -> 40[0] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 13/0 : 38[6] -> 37[5] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 22[6] -> 21[5] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 00/0 : 45[5] -> 44[4] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 08/0 : 46[6] -> 45[5] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 03/0 : 39[7] -> 32[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 02/0 : 31[7] -> 24[0] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 37[5] -> 36[4] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 02/0 : 15[7] -> 8[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 21[5] -> 20[4] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 23[7] -> 16[0] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 04/0 : 45[5] -> 44[4] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/IPC
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 13/0 : 46[6] -> 45[5] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 39[7] -> 32[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 01/0 : 44[4] -> 43[3] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 21[5] -> 20[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 37[5] -> 36[4] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 08/0 : 45[5] -> 44[4] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 15[7] -> 8[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 36[4] -> 35[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 03/0 : 47[7] -> 40[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 14[6] -> 13[5] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 21[5] -> 20[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 00/0 : 30[6] -> 29[5] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 02/0 : 44[4] -> 43[3] via P2P/IPC
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 05/0 : 22[6] -> 21[5] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 04/0 : 13[5] -> 12[4] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 31[7] -> 24[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 05/0 : 39[7] -> 32[0] via P2P/IPC
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 12/0 : 45[5] -> 44[4] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 00/0 : 29[5] -> 28[4] via P2P/IPC
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 21[5] -> 20[4] via P2P/IPC
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 37[5] -> 36[4] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 04/0 : 47[7] -> 40[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 01/0 : 36[4] -> 35[3] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 22[6] -> 21[5] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 03/0 : 23[7] -> 16[0] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 03/0 : 44[4] -> 43[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 04/0 : 15[7] -> 8[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 08/0 : 14[6] -> 13[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 08/0 : 13[5] -> 12[4] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 39[7] -> 32[0] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 20[4] -> 19[3] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 30[6] -> 29[5] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 05/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 05/0 : 44[4] -> 43[3] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 04/0 : 29[5] -> 28[4] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 36[4] -> 35[3] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 39[7] -> 32[0] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 15[7] -> 8[0] via P2P/IPC
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 13/0 : 22[6] -> 21[5] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 08/0 : 30[6] -> 29[5] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 23[7] -> 16[0] via P2P/IPC
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 14[6] -> 13[5] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 02/0 : 12[4] -> 11[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 06/0 : 47[7] -> 40[0] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 01/0 : 20[4] -> 19[3] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 12/0 : 13[5] -> 12[4] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 08/0 : 29[5] -> 28[4] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 06/0 : 44[4] -> 43[3] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 03/0 : 36[4] -> 35[3] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 09/0 : 39[7] -> 32[0] via P2P/IPC
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 30[6] -> 29[5] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 07/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 07/0 : 44[4] -> 43[3] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 05/0 : 23[7] -> 16[0] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 04/0 : 31[7] -> 24[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 05/0 : 36[4] -> 35[3] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 08/0 : 26[2] -> 25[1] via P2P/IPC
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 12/0 : 29[5] -> 28[4] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 20[4] -> 19[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 06/0 : 15[7] -> 8[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 12[4] -> 11[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 09/0 : 47[7] -> 40[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 39[7] -> 32[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 08/0 : 44[4] -> 43[3] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 23[7] -> 16[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 36[4] -> 35[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 15[7] -> 8[0] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 03/0 : 20[4] -> 19[3] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 12[4] -> 11[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 10/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 09/0 : 44[4] -> 43[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 07/0 : 23[7] -> 16[0] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 08/0 : 25[1] -> 24[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 31[7] -> 24[0] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 00/0 : 28[4] -> 27[3] via P2P/IPC
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 26[2] -> 25[1] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 05/0 : 20[4] -> 19[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 07/0 : 36[4] -> 35[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 11/0 : 47[7] -> 40[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 11/0 : 39[7] -> 32[0] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 06/0 : 12[4] -> 11[3] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 10/0 : 44[4] -> 43[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 09/0 : 23[7] -> 16[0] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 36[4] -> 35[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 20[4] -> 19[3] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 39[7] -> 32[0] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 12[4] -> 11[3] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 11/0 : 44[4] -> 43[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 15[7] -> 8[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 06/0 : 31[7] -> 24[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 09/0 : 36[4] -> 35[3] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 28[4] -> 27[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 12/0 : 47[7] -> 40[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 13/0 : 39[7] -> 32[0] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 23[7] -> 16[0] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 00/0 : 43[3] -> 42[2] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 07/0 : 20[4] -> 19[3] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 36[4] -> 35[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 08/0 : 12[4] -> 11[3] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 02/0 : 28[4] -> 27[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 10/0 : 15[7] -> 8[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 31[7] -> 24[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 13/0 : 44[4] -> 43[3] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 13/0 : 47[7] -> 40[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 39[7] -> 32[0] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 11/0 : 23[7] -> 16[0] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 02/0 : 43[3] -> 42[2] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 12[4] -> 11[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 20[4] -> 19[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 15[7] -> 8[0] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 31[7] -> 24[0] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 11/0 : 36[4] -> 35[3] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 14/0 : 44[4] -> 43[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 23[7] -> 16[0] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 14/0 : 47[7] -> 40[0] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 15/0 : 39[7] -> 32[0] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 08/0 : 43[3] -> 42[2] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 10/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 12/0 : 15[7] -> 8[0] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 10/0 : 31[7] -> 24[0] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 09/0 : 20[4] -> 19[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 13/0 : 36[4] -> 35[3] via P2P/IPC
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 15/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 15/0 : 44[4] -> 43[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 13/0 : 23[7] -> 16[0] via P2P/IPC
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 10/0 : 43[3] -> 42[2] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 08/0 : 11[3] -> 10[2] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 20[4] -> 19[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 15[7] -> 8[0] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 31[7] -> 24[0] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 06/0 : 28[4] -> 27[3] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 36[4] -> 35[3] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 23[7] -> 16[0] via P2P/IPC
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 15/0 : 36[4] -> 35[3] via P2P/IPC
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 10/0 : 11[3] -> 10[2] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 11/0 : 20[4] -> 19[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 12[4] -> 11[3] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 12/0 : 31[7] -> 24[0] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 14/0 : 15[7] -> 8[0] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 28[4] -> 27[3] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 19[3] -> 18[2] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 15/0 : 23[7] -> 16[0] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 35[3] -> 34[2] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 13/0 : 20[4] -> 19[3] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 14/0 : 12[4] -> 11[3] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 31[7] -> 24[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 19[3] -> 18[2] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 15/0 : 15[7] -> 8[0] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 08/0 : 28[4] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 00/0 : 47[7] -> 46[6] via P2P/IPC
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 12[4] -> 11[3] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 20[4] -> 19[3] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 14/0 : 31[7] -> 24[0] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 19[3] -> 18[2] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 28[4] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/IPC
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 15/0 : 20[4] -> 19[3] via P2P/IPC
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 19[3] -> 18[2] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 15/0 : 31[7] -> 24[0] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 23[7] -> 22[6] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 10/0 : 28[4] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 08/0 : 27[3] -> 26[2] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 28[4] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 10/0 : 27[3] -> 26[2] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 00/0 : 31[7] -> 30[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 23[7] -> 22[6] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 14/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 06/0 : 31[7] -> 30[6] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 35[3] -> 34[2] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 08/0 : 31[7] -> 30[6] via P2P/IPC
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 28[4] -> 27[3] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 39[7] -> 38[6] via P2P/IPC
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 35[3] -> 34[2] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 23[7] -> 22[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 39[7] -> 38[6] via P2P/IPC
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 35[3] -> 34[2] via P2P/IPC
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 14/0 : 31[7] -> 30[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 06/0 : 47[7] -> 46[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 39[7] -> 38[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 08/0 : 47[7] -> 46[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 06/0 : 15[7] -> 14[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 08/0 : 15[7] -> 14[6] via P2P/IPC
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 39[7] -> 38[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 14/0 : 47[7] -> 46[6] via P2P/IPC
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 23[7] -> 22[6] via P2P/IPC
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 14/0 : 15[7] -> 14[6] via P2P/IPC
 2: nodeai01:2157662:2158849 [2] NCCL INFO Connected all trees
 2: nodeai01:2157662:2158849 [2] NCCL INFO NVLS comm 0x556a1ae3d180 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
10: nodeai02:2459393:2460578 [2] NCCL INFO Connected all trees
45: nodeai07:2435732:2436926 [5] NCCL INFO Connected all trees
45: nodeai07:2435732:2436926 [5] NCCL INFO NVLS comm 0x56314f520540 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
10: nodeai02:2459393:2460578 [2] NCCL INFO NVLS comm 0x556f37828780 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
42: nodeai07:2435730:2436923 [2] NCCL INFO Connected all trees
42: nodeai07:2435730:2436923 [2] NCCL INFO NVLS comm 0x55c42d8bcb80 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
40: nodeai07:2435734:2436919 [0] NCCL INFO Connected all trees
40: nodeai07:2435734:2436919 [0] NCCL INFO NVLS comm 0x5581fe0eb6c0 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
43: nodeai07:2435733:2436924 [3] NCCL INFO Connected all trees
37: nodeai06:2504432:2505626 [5] NCCL INFO Connected all trees
43: nodeai07:2435733:2436924 [3] NCCL INFO NVLS comm 0x559b9d0385c0 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
37: nodeai06:2504432:2505626 [5] NCCL INFO NVLS comm 0x5622d3b13780 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
41: nodeai07:2435735:2436921 [1] NCCL INFO Connected all trees
44: nodeai07:2435729:2436925 [4] NCCL INFO Connected all trees
41: nodeai07:2435735:2436921 [1] NCCL INFO NVLS comm 0x55cf5455bb80 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
46: nodeai07:2435731:2436922 [6] NCCL INFO Connected all trees
44: nodeai07:2435729:2436925 [4] NCCL INFO NVLS comm 0x564b194ce040 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
46: nodeai07:2435731:2436922 [6] NCCL INFO NVLS comm 0x561627d2c000 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
34: nodeai06:2504435:2505620 [2] NCCL INFO Connected all trees
 5: nodeai01:2157665:2158846 [5] NCCL INFO Connected all trees
34: nodeai06:2504435:2505620 [2] NCCL INFO NVLS comm 0x5630f97acd40 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 5: nodeai01:2157665:2158846 [5] NCCL INFO NVLS comm 0x55f9facf8dc0 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
18: nodeai04:2417512:2418693 [2] NCCL INFO Connected all trees
47: nodeai07:2435736:2436920 [7] NCCL INFO Connected all trees
18: nodeai04:2417512:2418693 [2] NCCL INFO NVLS comm 0x564c4f3cf680 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
17: nodeai04:2417509:2418692 [1] NCCL INFO Connected all trees
47: nodeai07:2435736:2436920 [7] NCCL INFO NVLS comm 0x564e1f98c4c0 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
13: nodeai02:2459392:2460582 [5] NCCL INFO Connected all trees
35: nodeai06:2504431:2505625 [3] NCCL INFO Connected all trees
17: nodeai04:2417509:2418692 [1] NCCL INFO NVLS comm 0x55c6a815d6c0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
13: nodeai02:2459392:2460582 [5] NCCL INFO NVLS comm 0x55d980822880 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
35: nodeai06:2504431:2505625 [3] NCCL INFO NVLS comm 0x55cbe345b740 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
36: nodeai06:2504434:2505624 [4] NCCL INFO Connected all trees
 9: nodeai02:2459396:2460580 [1] NCCL INFO Connected all trees
 1: nodeai01:2157663:2158848 [1] NCCL INFO Connected all trees
36: nodeai06:2504434:2505624 [4] NCCL INFO NVLS comm 0x5619adeb56c0 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 9: nodeai02:2459396:2460580 [1] NCCL INFO NVLS comm 0x559b064037c0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 1: nodeai01:2157663:2158848 [1] NCCL INFO NVLS comm 0x55cb52b80bc0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
38: nodeai06:2504437:2505621 [6] NCCL INFO Connected all trees
12: nodeai02:2459394:2460576 [4] NCCL INFO Connected all trees
38: nodeai06:2504437:2505621 [6] NCCL INFO NVLS comm 0x55f04b397a80 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
26: nodeai05:1971443:1972633 [2] NCCL INFO Connected all trees
22: nodeai04:2417511:2418696 [6] NCCL INFO Connected all trees
12: nodeai02:2459394:2460576 [4] NCCL INFO NVLS comm 0x55d806bc8ac0 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
26: nodeai05:1971443:1972633 [2] NCCL INFO NVLS comm 0x55be6917b200 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 4: nodeai01:2157666:2158845 [4] NCCL INFO Connected all trees
22: nodeai04:2417511:2418696 [6] NCCL INFO NVLS comm 0x560f6bdcbc80 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
39: nodeai06:2504430:2505623 [7] NCCL INFO Connected all trees
 4: nodeai01:2157666:2158845 [4] NCCL INFO NVLS comm 0x556b87fd8840 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
33: nodeai06:2504433:2505622 [1] NCCL INFO Connected all trees
 8: nodeai02:2459395:2460579 [0] NCCL INFO Connected all trees
32: nodeai06:2504436:2505627 [0] NCCL INFO Connected all trees
39: nodeai06:2504430:2505623 [7] NCCL INFO NVLS comm 0x5654f999ea00 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
19: nodeai04:2417510:2418691 [3] NCCL INFO Connected all trees
33: nodeai06:2504433:2505622 [1] NCCL INFO NVLS comm 0x5601eae71980 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
25: nodeai05:1971445:1972631 [1] NCCL INFO Connected all trees
32: nodeai06:2504436:2505627 [0] NCCL INFO NVLS comm 0x56050cd8ad80 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 8: nodeai02:2459395:2460579 [0] NCCL INFO NVLS comm 0x55adf155d980 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
19: nodeai04:2417510:2418691 [3] NCCL INFO NVLS comm 0x55e304d4de40 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
11: nodeai02:2459390:2460577 [3] NCCL INFO Connected all trees
 0: nodeai01:2157660:2158843 [0] NCCL INFO Connected all trees
16: nodeai04:2417513:2418698 [0] NCCL INFO Connected all trees
25: nodeai05:1971445:1972631 [1] NCCL INFO NVLS comm 0x55c3e3c1bc80 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 0: nodeai01:2157660:2158843 [0] NCCL INFO NVLS comm 0x5609149c7900 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
11: nodeai02:2459390:2460577 [3] NCCL INFO NVLS comm 0x564a9a78c300 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
16: nodeai04:2417513:2418698 [0] NCCL INFO NVLS comm 0x55ea13bc9dc0 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
14: nodeai02:2459389:2460583 [6] NCCL INFO Connected all trees
 3: nodeai01:2157667:2158850 [3] NCCL INFO Connected all trees
14: nodeai02:2459389:2460583 [6] NCCL INFO NVLS comm 0x55ec378e1840 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 3: nodeai01:2157667:2158850 [3] NCCL INFO NVLS comm 0x5571536aaac0 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
23: nodeai04:2417507:2418697 [7] NCCL INFO Connected all trees
30: nodeai05:1971448:1972630 [6] NCCL INFO Connected all trees
30: nodeai05:1971448:1972630 [6] NCCL INFO NVLS comm 0x55754205dbc0 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
23: nodeai04:2417507:2418697 [7] NCCL INFO NVLS comm 0x55a36f03a600 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
21: nodeai04:2417508:2418695 [5] NCCL INFO Connected all trees
27: nodeai05:1971446:1972637 [3] NCCL INFO Connected all trees
21: nodeai04:2417508:2418695 [5] NCCL INFO NVLS comm 0x558756c399c0 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
27: nodeai05:1971446:1972637 [3] NCCL INFO NVLS comm 0x55bd9df48280 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
15: nodeai02:2459391:2460581 [7] NCCL INFO Connected all trees
24: nodeai05:1971447:1972632 [0] NCCL INFO Connected all trees
15: nodeai02:2459391:2460581 [7] NCCL INFO NVLS comm 0x5619f3e1f040 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
31: nodeai05:1971449:1972636 [7] NCCL INFO Connected all trees
24: nodeai05:1971447:1972632 [0] NCCL INFO NVLS comm 0x5643eedcebc0 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 6: nodeai01:2157661:2158844 [6] NCCL INFO Connected all trees
31: nodeai05:1971449:1972636 [7] NCCL INFO NVLS comm 0x55a01ffbc340 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
29: nodeai05:1971444:1972635 [5] NCCL INFO Connected all trees
 6: nodeai01:2157661:2158844 [6] NCCL INFO NVLS comm 0x55aa516d7b00 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
29: nodeai05:1971444:1972635 [5] NCCL INFO NVLS comm 0x56523b50c3c0 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
28: nodeai05:1971442:1972634 [4] NCCL INFO Connected all trees
 7: nodeai01:2157664:2158847 [7] NCCL INFO Connected all trees
28: nodeai05:1971442:1972634 [4] NCCL INFO NVLS comm 0x560ddfef6e80 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 7: nodeai01:2157664:2158847 [7] NCCL INFO NVLS comm 0x561beaa3d7c0 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
20: nodeai04:2417506:2418694 [4] NCCL INFO Connected all trees
20: nodeai04:2417506:2418694 [4] NCCL INFO NVLS comm 0x5615b35ac500 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 00/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 02/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 04/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 06/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 08/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 10/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 12/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 14/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 01/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 03/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 05/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 07/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 09/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 11/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 13/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 15/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 03/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 05/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 07/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 11/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 13/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 15/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 02/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 10/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 00/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 02/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 04/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 07/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 08/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 10/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 12/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 15/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 07/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 15/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 00/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 02/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 00/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 04/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 04/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 06/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 06/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 08/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 08/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 10/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 10/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 12/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 12/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 14/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 14/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 01/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 01/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 03/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 05/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 05/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 07/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 09/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 06/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 07/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 09/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 13/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 11/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 15/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 13/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 03/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 15/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 14/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 05/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 04/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 01/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 07/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 03/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 09/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 12/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 06/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 11/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 07/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 13/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 09/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 15/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 11/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 14/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 15/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 01/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 03/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 05/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 07/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 09/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 13/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 15/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 02/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 04/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 06/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 08/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 10/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 12/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 14/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 01/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 03/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 05/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 07/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 09/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 11/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 13/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 15/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 04/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 12/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 01/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 04/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 05/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 07/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 01/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 03/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 09/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 07/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 12/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 09/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 13/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 11/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 15/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 15/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 06/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 14/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 00/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 02/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 04/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 08/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 10/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 00/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 12/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 02/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 14/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 04/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 06/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 08/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 10/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 12/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 14/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 01/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 03/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 05/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 07/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 09/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 00/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 11/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 02/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 13/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 00/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 04/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 15/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 04/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 06/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 08/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 06/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 10/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 12/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 08/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 14/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 11/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 12/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 14/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 03/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 11/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 01/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 01/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 05/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 03/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 05/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 07/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 07/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 09/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 11/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 09/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 13/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 11/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 13/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 15/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 15/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 01/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 03/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 05/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 09/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 11/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 13/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 01/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 03/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 05/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 07/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 09/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 00/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 02/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 02/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 11/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 04/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 13/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 04/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 06/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 06/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 08/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 10/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 10/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 12/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 12/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 14/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 14/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 00/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 02/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 04/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 06/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 08/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 10/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 12/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 14/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 00/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 02/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 04/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 06/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 01/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 08/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 03/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 10/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 05/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 12/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 07/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 06/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 14/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 09/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 11/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 15/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 14/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 01/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 06/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 03/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 05/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 14/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 09/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 11/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 13/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 01/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 01/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 03/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 03/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 05/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 05/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 07/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 07/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 09/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 09/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 11/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 11/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 13/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 13/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 15/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 15/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 03/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 00/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 02/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 04/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 01/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 03/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 11/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 05/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 07/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 06/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 09/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 11/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 08/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 13/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 10/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 15/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 12/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 14/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 00/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 04/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 06/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 08/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 12/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 14/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 00/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 02/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 06/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 00/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 08/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 10/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 02/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 12/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 04/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 06/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 14/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 08/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 10/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 12/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 14/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 01/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 03/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 05/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 01/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 07/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 03/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 09/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 05/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 11/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 07/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 13/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 02/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 11/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 15/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 13/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 15/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 10/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 02/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 10/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 00/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 02/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 04/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 06/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 08/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 10/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 12/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 14/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 00/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 02/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 00/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 00/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 02/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 04/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 02/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 04/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 06/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 06/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 08/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 06/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 08/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 10/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 08/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 10/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 12/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 10/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 12/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 14/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 14/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 14/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 04/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 04/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 12/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 12/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 04/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 02/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 12/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 01/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 10/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 03/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 05/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 07/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 02/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 03/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 09/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 04/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 11/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 05/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 13/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 07/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 15/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 10/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 12/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 11/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 13/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 15/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 07/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 15/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 02/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 02/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 04/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 06/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 10/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 10/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 12/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 14/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 02/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 02/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 10/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 10/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 02/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 10/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 01/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 03/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 00/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 05/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 02/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 02/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 07/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 04/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 03/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 11/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 06/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 13/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 08/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 05/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 15/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158848 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 10/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 12/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 07/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 14/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 01/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 10/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 03/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 11/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 05/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 07/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 13/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 11/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 13/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 15/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505622 [1] NCCL INFO Channel 15/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 00/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 02/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 04/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 06/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 01/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 08/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 10/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 12/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418692 [1] NCCL INFO Channel 14/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 00/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 02/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 03/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 04/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 05/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 06/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 00/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 02/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 04/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 06/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 08/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 10/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 12/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 14/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 07/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 08/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 10/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 00/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 02/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 11/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 04/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 12/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460580 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 06/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 13/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 08/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 14/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 10/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972631 [1] NCCL INFO Channel 15/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 12/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Channel 14/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 00/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 02/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 04/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 06/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 08/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 02/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 10/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 04/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 12/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 06/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 14/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 08/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 10/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 12/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 14/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 00/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 02/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 04/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 06/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 04/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 08/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 10/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 02/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 12/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 04/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 14/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 12/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 06/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 08/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 01/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 10/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 03/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 12/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 05/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 14/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 07/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 04/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 09/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 01/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 13/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 03/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 12/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 15/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 05/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 07/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 09/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 11/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 13/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 15/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 00/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 02/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 03/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 01/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 02/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 04/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 04/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 03/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 06/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 04/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 06/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 3: nodeai01:2157667:2158850 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 08/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 00/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 05/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 08/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 10/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 05/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 02/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 10/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 04/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 12/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 07/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 12/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 06/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 07/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 14/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158843 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 14/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 00/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 08/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 09/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 01/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 09/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 01/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 10/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 01/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 03/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 12/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 02/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 03/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 02/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 05/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 11/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 12/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 04/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 05/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 04/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 07/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 07/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 13/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 05/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 02/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
43: nodeai07:2435733:2436924 [3] NCCL INFO Channel 14/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 04/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 06/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 09/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 13/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 06/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 09/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 00/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 06/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 08/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 11/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 13/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 15/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 10/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 07/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 13/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 15/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
35: nodeai06:2504431:2505625 [3] NCCL INFO Channel 15/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 00/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 12/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 08/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 02/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 08/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 10/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 02/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2436919 [0] NCCL INFO Channel 14/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 02/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 04/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 09/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 04/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 12/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 04/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 06/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 10/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 06/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 14/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 08/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 06/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 12/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 08/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 10/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 08/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 10/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 13/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 10/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 12/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 14/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 12/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 12/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 14/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418698 [0] NCCL INFO Channel 14/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972637 [3] NCCL INFO Channel 15/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 00/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418691 [3] NCCL INFO Channel 14/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 00/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 02/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 02/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 00/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 04/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 04/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 02/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 06/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 00/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 08/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
32: nodeai06:2504436:2505627 [0] NCCL INFO Channel 15/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460579 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 04/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 08/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 02/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 10/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 06/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 10/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 06/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 12/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 08/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 12/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 08/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 02/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 10/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 03/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460577 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 10/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 14/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 12/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 04/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 14/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 14/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 05/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 00/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 06/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 02/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 07/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 04/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 09/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 08/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 10/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 10/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 00/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 00/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 11/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 12/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 02/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 12/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 02/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 14/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 13/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 06/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 04/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 00/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 14/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 08/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 06/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 02/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 00/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 07/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972632 [0] NCCL INFO Channel 15/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 10/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 04/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 08/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 02/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 12/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 06/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 10/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 14/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 04/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 08/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 12/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 10/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 14/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 08/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 00/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 15/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 10/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 12/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 02/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 14/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 12/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 06/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 08/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 14/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 06/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 00/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 07/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 10/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 12/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 14/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 14/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 02/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 00/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 04/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 02/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 15/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 06/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 04/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 08/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 06/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 10/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 08/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 01/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 03/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 12/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 10/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 14/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 07/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 12/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 14/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 06/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 07/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 15/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 05/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 01/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 07/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 01/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 03/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 01/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 09/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 03/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 05/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 03/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 14/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 11/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 05/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 07/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 05/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 13/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 07/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 09/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 15/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 07/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 15/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 09/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 11/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 09/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 15/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 11/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 11/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 06/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 00/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 07/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 01/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 13/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 13/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 02/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 15/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 04/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 03/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 08/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 14/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 10/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 00/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 05/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 00/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 15/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 12/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 07/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 02/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 02/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 14/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 06/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 06/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 01/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 00/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158844 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 06/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 09/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 08/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 08/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 00/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 03/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 01/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 02/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 10/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 10/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 01/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 05/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 06/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 02/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 12/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 11/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 12/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 07/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 08/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 03/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 04/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2436925 [4] NCCL INFO Channel 14/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 09/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 14/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 00/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 03/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 10/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 06/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 05/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 14/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 13/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 02/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 11/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158845 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 12/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 00/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 08/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 00/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 13/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 04/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 02/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 14/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 05/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 10/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 01/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 15/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158847 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 07/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505621 [6] NCCL INFO Channel 15/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 06/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 01/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 04/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 02/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 00/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 05/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 12/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 08/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 00/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 00/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 08/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 03/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 02/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 00/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 03/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 00/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 14/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 09/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 02/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 02/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 02/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 01/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 04/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 02/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 10/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 04/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 04/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 06/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 05/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 10/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 05/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 09/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 04/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 01/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 06/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 08/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 08/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 12/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 08/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 07/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 10/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 04/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 06/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 08/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 03/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 12/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 00/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 11/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 03/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 10/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 09/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 14/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 07/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 11/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460576 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 14/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 06/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 02/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 05/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 13/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 11/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 12/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 08/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 08/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 07/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 04/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 10/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 10/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 12/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 12/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 12/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 06/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2436926 [5] NCCL INFO Channel 14/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 13/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418696 [6] NCCL INFO Channel 14/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2436922 [6] NCCL INFO Channel 14/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 00/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
47: nodeai07:2435736:2436920 [7] NCCL INFO Channel 14/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 13/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 10/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158846 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 09/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 06/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 01/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 07/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505624 [4] NCCL INFO Channel 15/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 15/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 11/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 07/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 12/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 08/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 02/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 07/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 00/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 14/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 00/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 10/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505623 [7] NCCL INFO Channel 13/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 03/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 09/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 02/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 01/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 02/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 12/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 04/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 00/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 06/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 03/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 04/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 14/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 06/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460582 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 04/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 08/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 05/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 11/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 07/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 06/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 06/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 07/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 00/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 10/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460583 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 15/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 08/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 03/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 08/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 01/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 12/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 09/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 10/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 00/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 14/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 08/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 09/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418694 [4] NCCL INFO Channel 14/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 02/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 12/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 11/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 15/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 01/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 10/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 10/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418697 [7] NCCL INFO Channel 14/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 00/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505626 [5] NCCL INFO Channel 15/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 03/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 09/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 02/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 11/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 12/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 04/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 02/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 10/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 03/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 12/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 04/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 05/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 14/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 11/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 04/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460581 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 14/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 13/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 06/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 11/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 06/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 05/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972635 [5] NCCL INFO Channel 15/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 08/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 14/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 00/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 08/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 07/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 10/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972634 [4] NCCL INFO Channel 15/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 09/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 08/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 04/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 12/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 10/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 09/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418695 [5] NCCL INFO Channel 14/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 11/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 06/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 10/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 12/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 08/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 11/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 13/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 10/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 12/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972636 [7] NCCL INFO Channel 14/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 12/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 13/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 14/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972630 [6] NCCL INFO Channel 15/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 03/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 11/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 01/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 03/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 03/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 05/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 03/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 07/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 09/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 11/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 11/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 11/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 13/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 15/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 03/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 11/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 00/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 04/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 01/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 06/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 08/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 10/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 05/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158849 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 12/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 03/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 14/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 07/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 01/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 03/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 09/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 05/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 11/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 07/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 09/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 13/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 11/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 13/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 15/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 00/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 00/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 00/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 04/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 04/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 04/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 06/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 06/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 06/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 08/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 08/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 08/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 10/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 10/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 12/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418693 [2] NCCL INFO Channel 14/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 10/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505620 [2] NCCL INFO Channel 15/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 12/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2436923 [2] NCCL INFO Channel 14/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 12/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 14/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460578 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 11/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 00/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 01/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 04/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 05/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 06/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 07/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 08/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 09/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 11/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 12/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 13/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 14/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972633 [2] NCCL INFO Channel 15/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2436921 [1] NCCL INFO Connected NVLS tree
41: nodeai07:2435735:2436921 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
41: nodeai07:2435735:2436921 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
47: nodeai07:2435736:2436920 [7] NCCL INFO Connected NVLS tree
47: nodeai07:2435736:2436920 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
47: nodeai07:2435736:2436920 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
43: nodeai07:2435733:2436924 [3] NCCL INFO Connected NVLS tree
43: nodeai07:2435733:2436924 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
43: nodeai07:2435733:2436924 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
45: nodeai07:2435732:2436926 [5] NCCL INFO Connected NVLS tree
45: nodeai07:2435732:2436926 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
45: nodeai07:2435732:2436926 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
40: nodeai07:2435734:2436919 [0] NCCL INFO Connected NVLS tree
40: nodeai07:2435734:2436919 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
40: nodeai07:2435734:2436919 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
44: nodeai07:2435729:2436925 [4] NCCL INFO Connected NVLS tree
44: nodeai07:2435729:2436925 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
44: nodeai07:2435729:2436925 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
46: nodeai07:2435731:2436922 [6] NCCL INFO Connected NVLS tree
46: nodeai07:2435731:2436922 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
46: nodeai07:2435731:2436922 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
42: nodeai07:2435730:2436923 [2] NCCL INFO Connected NVLS tree
42: nodeai07:2435730:2436923 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
42: nodeai07:2435730:2436923 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
42: nodeai07:2435730:2436923 [2] NCCL INFO comm 0x55c42d8bcb80 rank 42 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init COMPLETE
44: nodeai07:2435729:2436925 [4] NCCL INFO comm 0x564b194ce040 rank 44 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init COMPLETE
46: nodeai07:2435731:2436922 [6] NCCL INFO comm 0x561627d2c000 rank 46 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init COMPLETE
40: nodeai07:2435734:2436919 [0] NCCL INFO comm 0x5581fe0eb6c0 rank 40 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init COMPLETE
47: nodeai07:2435736:2436920 [7] NCCL INFO comm 0x564e1f98c4c0 rank 47 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init COMPLETE
43: nodeai07:2435733:2436924 [3] NCCL INFO comm 0x559b9d0385c0 rank 43 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init COMPLETE
41: nodeai07:2435735:2436921 [1] NCCL INFO comm 0x55cf5455bb80 rank 41 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init COMPLETE
45: nodeai07:2435732:2436926 [5] NCCL INFO comm 0x56314f520540 rank 45 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init COMPLETE
 1: nodeai01:2157663:2158848 [1] NCCL INFO Connected NVLS tree
 1: nodeai01:2157663:2158848 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 1: nodeai01:2157663:2158848 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 7: nodeai01:2157664:2158847 [7] NCCL INFO Connected NVLS tree
 7: nodeai01:2157664:2158847 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 7: nodeai01:2157664:2158847 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 3: nodeai01:2157667:2158850 [3] NCCL INFO Connected NVLS tree
 3: nodeai01:2157667:2158850 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 3: nodeai01:2157667:2158850 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 5: nodeai01:2157665:2158846 [5] NCCL INFO Connected NVLS tree
 5: nodeai01:2157665:2158846 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 5: nodeai01:2157665:2158846 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 9: nodeai02:2459396:2460580 [1] NCCL INFO Connected NVLS tree
 9: nodeai02:2459396:2460580 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 9: nodeai02:2459396:2460580 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
30: nodeai05:1971448:1972630 [6] NCCL INFO Connected NVLS tree
30: nodeai05:1971448:1972630 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
30: nodeai05:1971448:1972630 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
38: nodeai06:2504437:2505621 [6] NCCL INFO Connected NVLS tree
38: nodeai06:2504437:2505621 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
38: nodeai06:2504437:2505621 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 8: nodeai02:2459395:2460579 [0] NCCL INFO Connected NVLS tree
 8: nodeai02:2459395:2460579 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 8: nodeai02:2459395:2460579 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 0: nodeai01:2157660:2158843 [0] NCCL INFO Connected NVLS tree
 0: nodeai01:2157660:2158843 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 0: nodeai01:2157660:2158843 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
33: nodeai06:2504433:2505622 [1] NCCL INFO Connected NVLS tree
33: nodeai06:2504433:2505622 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
33: nodeai06:2504433:2505622 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
25: nodeai05:1971445:1972631 [1] NCCL INFO Connected NVLS tree
25: nodeai05:1971445:1972631 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
25: nodeai05:1971445:1972631 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 6: nodeai01:2157661:2158844 [6] NCCL INFO Connected NVLS tree
 6: nodeai01:2157661:2158844 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 6: nodeai01:2157661:2158844 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
36: nodeai06:2504434:2505624 [4] NCCL INFO Connected NVLS tree
36: nodeai06:2504434:2505624 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
36: nodeai06:2504434:2505624 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
16: nodeai04:2417513:2418698 [0] NCCL INFO Connected NVLS tree
16: nodeai04:2417513:2418698 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
16: nodeai04:2417513:2418698 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
28: nodeai05:1971442:1972634 [4] NCCL INFO Connected NVLS tree
28: nodeai05:1971442:1972634 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
28: nodeai05:1971442:1972634 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
32: nodeai06:2504436:2505627 [0] NCCL INFO Connected NVLS tree
32: nodeai06:2504436:2505627 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
32: nodeai06:2504436:2505627 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
20: nodeai04:2417506:2418694 [4] NCCL INFO Connected NVLS tree
20: nodeai04:2417506:2418694 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
20: nodeai04:2417506:2418694 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
24: nodeai05:1971447:1972632 [0] NCCL INFO Connected NVLS tree
24: nodeai05:1971447:1972632 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
24: nodeai05:1971447:1972632 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
12: nodeai02:2459394:2460576 [4] NCCL INFO Connected NVLS tree
12: nodeai02:2459394:2460576 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
12: nodeai02:2459394:2460576 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
14: nodeai02:2459389:2460583 [6] NCCL INFO Connected NVLS tree
14: nodeai02:2459389:2460583 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
14: nodeai02:2459389:2460583 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 4: nodeai01:2157666:2158845 [4] NCCL INFO Connected NVLS tree
 4: nodeai01:2157666:2158845 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 4: nodeai01:2157666:2158845 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
18: nodeai04:2417512:2418693 [2] NCCL INFO Connected NVLS tree
18: nodeai04:2417512:2418693 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
18: nodeai04:2417512:2418693 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
34: nodeai06:2504435:2505620 [2] NCCL INFO Connected NVLS tree
34: nodeai06:2504435:2505620 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
34: nodeai06:2504435:2505620 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
22: nodeai04:2417511:2418696 [6] NCCL INFO Connected NVLS tree
22: nodeai04:2417511:2418696 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
22: nodeai04:2417511:2418696 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
35: nodeai06:2504431:2505625 [3] NCCL INFO Connected NVLS tree
35: nodeai06:2504431:2505625 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
35: nodeai06:2504431:2505625 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
31: nodeai05:1971449:1972636 [7] NCCL INFO Connected NVLS tree
31: nodeai05:1971449:1972636 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
31: nodeai05:1971449:1972636 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
37: nodeai06:2504432:2505626 [5] NCCL INFO Connected NVLS tree
37: nodeai06:2504432:2505626 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
37: nodeai06:2504432:2505626 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
13: nodeai02:2459392:2460582 [5] NCCL INFO Connected NVLS tree
13: nodeai02:2459392:2460582 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
13: nodeai02:2459392:2460582 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
17: nodeai04:2417509:2418692 [1] NCCL INFO Connected NVLS tree
17: nodeai04:2417509:2418692 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
17: nodeai04:2417509:2418692 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
10: nodeai02:2459393:2460578 [2] NCCL INFO Connected NVLS tree
10: nodeai02:2459393:2460578 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
10: nodeai02:2459393:2460578 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
26: nodeai05:1971443:1972633 [2] NCCL INFO Connected NVLS tree
26: nodeai05:1971443:1972633 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
26: nodeai05:1971443:1972633 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
39: nodeai06:2504430:2505623 [7] NCCL INFO Connected NVLS tree
39: nodeai06:2504430:2505623 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
39: nodeai06:2504430:2505623 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
11: nodeai02:2459390:2460577 [3] NCCL INFO Connected NVLS tree
11: nodeai02:2459390:2460577 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
11: nodeai02:2459390:2460577 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
27: nodeai05:1971446:1972637 [3] NCCL INFO Connected NVLS tree
27: nodeai05:1971446:1972637 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
27: nodeai05:1971446:1972637 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
29: nodeai05:1971444:1972635 [5] NCCL INFO Connected NVLS tree
29: nodeai05:1971444:1972635 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
29: nodeai05:1971444:1972635 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 2: nodeai01:2157662:2158849 [2] NCCL INFO Connected NVLS tree
 2: nodeai01:2157662:2158849 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 2: nodeai01:2157662:2158849 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
21: nodeai04:2417508:2418695 [5] NCCL INFO Connected NVLS tree
21: nodeai04:2417508:2418695 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
21: nodeai04:2417508:2418695 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
15: nodeai02:2459391:2460581 [7] NCCL INFO Connected NVLS tree
15: nodeai02:2459391:2460581 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
15: nodeai02:2459391:2460581 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
19: nodeai04:2417510:2418691 [3] NCCL INFO Connected NVLS tree
19: nodeai04:2417510:2418691 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
19: nodeai04:2417510:2418691 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
23: nodeai04:2417507:2418697 [7] NCCL INFO Connected NVLS tree
23: nodeai04:2417507:2418697 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
23: nodeai04:2417507:2418697 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 2: nodeai01:2157662:2158849 [2] NCCL INFO comm 0x556a1ae3d180 rank 2 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init COMPLETE
 3: nodeai01:2157667:2158850 [3] NCCL INFO comm 0x5571536aaac0 rank 3 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init COMPLETE
 4: nodeai01:2157666:2158845 [4] NCCL INFO comm 0x556b87fd8840 rank 4 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init COMPLETE
 6: nodeai01:2157661:2158844 [6] NCCL INFO comm 0x55aa516d7b00 rank 6 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init COMPLETE
 7: nodeai01:2157664:2158847 [7] NCCL INFO comm 0x561beaa3d7c0 rank 7 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init COMPLETE
 0: nodeai01:2157660:2158843 [0] NCCL INFO comm 0x5609149c7900 rank 0 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init COMPLETE
 5: nodeai01:2157665:2158846 [5] NCCL INFO comm 0x55f9facf8dc0 rank 5 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init COMPLETE
 1: nodeai01:2157663:2158848 [1] NCCL INFO comm 0x55cb52b80bc0 rank 1 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init COMPLETE
27: nodeai05:1971446:1972637 [3] NCCL INFO comm 0x55bd9df48280 rank 27 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init COMPLETE
29: nodeai05:1971444:1972635 [5] NCCL INFO comm 0x56523b50c3c0 rank 29 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init COMPLETE
31: nodeai05:1971449:1972636 [7] NCCL INFO comm 0x55a01ffbc340 rank 31 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init COMPLETE
25: nodeai05:1971445:1972631 [1] NCCL INFO comm 0x55c3e3c1bc80 rank 25 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init COMPLETE
24: nodeai05:1971447:1972632 [0] NCCL INFO comm 0x5643eedcebc0 rank 24 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init COMPLETE
28: nodeai05:1971442:1972634 [4] NCCL INFO comm 0x560ddfef6e80 rank 28 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init COMPLETE
26: nodeai05:1971443:1972633 [2] NCCL INFO comm 0x55be6917b200 rank 26 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init COMPLETE
30: nodeai05:1971448:1972630 [6] NCCL INFO comm 0x55754205dbc0 rank 30 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init COMPLETE
32: nodeai06:2504436:2505627 [0] NCCL INFO comm 0x56050cd8ad80 rank 32 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init COMPLETE
33: nodeai06:2504433:2505622 [1] NCCL INFO comm 0x5601eae71980 rank 33 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init COMPLETE
34: nodeai06:2504435:2505620 [2] NCCL INFO comm 0x5630f97acd40 rank 34 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init COMPLETE
35: nodeai06:2504431:2505625 [3] NCCL INFO comm 0x55cbe345b740 rank 35 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init COMPLETE
36: nodeai06:2504434:2505624 [4] NCCL INFO comm 0x5619adeb56c0 rank 36 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init COMPLETE
37: nodeai06:2504432:2505626 [5] NCCL INFO comm 0x5622d3b13780 rank 37 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init COMPLETE
38: nodeai06:2504437:2505621 [6] NCCL INFO comm 0x55f04b397a80 rank 38 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init COMPLETE
39: nodeai06:2504430:2505623 [7] NCCL INFO comm 0x5654f999ea00 rank 39 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init COMPLETE
 8: nodeai02:2459395:2460579 [0] NCCL INFO comm 0x55adf155d980 rank 8 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init COMPLETE
 9: nodeai02:2459396:2460580 [1] NCCL INFO comm 0x559b064037c0 rank 9 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init COMPLETE
10: nodeai02:2459393:2460578 [2] NCCL INFO comm 0x556f37828780 rank 10 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init COMPLETE
11: nodeai02:2459390:2460577 [3] NCCL INFO comm 0x564a9a78c300 rank 11 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init COMPLETE
12: nodeai02:2459394:2460576 [4] NCCL INFO comm 0x55d806bc8ac0 rank 12 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init COMPLETE
13: nodeai02:2459392:2460582 [5] NCCL INFO comm 0x55d980822880 rank 13 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init COMPLETE
14: nodeai02:2459389:2460583 [6] NCCL INFO comm 0x55ec378e1840 rank 14 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init COMPLETE
15: nodeai02:2459391:2460581 [7] NCCL INFO comm 0x5619f3e1f040 rank 15 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init COMPLETE
16: nodeai04:2417513:2418698 [0] NCCL INFO comm 0x55ea13bc9dc0 rank 16 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x6e735a9c9818b595 - Init COMPLETE
17: nodeai04:2417509:2418692 [1] NCCL INFO comm 0x55c6a815d6c0 rank 17 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x6e735a9c9818b595 - Init COMPLETE
18: nodeai04:2417512:2418693 [2] NCCL INFO comm 0x564c4f3cf680 rank 18 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x6e735a9c9818b595 - Init COMPLETE
19: nodeai04:2417510:2418691 [3] NCCL INFO comm 0x55e304d4de40 rank 19 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x6e735a9c9818b595 - Init COMPLETE
20: nodeai04:2417506:2418694 [4] NCCL INFO comm 0x5615b35ac500 rank 20 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x6e735a9c9818b595 - Init COMPLETE
21: nodeai04:2417508:2418695 [5] NCCL INFO comm 0x558756c399c0 rank 21 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x6e735a9c9818b595 - Init COMPLETE
22: nodeai04:2417511:2418696 [6] NCCL INFO comm 0x560f6bdcbc80 rank 22 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x6e735a9c9818b595 - Init COMPLETE
23: nodeai04:2417507:2418697 [7] NCCL INFO comm 0x55a36f03a600 rank 23 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x6e735a9c9818b595 - Init COMPLETE
32: NCCL version 2.18.5+cuda12.2
16: NCCL version 2.18.5+cuda12.2
 0: nodeai01:2157660:2158956 [0] NCCL INFO Using network IBext
 3: nodeai01:2157667:2158958 [3] NCCL INFO Using network IBext
 4: nodeai01:2157666:2158957 [4] NCCL INFO Using network IBext
 6: nodeai01:2157661:2158959 [6] NCCL INFO Using network IBext
 7: nodeai01:2157664:2158962 [7] NCCL INFO Using network IBext
 8: NCCL version 2.18.5+cuda12.2
24: NCCL version 2.18.5+cuda12.2
40: NCCL version 2.18.5+cuda12.2
 1: nodeai01:2157663:2158960 [1] NCCL INFO Using network IBext
 2: nodeai01:2157662:2158961 [2] NCCL INFO Using network IBext
 5: nodeai01:2157665:2158963 [5] NCCL INFO Using network IBext
32: nodeai06:2504436:2505735 [0] NCCL INFO Using network IBext
16: nodeai04:2417513:2418803 [0] NCCL INFO Using network IBext
40: nodeai07:2435734:2437028 [0] NCCL INFO Using network IBext
33: nodeai06:2504433:2505737 [1] NCCL INFO Using network IBext
35: nodeai06:2504431:2505736 [3] NCCL INFO Using network IBext
36: nodeai06:2504434:2505739 [4] NCCL INFO Using network IBext
24: nodeai05:1971447:1972739 [0] NCCL INFO Using network IBext
 8: nodeai02:2459395:2460692 [0] NCCL INFO Using network IBext
38: nodeai06:2504437:2505740 [6] NCCL INFO Using network IBext
37: nodeai06:2504432:2505738 [5] NCCL INFO Using network IBext
41: nodeai07:2435735:2437033 [1] NCCL INFO Using network IBext
39: nodeai06:2504430:2505741 [7] NCCL INFO Using network IBext
34: nodeai06:2504435:2505742 [2] NCCL INFO Using network IBext
42: nodeai07:2435730:2437034 [2] NCCL INFO Using network IBext
18: nodeai04:2417512:2418808 [2] NCCL INFO Using network IBext
19: nodeai04:2417510:2418807 [3] NCCL INFO Using network IBext
23: nodeai04:2417507:2418809 [7] NCCL INFO Using network IBext
17: nodeai04:2417509:2418810 [1] NCCL INFO Using network IBext
20: nodeai04:2417506:2418811 [4] NCCL INFO Using network IBext
43: nodeai07:2435733:2437035 [3] NCCL INFO Using network IBext
 9: nodeai02:2459396:2460695 [1] NCCL INFO Using network IBext
21: nodeai04:2417508:2418812 [5] NCCL INFO Using network IBext
22: nodeai04:2417511:2418813 [6] NCCL INFO Using network IBext
27: nodeai05:1971446:1972743 [3] NCCL INFO Using network IBext
26: nodeai05:1971443:1972744 [2] NCCL INFO Using network IBext
25: nodeai05:1971445:1972746 [1] NCCL INFO Using network IBext
29: nodeai05:1971444:1972745 [5] NCCL INFO Using network IBext
46: nodeai07:2435731:2437036 [6] NCCL INFO Using network IBext
47: nodeai07:2435736:2437037 [7] NCCL INFO Using network IBext
44: nodeai07:2435729:2437038 [4] NCCL INFO Using network IBext
10: nodeai02:2459393:2460697 [2] NCCL INFO Using network IBext
12: nodeai02:2459394:2460696 [4] NCCL INFO Using network IBext
45: nodeai07:2435732:2437039 [5] NCCL INFO Using network IBext
11: nodeai02:2459390:2460698 [3] NCCL INFO Using network IBext
15: nodeai02:2459391:2460699 [7] NCCL INFO Using network IBext
30: nodeai05:1971448:1972747 [6] NCCL INFO Using network IBext
28: nodeai05:1971442:1972748 [4] NCCL INFO Using network IBext
34: nodeai06:2504435:2505742 [2] NCCL INFO comm 0x5630f97a2400 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x244ce223fdce877b - Init START
31: nodeai05:1971449:1972749 [7] NCCL INFO Using network IBext
14: nodeai02:2459389:2460700 [6] NCCL INFO Using network IBext
33: nodeai06:2504433:2505737 [1] NCCL INFO comm 0x5601eae6d140 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x244ce223fdce877b - Init START
37: nodeai06:2504432:2505738 [5] NCCL INFO comm 0x5622d3b08e40 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x244ce223fdce877b - Init START
38: nodeai06:2504437:2505740 [6] NCCL INFO comm 0x55f04b393240 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x244ce223fdce877b - Init START
39: nodeai06:2504430:2505741 [7] NCCL INFO comm 0x5654f999a1c0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x244ce223fdce877b - Init START
32: nodeai06:2504436:2505735 [0] NCCL INFO comm 0x56050cd86540 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x244ce223fdce877b - Init START
35: nodeai06:2504431:2505736 [3] NCCL INFO comm 0x55cbe3456f00 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x244ce223fdce877b - Init START
36: nodeai06:2504434:2505739 [4] NCCL INFO comm 0x5619adeaad80 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x244ce223fdce877b - Init START
13: nodeai02:2459392:2460701 [5] NCCL INFO Using network IBext
17: nodeai04:2417509:2418810 [1] NCCL INFO comm 0x55c6a8158e80 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x7aceba865fd02d4c - Init START
18: nodeai04:2417512:2418808 [2] NCCL INFO comm 0x564c4f3cae40 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x7aceba865fd02d4c - Init START
23: nodeai04:2417507:2418809 [7] NCCL INFO comm 0x55a36f02fcc0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x7aceba865fd02d4c - Init START
16: nodeai04:2417513:2418803 [0] NCCL INFO comm 0x55ea13bbf480 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x7aceba865fd02d4c - Init START
19: nodeai04:2417510:2418807 [3] NCCL INFO comm 0x55e304d49600 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x7aceba865fd02d4c - Init START
20: nodeai04:2417506:2418811 [4] NCCL INFO comm 0x5615b35a1bc0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x7aceba865fd02d4c - Init START
21: nodeai04:2417508:2418812 [5] NCCL INFO comm 0x558756c2f080 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x7aceba865fd02d4c - Init START
22: nodeai04:2417511:2418813 [6] NCCL INFO comm 0x560f6bdc1340 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x7aceba865fd02d4c - Init START
30: nodeai05:1971448:1972747 [6] NCCL INFO comm 0x557542053280 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x534086292a0b7ada - Init START
26: nodeai05:1971443:1972744 [2] NCCL INFO comm 0x55be691708c0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x534086292a0b7ada - Init START
27: nodeai05:1971446:1972743 [3] NCCL INFO comm 0x55bd9df3d940 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x534086292a0b7ada - Init START
28: nodeai05:1971442:1972748 [4] NCCL INFO comm 0x560ddfeec540 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x534086292a0b7ada - Init START
29: nodeai05:1971444:1972745 [5] NCCL INFO comm 0x56523b507b80 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x534086292a0b7ada - Init START
24: nodeai05:1971447:1972739 [0] NCCL INFO comm 0x5643eedc4280 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x534086292a0b7ada - Init START
25: nodeai05:1971445:1972746 [1] NCCL INFO comm 0x55c3e3c11340 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x534086292a0b7ada - Init START
31: nodeai05:1971449:1972749 [7] NCCL INFO comm 0x55a01ffb1a00 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x534086292a0b7ada - Init START
14: nodeai02:2459389:2460700 [6] NCCL INFO comm 0x55ec378dd000 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x61ece085497a7064 - Init START
 9: nodeai02:2459396:2460695 [1] NCCL INFO comm 0x559b063f8e80 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x61ece085497a7064 - Init START
10: nodeai02:2459393:2460697 [2] NCCL INFO comm 0x556f37823f40 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x61ece085497a7064 - Init START
11: nodeai02:2459390:2460698 [3] NCCL INFO comm 0x564a9a889f40 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x61ece085497a7064 - Init START
13: nodeai02:2459392:2460701 [5] NCCL INFO comm 0x55d98081e040 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x61ece085497a7064 - Init START
15: nodeai02:2459391:2460699 [7] NCCL INFO comm 0x5619f3e14700 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x61ece085497a7064 - Init START
 8: nodeai02:2459395:2460692 [0] NCCL INFO comm 0x55adf1553040 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x61ece085497a7064 - Init START
12: nodeai02:2459394:2460696 [4] NCCL INFO comm 0x55d806bbe180 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x61ece085497a7064 - Init START
 0: nodeai01:2157660:2158956 [0] NCCL INFO comm 0x5609149bcfc0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x3838e9a601809337 - Init START
 1: nodeai01:2157663:2158960 [1] NCCL INFO comm 0x55cb52b76280 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x3838e9a601809337 - Init START
 2: nodeai01:2157662:2158961 [2] NCCL INFO comm 0x556a1ae32840 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x3838e9a601809337 - Init START
 3: nodeai01:2157667:2158958 [3] NCCL INFO comm 0x5571536a0180 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x3838e9a601809337 - Init START
 4: nodeai01:2157666:2158957 [4] NCCL INFO comm 0x556b87fd4000 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x3838e9a601809337 - Init START
 5: nodeai01:2157665:2158963 [5] NCCL INFO comm 0x55f9facf4580 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x3838e9a601809337 - Init START
 6: nodeai01:2157661:2158959 [6] NCCL INFO comm 0x55aa516cd1c0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x3838e9a601809337 - Init START
 7: nodeai01:2157664:2158962 [7] NCCL INFO comm 0x561beaa32e80 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x3838e9a601809337 - Init START
40: nodeai07:2435734:2437028 [0] NCCL INFO comm 0x5581fe0e0d80 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x66d16d2583d6a9f3 - Init START
41: nodeai07:2435735:2437033 [1] NCCL INFO comm 0x55cf54557340 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x66d16d2583d6a9f3 - Init START
42: nodeai07:2435730:2437034 [2] NCCL INFO comm 0x55c42d8b2240 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x66d16d2583d6a9f3 - Init START
43: nodeai07:2435733:2437035 [3] NCCL INFO comm 0x559b9d033d80 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x66d16d2583d6a9f3 - Init START
44: nodeai07:2435729:2437038 [4] NCCL INFO comm 0x564b194c3700 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x66d16d2583d6a9f3 - Init START
45: nodeai07:2435732:2437039 [5] NCCL INFO comm 0x56314f515c00 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x66d16d2583d6a9f3 - Init START
46: nodeai07:2435731:2437036 [6] NCCL INFO comm 0x561627d216c0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x66d16d2583d6a9f3 - Init START
47: nodeai07:2435736:2437037 [7] NCCL INFO comm 0x564e1f981b80 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x66d16d2583d6a9f3 - Init START
25: nodeai05:1971445:1972746 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
25: nodeai05:1971445:1972746 [1] NCCL INFO NVLS multicast support is available on dev 1
26: nodeai05:1971443:1972744 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
26: nodeai05:1971443:1972744 [2] NCCL INFO NVLS multicast support is available on dev 2
29: nodeai05:1971444:1972745 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
29: nodeai05:1971444:1972745 [5] NCCL INFO NVLS multicast support is available on dev 5
28: nodeai05:1971442:1972748 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
28: nodeai05:1971442:1972748 [4] NCCL INFO NVLS multicast support is available on dev 4
24: nodeai05:1971447:1972739 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
24: nodeai05:1971447:1972739 [0] NCCL INFO NVLS multicast support is available on dev 0
31: nodeai05:1971449:1972749 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
31: nodeai05:1971449:1972749 [7] NCCL INFO NVLS multicast support is available on dev 7
30: nodeai05:1971448:1972747 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
30: nodeai05:1971448:1972747 [6] NCCL INFO NVLS multicast support is available on dev 6
27: nodeai05:1971446:1972743 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
27: nodeai05:1971446:1972743 [3] NCCL INFO NVLS multicast support is available on dev 3
26: nodeai05:1971443:1972744 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
26: nodeai05:1971443:1972744 [2] NCCL INFO P2P Chunksize set to 524288
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
27: nodeai05:1971446:1972743 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
27: nodeai05:1971446:1972743 [3] NCCL INFO P2P Chunksize set to 524288
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
25: nodeai05:1971445:1972746 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
25: nodeai05:1971445:1972746 [1] NCCL INFO P2P Chunksize set to 524288
28: nodeai05:1971442:1972748 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
28: nodeai05:1971442:1972748 [4] NCCL INFO P2P Chunksize set to 524288
29: nodeai05:1971444:1972745 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
29: nodeai05:1971444:1972745 [5] NCCL INFO P2P Chunksize set to 524288
30: nodeai05:1971448:1972747 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
30: nodeai05:1971448:1972747 [6] NCCL INFO P2P Chunksize set to 524288
31: nodeai05:1971449:1972749 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
31: nodeai05:1971449:1972749 [7] NCCL INFO P2P Chunksize set to 524288
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
24: nodeai05:1971447:1972739 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
24: nodeai05:1971447:1972739 [0] NCCL INFO P2P Chunksize set to 524288
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
47: nodeai07:2435736:2437037 [7] NCCL INFO NVLS multicast support is available on dev 7
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
42: nodeai07:2435730:2437034 [2] NCCL INFO NVLS multicast support is available on dev 2
24: nodeai05:1971447:1972739 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
43: nodeai07:2435733:2437035 [3] NCCL INFO NVLS multicast support is available on dev 3
45: nodeai07:2435732:2437039 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
45: nodeai07:2435732:2437039 [5] NCCL INFO NVLS multicast support is available on dev 5
44: nodeai07:2435729:2437038 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
44: nodeai07:2435729:2437038 [4] NCCL INFO NVLS multicast support is available on dev 4
46: nodeai07:2435731:2437036 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
46: nodeai07:2435731:2437036 [6] NCCL INFO NVLS multicast support is available on dev 6
41: nodeai07:2435735:2437033 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
41: nodeai07:2435735:2437033 [1] NCCL INFO NVLS multicast support is available on dev 1
40: nodeai07:2435734:2437028 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
40: nodeai07:2435734:2437028 [0] NCCL INFO NVLS multicast support is available on dev 0
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
46: nodeai07:2435731:2437036 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
46: nodeai07:2435731:2437036 [6] NCCL INFO P2P Chunksize set to 524288
47: nodeai07:2435736:2437037 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
47: nodeai07:2435736:2437037 [7] NCCL INFO P2P Chunksize set to 524288
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
40: nodeai07:2435734:2437028 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
40: nodeai07:2435734:2437028 [0] NCCL INFO P2P Chunksize set to 524288
41: nodeai07:2435735:2437033 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
41: nodeai07:2435735:2437033 [1] NCCL INFO P2P Chunksize set to 524288
42: nodeai07:2435730:2437034 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
42: nodeai07:2435730:2437034 [2] NCCL INFO P2P Chunksize set to 524288
43: nodeai07:2435733:2437035 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
43: nodeai07:2435733:2437035 [3] NCCL INFO P2P Chunksize set to 524288
44: nodeai07:2435729:2437038 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
44: nodeai07:2435729:2437038 [4] NCCL INFO P2P Chunksize set to 524288
45: nodeai07:2435732:2437039 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
45: nodeai07:2435732:2437039 [5] NCCL INFO P2P Chunksize set to 524288
14: nodeai02:2459389:2460700 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
14: nodeai02:2459389:2460700 [6] NCCL INFO NVLS multicast support is available on dev 6
12: nodeai02:2459394:2460696 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
12: nodeai02:2459394:2460696 [4] NCCL INFO NVLS multicast support is available on dev 4
15: nodeai02:2459391:2460699 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
15: nodeai02:2459391:2460699 [7] NCCL INFO NVLS multicast support is available on dev 7
13: nodeai02:2459392:2460701 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
11: nodeai02:2459390:2460698 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
13: nodeai02:2459392:2460701 [5] NCCL INFO NVLS multicast support is available on dev 5
11: nodeai02:2459390:2460698 [3] NCCL INFO NVLS multicast support is available on dev 3
10: nodeai02:2459393:2460697 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
10: nodeai02:2459393:2460697 [2] NCCL INFO NVLS multicast support is available on dev 2
 9: nodeai02:2459396:2460695 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
 9: nodeai02:2459396:2460695 [1] NCCL INFO NVLS multicast support is available on dev 1
 8: nodeai02:2459395:2460692 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
 8: nodeai02:2459395:2460692 [0] NCCL INFO NVLS multicast support is available on dev 0
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
15: nodeai02:2459391:2460699 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
15: nodeai02:2459391:2460699 [7] NCCL INFO P2P Chunksize set to 524288
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
 9: nodeai02:2459396:2460695 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
 9: nodeai02:2459396:2460695 [1] NCCL INFO P2P Chunksize set to 524288
10: nodeai02:2459393:2460697 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
10: nodeai02:2459393:2460697 [2] NCCL INFO P2P Chunksize set to 524288
11: nodeai02:2459390:2460698 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
11: nodeai02:2459390:2460698 [3] NCCL INFO P2P Chunksize set to 524288
12: nodeai02:2459394:2460696 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
12: nodeai02:2459394:2460696 [4] NCCL INFO P2P Chunksize set to 524288
13: nodeai02:2459392:2460701 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
13: nodeai02:2459392:2460701 [5] NCCL INFO P2P Chunksize set to 524288
14: nodeai02:2459389:2460700 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
14: nodeai02:2459389:2460700 [6] NCCL INFO P2P Chunksize set to 524288
 8: nodeai02:2459395:2460692 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
 8: nodeai02:2459395:2460692 [0] NCCL INFO P2P Chunksize set to 524288
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
36: nodeai06:2504434:2505739 [4] NCCL INFO NVLS multicast support is available on dev 4
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO NVLS multicast support is available on dev 5
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
38: nodeai06:2504437:2505740 [6] NCCL INFO NVLS multicast support is available on dev 6
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
35: nodeai06:2504431:2505736 [3] NCCL INFO NVLS multicast support is available on dev 3
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437028 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
39: nodeai06:2504430:2505741 [7] NCCL INFO NVLS multicast support is available on dev 7
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
34: nodeai06:2504435:2505742 [2] NCCL INFO NVLS multicast support is available on dev 2
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
33: nodeai06:2504433:2505737 [1] NCCL INFO NVLS multicast support is available on dev 1
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
32: nodeai06:2504436:2505735 [0] NCCL INFO NVLS multicast support is available on dev 0
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
39: nodeai06:2504430:2505741 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
39: nodeai06:2504430:2505741 [7] NCCL INFO P2P Chunksize set to 524288
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
33: nodeai06:2504433:2505737 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
33: nodeai06:2504433:2505737 [1] NCCL INFO P2P Chunksize set to 524288
34: nodeai06:2504435:2505742 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
34: nodeai06:2504435:2505742 [2] NCCL INFO P2P Chunksize set to 524288
35: nodeai06:2504431:2505736 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
35: nodeai06:2504431:2505736 [3] NCCL INFO P2P Chunksize set to 524288
36: nodeai06:2504434:2505739 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
36: nodeai06:2504434:2505739 [4] NCCL INFO P2P Chunksize set to 524288
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
37: nodeai06:2504432:2505738 [5] NCCL INFO P2P Chunksize set to 524288
38: nodeai06:2504437:2505740 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
38: nodeai06:2504437:2505740 [6] NCCL INFO P2P Chunksize set to 524288
32: nodeai06:2504436:2505735 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
32: nodeai06:2504436:2505735 [0] NCCL INFO P2P Chunksize set to 524288
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC
 8: nodeai02:2459395:2460692 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
17: nodeai04:2417509:2418810 [1] NCCL INFO NVLS multicast support is available on dev 1
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
23: nodeai04:2417507:2418809 [7] NCCL INFO NVLS multicast support is available on dev 7
22: nodeai04:2417511:2418813 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
22: nodeai04:2417511:2418813 [6] NCCL INFO NVLS multicast support is available on dev 6
18: nodeai04:2417512:2418808 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
18: nodeai04:2417512:2418808 [2] NCCL INFO NVLS multicast support is available on dev 2
16: nodeai04:2417513:2418803 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
16: nodeai04:2417513:2418803 [0] NCCL INFO NVLS multicast support is available on dev 0
21: nodeai04:2417508:2418812 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
21: nodeai04:2417508:2418812 [5] NCCL INFO NVLS multicast support is available on dev 5
20: nodeai04:2417506:2418811 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
20: nodeai04:2417506:2418811 [4] NCCL INFO NVLS multicast support is available on dev 4
19: nodeai04:2417510:2418807 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
19: nodeai04:2417510:2418807 [3] NCCL INFO NVLS multicast support is available on dev 3
18: nodeai04:2417512:2418808 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
18: nodeai04:2417512:2418808 [2] NCCL INFO P2P Chunksize set to 524288
19: nodeai04:2417510:2418807 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
19: nodeai04:2417510:2418807 [3] NCCL INFO P2P Chunksize set to 524288
17: nodeai04:2417509:2418810 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
17: nodeai04:2417509:2418810 [1] NCCL INFO P2P Chunksize set to 524288
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
22: nodeai04:2417511:2418813 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
22: nodeai04:2417511:2418813 [6] NCCL INFO P2P Chunksize set to 524288
23: nodeai04:2417507:2418809 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
23: nodeai04:2417507:2418809 [7] NCCL INFO P2P Chunksize set to 524288
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
16: nodeai04:2417513:2418803 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
16: nodeai04:2417513:2418803 [0] NCCL INFO P2P Chunksize set to 524288
20: nodeai04:2417506:2418811 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
20: nodeai04:2417506:2418811 [4] NCCL INFO P2P Chunksize set to 524288
21: nodeai04:2417508:2418812 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
21: nodeai04:2417508:2418812 [5] NCCL INFO P2P Chunksize set to 524288
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
 3: nodeai01:2157667:2158958 [3] NCCL INFO NVLS multicast support is available on dev 3
 2: nodeai01:2157662:2158961 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
 2: nodeai01:2157662:2158961 [2] NCCL INFO NVLS multicast support is available on dev 2
 5: nodeai01:2157665:2158963 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 5: nodeai01:2157665:2158963 [5] NCCL INFO NVLS multicast support is available on dev 5
 0: nodeai01:2157660:2158956 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
 0: nodeai01:2157660:2158956 [0] NCCL INFO NVLS multicast support is available on dev 0
 6: nodeai01:2157661:2158959 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 6: nodeai01:2157661:2158959 [6] NCCL INFO NVLS multicast support is available on dev 6
 4: nodeai01:2157666:2158957 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 4: nodeai01:2157666:2158957 [4] NCCL INFO NVLS multicast support is available on dev 4
 1: nodeai01:2157663:2158960 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
 1: nodeai01:2157663:2158960 [1] NCCL INFO NVLS multicast support is available on dev 1
 7: nodeai01:2157664:2158962 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 7: nodeai01:2157664:2158962 [7] NCCL INFO NVLS multicast support is available on dev 7
 6: nodeai01:2157661:2158959 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
 6: nodeai01:2157661:2158959 [6] NCCL INFO P2P Chunksize set to 524288
 7: nodeai01:2157664:2158962 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
 1: nodeai01:2157663:2158960 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
 1: nodeai01:2157663:2158960 [1] NCCL INFO P2P Chunksize set to 524288
 3: nodeai01:2157667:2158958 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
 3: nodeai01:2157667:2158958 [3] NCCL INFO P2P Chunksize set to 524288
 4: nodeai01:2157666:2158957 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
 4: nodeai01:2157666:2158957 [4] NCCL INFO P2P Chunksize set to 524288
 5: nodeai01:2157665:2158963 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
 5: nodeai01:2157665:2158963 [5] NCCL INFO P2P Chunksize set to 524288
 7: nodeai01:2157664:2158962 [7] NCCL INFO P2P Chunksize set to 524288
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
 0: nodeai01:2157660:2158956 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
 0: nodeai01:2157660:2158956 [0] NCCL INFO P2P Chunksize set to 524288
 2: nodeai01:2157662:2158961 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
 2: nodeai01:2157662:2158961 [2] NCCL INFO P2P Chunksize set to 524288
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Connected all rings
46: nodeai07:2435731:2437036 [6] NCCL INFO Connected all rings
15: nodeai02:2459391:2460699 [7] NCCL INFO Connected all rings
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Connected all rings
44: nodeai07:2435729:2437038 [4] NCCL INFO Connected all rings
29: nodeai05:1971444:1972745 [5] NCCL INFO Connected all rings
12: nodeai02:2459394:2460696 [4] NCCL INFO Connected all rings
31: nodeai05:1971449:1972749 [7] NCCL INFO Connected all rings
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Connected all rings
14: nodeai02:2459389:2460700 [6] NCCL INFO Connected all rings
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Connected all rings
25: nodeai05:1971445:1972746 [1] NCCL INFO Connected all rings
47: nodeai07:2435736:2437037 [7] NCCL INFO Connected all rings
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
24: nodeai05:1971447:1972739 [0] NCCL INFO Connected all rings
40: nodeai07:2435734:2437028 [0] NCCL INFO Connected all rings
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Connected all rings
26: nodeai05:1971443:1972744 [2] NCCL INFO Connected all rings
42: nodeai07:2435730:2437034 [2] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460692 [0] NCCL INFO Connected all rings
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Connected all rings
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Connected all rings
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Connected all rings
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Connected all rings
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Connected all rings
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC
16: nodeai04:2417513:2418803 [0] NCCL INFO Connected all rings
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Connected all rings
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
31: nodeai05:1971449:1972749 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Connected all rings
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
46: nodeai07:2435731:2437036 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Connected all rings
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC
43: nodeai07:2435733:2437035 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC
44: nodeai07:2435729:2437038 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
45: nodeai07:2435732:2437039 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Connected all rings
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC
15: nodeai02:2459391:2460699 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972748 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC
29: nodeai05:1971444:1972745 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
25: nodeai05:1971445:1972746 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC
26: nodeai05:1971443:1972744 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
30: nodeai05:1971448:1972747 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
41: nodeai07:2435735:2437033 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
42: nodeai07:2435730:2437034 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Connected all rings
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
27: nodeai05:1971446:1972743 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Connected all rings
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Connected all rings
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
12: nodeai02:2459394:2460696 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC
14: nodeai02:2459389:2460700 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460698 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC
10: nodeai02:2459393:2460697 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC
13: nodeai02:2459392:2460701 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460695 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418809 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
32: nodeai06:2504436:2505735 [0] NCCL INFO Connected all rings
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC
20: nodeai04:2417506:2418811 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC
18: nodeai04:2417512:2418808 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC
17: nodeai04:2417509:2418810 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Connected all rings
21: nodeai04:2417508:2418812 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC
22: nodeai04:2417511:2418813 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC
19: nodeai04:2417510:2418807 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Connected all rings
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Connected all rings
 5: nodeai01:2157665:2158963 [5] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Connected all rings
37: nodeai06:2504432:2505738 [5] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Connected all rings
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505741 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158956 [0] NCCL INFO Connected all rings
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC
35: nodeai06:2504431:2505736 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505742 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC
38: nodeai06:2504437:2505740 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC
36: nodeai06:2504434:2505739 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC
37: nodeai06:2504432:2505738 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Connected all rings
 1: nodeai01:2157663:2158960 [1] NCCL INFO Connected all rings
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158962 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158963 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158959 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC
 2: nodeai01:2157662:2158961 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158957 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC
33: nodeai06:2504433:2505737 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC
 1: nodeai01:2157663:2158960 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC
 3: nodeai01:2157667:2158958 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC
47: nodeai07:2435736:2437037 [7] NCCL INFO Connected all trees
47: nodeai07:2435736:2437037 [7] NCCL INFO NVLS comm 0x564e1f981b80 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
15: nodeai02:2459391:2460699 [7] NCCL INFO Connected all trees
15: nodeai02:2459391:2460699 [7] NCCL INFO NVLS comm 0x5619f3e14700 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
24: nodeai05:1971447:1972739 [0] NCCL INFO Connected all trees
24: nodeai05:1971447:1972739 [0] NCCL INFO NVLS comm 0x5643eedc4280 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 8: nodeai02:2459395:2460692 [0] NCCL INFO Connected all trees
 8: nodeai02:2459395:2460692 [0] NCCL INFO NVLS comm 0x55adf1553040 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
31: nodeai05:1971449:1972749 [7] NCCL INFO Connected all trees
31: nodeai05:1971449:1972749 [7] NCCL INFO NVLS comm 0x55a01ffb1a00 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
40: nodeai07:2435734:2437028 [0] NCCL INFO Connected all trees
40: nodeai07:2435734:2437028 [0] NCCL INFO NVLS comm 0x5581fe0e0d80 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
16: nodeai04:2417513:2418803 [0] NCCL INFO Connected all trees
16: nodeai04:2417513:2418803 [0] NCCL INFO NVLS comm 0x55ea13bbf480 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
23: nodeai04:2417507:2418809 [7] NCCL INFO Connected all trees
23: nodeai04:2417507:2418809 [7] NCCL INFO NVLS comm 0x55a36f02fcc0 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
39: nodeai06:2504430:2505741 [7] NCCL INFO Connected all trees
39: nodeai06:2504430:2505741 [7] NCCL INFO NVLS comm 0x5654f999a1c0 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 7: nodeai01:2157664:2158962 [7] NCCL INFO Connected all trees
 7: nodeai01:2157664:2158962 [7] NCCL INFO NVLS comm 0x561beaa32e80 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
32: nodeai06:2504436:2505735 [0] NCCL INFO Connected all trees
32: nodeai06:2504436:2505735 [0] NCCL INFO NVLS comm 0x56050cd86540 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 0: nodeai01:2157660:2158956 [0] NCCL INFO Connected all trees
 0: nodeai01:2157660:2158956 [0] NCCL INFO NVLS comm 0x5609149bcfc0 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
44: nodeai07:2435729:2437038 [4] NCCL INFO Connected all trees
44: nodeai07:2435729:2437038 [4] NCCL INFO NVLS comm 0x564b194c3700 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
46: nodeai07:2435731:2437036 [6] NCCL INFO Connected all trees
46: nodeai07:2435731:2437036 [6] NCCL INFO NVLS comm 0x561627d216c0 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
35: nodeai06:2504431:2505736 [3] NCCL INFO Connected all trees
35: nodeai06:2504431:2505736 [3] NCCL INFO NVLS comm 0x55cbe3456f00 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
45: nodeai07:2435732:2437039 [5] NCCL INFO Connected all trees
45: nodeai07:2435732:2437039 [5] NCCL INFO NVLS comm 0x56314f515c00 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
21: nodeai04:2417508:2418812 [5] NCCL INFO Connected all trees
21: nodeai04:2417508:2418812 [5] NCCL INFO NVLS comm 0x558756c2f080 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
11: nodeai02:2459390:2460698 [3] NCCL INFO Connected all trees
11: nodeai02:2459390:2460698 [3] NCCL INFO NVLS comm 0x564a9a889f40 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
29: nodeai05:1971444:1972745 [5] NCCL INFO Connected all trees
29: nodeai05:1971444:1972745 [5] NCCL INFO NVLS comm 0x56523b507b80 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
25: nodeai05:1971445:1972746 [1] NCCL INFO Connected all trees
25: nodeai05:1971445:1972746 [1] NCCL INFO NVLS comm 0x55c3e3c11340 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
18: nodeai04:2417512:2418808 [2] NCCL INFO Connected all trees
26: nodeai05:1971443:1972744 [2] NCCL INFO Connected all trees
18: nodeai04:2417512:2418808 [2] NCCL INFO NVLS comm 0x564c4f3cae40 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
26: nodeai05:1971443:1972744 [2] NCCL INFO NVLS comm 0x55be691708c0 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
13: nodeai02:2459392:2460701 [5] NCCL INFO Connected all trees
13: nodeai02:2459392:2460701 [5] NCCL INFO NVLS comm 0x55d98081e040 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
28: nodeai05:1971442:1972748 [4] NCCL INFO Connected all trees
28: nodeai05:1971442:1972748 [4] NCCL INFO NVLS comm 0x560ddfeec540 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 9: nodeai02:2459396:2460695 [1] NCCL INFO Connected all trees
 9: nodeai02:2459396:2460695 [1] NCCL INFO NVLS comm 0x559b063f8e80 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
10: nodeai02:2459393:2460697 [2] NCCL INFO Connected all trees
10: nodeai02:2459393:2460697 [2] NCCL INFO NVLS comm 0x556f37823f40 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
42: nodeai07:2435730:2437034 [2] NCCL INFO Connected all trees
42: nodeai07:2435730:2437034 [2] NCCL INFO NVLS comm 0x55c42d8b2240 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
30: nodeai05:1971448:1972747 [6] NCCL INFO Connected all trees
30: nodeai05:1971448:1972747 [6] NCCL INFO NVLS comm 0x557542053280 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
41: nodeai07:2435735:2437033 [1] NCCL INFO Connected all trees
41: nodeai07:2435735:2437033 [1] NCCL INFO NVLS comm 0x55cf54557340 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
27: nodeai05:1971446:1972743 [3] NCCL INFO Connected all trees
27: nodeai05:1971446:1972743 [3] NCCL INFO NVLS comm 0x55bd9df3d940 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
14: nodeai02:2459389:2460700 [6] NCCL INFO Connected all trees
14: nodeai02:2459389:2460700 [6] NCCL INFO NVLS comm 0x55ec378dd000 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
19: nodeai04:2417510:2418807 [3] NCCL INFO Connected all trees
19: nodeai04:2417510:2418807 [3] NCCL INFO NVLS comm 0x55e304d49600 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
43: nodeai07:2435733:2437035 [3] NCCL INFO Connected all trees
43: nodeai07:2435733:2437035 [3] NCCL INFO NVLS comm 0x559b9d033d80 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
22: nodeai04:2417511:2418813 [6] NCCL INFO Connected all trees
22: nodeai04:2417511:2418813 [6] NCCL INFO NVLS comm 0x560f6bdc1340 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
12: nodeai02:2459394:2460696 [4] NCCL INFO Connected all trees
12: nodeai02:2459394:2460696 [4] NCCL INFO NVLS comm 0x55d806bbe180 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
17: nodeai04:2417509:2418810 [1] NCCL INFO Connected all trees
17: nodeai04:2417509:2418810 [1] NCCL INFO NVLS comm 0x55c6a8158e80 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
20: nodeai04:2417506:2418811 [4] NCCL INFO Connected all trees
20: nodeai04:2417506:2418811 [4] NCCL INFO NVLS comm 0x5615b35a1bc0 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
36: nodeai06:2504434:2505739 [4] NCCL INFO Connected all trees
36: nodeai06:2504434:2505739 [4] NCCL INFO NVLS comm 0x5619adeaad80 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
28: nodeai05:1971442:1972748 [4] NCCL INFO Connected NVLS tree
28: nodeai05:1971442:1972748 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
28: nodeai05:1971442:1972748 [4] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
24: nodeai05:1971447:1972739 [0] NCCL INFO Connected NVLS tree
30: nodeai05:1971448:1972747 [6] NCCL INFO Connected NVLS tree
30: nodeai05:1971448:1972747 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
30: nodeai05:1971448:1972747 [6] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
24: nodeai05:1971447:1972739 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
24: nodeai05:1971447:1972739 [0] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
26: nodeai05:1971443:1972744 [2] NCCL INFO Connected NVLS tree
26: nodeai05:1971443:1972744 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
26: nodeai05:1971443:1972744 [2] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
25: nodeai05:1971445:1972746 [1] NCCL INFO Connected NVLS tree
25: nodeai05:1971445:1972746 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
25: nodeai05:1971445:1972746 [1] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
29: nodeai05:1971444:1972745 [5] NCCL INFO Connected NVLS tree
29: nodeai05:1971444:1972745 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
29: nodeai05:1971444:1972745 [5] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
31: nodeai05:1971449:1972749 [7] NCCL INFO Connected NVLS tree
31: nodeai05:1971449:1972749 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
31: nodeai05:1971449:1972749 [7] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
27: nodeai05:1971446:1972743 [3] NCCL INFO Connected NVLS tree
27: nodeai05:1971446:1972743 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
27: nodeai05:1971446:1972743 [3] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
40: nodeai07:2435734:2437028 [0] NCCL INFO Connected NVLS tree
40: nodeai07:2435734:2437028 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
40: nodeai07:2435734:2437028 [0] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
42: nodeai07:2435730:2437034 [2] NCCL INFO Connected NVLS tree
42: nodeai07:2435730:2437034 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
42: nodeai07:2435730:2437034 [2] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
44: nodeai07:2435729:2437038 [4] NCCL INFO Connected NVLS tree
44: nodeai07:2435729:2437038 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
44: nodeai07:2435729:2437038 [4] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
46: nodeai07:2435731:2437036 [6] NCCL INFO Connected NVLS tree
46: nodeai07:2435731:2437036 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
46: nodeai07:2435731:2437036 [6] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
45: nodeai07:2435732:2437039 [5] NCCL INFO Connected NVLS tree
45: nodeai07:2435732:2437039 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
45: nodeai07:2435732:2437039 [5] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
38: nodeai06:2504437:2505740 [6] NCCL INFO Connected all trees
38: nodeai06:2504437:2505740 [6] NCCL INFO NVLS comm 0x55f04b393240 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
41: nodeai07:2435735:2437033 [1] NCCL INFO Connected NVLS tree
41: nodeai07:2435735:2437033 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
41: nodeai07:2435735:2437033 [1] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
47: nodeai07:2435736:2437037 [7] NCCL INFO Connected NVLS tree
47: nodeai07:2435736:2437037 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
47: nodeai07:2435736:2437037 [7] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
43: nodeai07:2435733:2437035 [3] NCCL INFO Connected NVLS tree
43: nodeai07:2435733:2437035 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
43: nodeai07:2435733:2437035 [3] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
13: nodeai02:2459392:2460701 [5] NCCL INFO Connected NVLS tree
13: nodeai02:2459392:2460701 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
13: nodeai02:2459392:2460701 [5] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
14: nodeai02:2459389:2460700 [6] NCCL INFO Connected NVLS tree
14: nodeai02:2459389:2460700 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
14: nodeai02:2459389:2460700 [6] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
15: nodeai02:2459391:2460699 [7] NCCL INFO Connected NVLS tree
15: nodeai02:2459391:2460699 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
15: nodeai02:2459391:2460699 [7] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 8: nodeai02:2459395:2460692 [0] NCCL INFO Connected NVLS tree
 8: nodeai02:2459395:2460692 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 8: nodeai02:2459395:2460692 [0] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
10: nodeai02:2459393:2460697 [2] NCCL INFO Connected NVLS tree
10: nodeai02:2459393:2460697 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
10: nodeai02:2459393:2460697 [2] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 9: nodeai02:2459396:2460695 [1] NCCL INFO Connected NVLS tree
 9: nodeai02:2459396:2460695 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 9: nodeai02:2459396:2460695 [1] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
11: nodeai02:2459390:2460698 [3] NCCL INFO Connected NVLS tree
11: nodeai02:2459390:2460698 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
11: nodeai02:2459390:2460698 [3] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
12: nodeai02:2459394:2460696 [4] NCCL INFO Connected NVLS tree
12: nodeai02:2459394:2460696 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
12: nodeai02:2459394:2460696 [4] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
37: nodeai06:2504432:2505738 [5] NCCL INFO Connected all trees
37: nodeai06:2504432:2505738 [5] NCCL INFO NVLS comm 0x5622d3b08e40 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 1: nodeai01:2157663:2158960 [1] NCCL INFO Connected all trees
 1: nodeai01:2157663:2158960 [1] NCCL INFO NVLS comm 0x55cb52b76280 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
40: nodeai07:2435734:2437028 [0] NCCL INFO comm 0x5581fe0e0d80 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
41: nodeai07:2435735:2437033 [1] NCCL INFO comm 0x55cf54557340 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
42: nodeai07:2435730:2437034 [2] NCCL INFO comm 0x55c42d8b2240 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
43: nodeai07:2435733:2437035 [3] NCCL INFO comm 0x559b9d033d80 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
44: nodeai07:2435729:2437038 [4] NCCL INFO comm 0x564b194c3700 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
45: nodeai07:2435732:2437039 [5] NCCL INFO comm 0x56314f515c00 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
46: nodeai07:2435731:2437036 [6] NCCL INFO comm 0x561627d216c0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
47: nodeai07:2435736:2437037 [7] NCCL INFO comm 0x564e1f981b80 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x66d16d2583d6a9f3 - Init COMPLETE
21: nodeai04:2417508:2418812 [5] NCCL INFO Connected NVLS tree
21: nodeai04:2417508:2418812 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
21: nodeai04:2417508:2418812 [5] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
17: nodeai04:2417509:2418810 [1] NCCL INFO Connected NVLS tree
17: nodeai04:2417509:2418810 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
17: nodeai04:2417509:2418810 [1] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
23: nodeai04:2417507:2418809 [7] NCCL INFO Connected NVLS tree
23: nodeai04:2417507:2418809 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
23: nodeai04:2417507:2418809 [7] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
19: nodeai04:2417510:2418807 [3] NCCL INFO Connected NVLS tree
19: nodeai04:2417510:2418807 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
19: nodeai04:2417510:2418807 [3] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
16: nodeai04:2417513:2418803 [0] NCCL INFO Connected NVLS tree
16: nodeai04:2417513:2418803 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
16: nodeai04:2417513:2418803 [0] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
22: nodeai04:2417511:2418813 [6] NCCL INFO Connected NVLS tree
22: nodeai04:2417511:2418813 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
22: nodeai04:2417511:2418813 [6] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
18: nodeai04:2417512:2418808 [2] NCCL INFO Connected NVLS tree
18: nodeai04:2417512:2418808 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
18: nodeai04:2417512:2418808 [2] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
20: nodeai04:2417506:2418811 [4] NCCL INFO Connected NVLS tree
20: nodeai04:2417506:2418811 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
20: nodeai04:2417506:2418811 [4] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
24: nodeai05:1971447:1972739 [0] NCCL INFO comm 0x5643eedc4280 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x534086292a0b7ada - Init COMPLETE
25: nodeai05:1971445:1972746 [1] NCCL INFO comm 0x55c3e3c11340 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x534086292a0b7ada - Init COMPLETE
26: nodeai05:1971443:1972744 [2] NCCL INFO comm 0x55be691708c0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x534086292a0b7ada - Init COMPLETE
27: nodeai05:1971446:1972743 [3] NCCL INFO comm 0x55bd9df3d940 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x534086292a0b7ada - Init COMPLETE
28: nodeai05:1971442:1972748 [4] NCCL INFO comm 0x560ddfeec540 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x534086292a0b7ada - Init COMPLETE
29: nodeai05:1971444:1972745 [5] NCCL INFO comm 0x56523b507b80 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x534086292a0b7ada - Init COMPLETE
30: nodeai05:1971448:1972747 [6] NCCL INFO comm 0x557542053280 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x534086292a0b7ada - Init COMPLETE
31: nodeai05:1971449:1972749 [7] NCCL INFO comm 0x55a01ffb1a00 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x534086292a0b7ada - Init COMPLETE
 8: nodeai02:2459395:2460692 [0] NCCL INFO comm 0x55adf1553040 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x61ece085497a7064 - Init COMPLETE
 9: nodeai02:2459396:2460695 [1] NCCL INFO comm 0x559b063f8e80 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x61ece085497a7064 - Init COMPLETE
10: nodeai02:2459393:2460697 [2] NCCL INFO comm 0x556f37823f40 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x61ece085497a7064 - Init COMPLETE
11: nodeai02:2459390:2460698 [3] NCCL INFO comm 0x564a9a889f40 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x61ece085497a7064 - Init COMPLETE
12: nodeai02:2459394:2460696 [4] NCCL INFO comm 0x55d806bbe180 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x61ece085497a7064 - Init COMPLETE
13: nodeai02:2459392:2460701 [5] NCCL INFO comm 0x55d98081e040 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x61ece085497a7064 - Init COMPLETE
14: nodeai02:2459389:2460700 [6] NCCL INFO comm 0x55ec378dd000 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x61ece085497a7064 - Init COMPLETE
15: nodeai02:2459391:2460699 [7] NCCL INFO comm 0x5619f3e14700 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x61ece085497a7064 - Init COMPLETE
33: nodeai06:2504433:2505737 [1] NCCL INFO Connected all trees
33: nodeai06:2504433:2505737 [1] NCCL INFO NVLS comm 0x5601eae6d140 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
34: nodeai06:2504435:2505742 [2] NCCL INFO Connected all trees
34: nodeai06:2504435:2505742 [2] NCCL INFO NVLS comm 0x5630f97a2400 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
16: nodeai04:2417513:2418803 [0] NCCL INFO comm 0x55ea13bbf480 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x7aceba865fd02d4c - Init COMPLETE
18: nodeai04:2417512:2418808 [2] NCCL INFO comm 0x564c4f3cae40 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x7aceba865fd02d4c - Init COMPLETE
20: nodeai04:2417506:2418811 [4] NCCL INFO comm 0x5615b35a1bc0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x7aceba865fd02d4c - Init COMPLETE
22: nodeai04:2417511:2418813 [6] NCCL INFO comm 0x560f6bdc1340 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x7aceba865fd02d4c - Init COMPLETE
21: nodeai04:2417508:2418812 [5] NCCL INFO comm 0x558756c2f080 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x7aceba865fd02d4c - Init COMPLETE
17: nodeai04:2417509:2418810 [1] NCCL INFO comm 0x55c6a8158e80 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x7aceba865fd02d4c - Init COMPLETE
19: nodeai04:2417510:2418807 [3] NCCL INFO comm 0x55e304d49600 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x7aceba865fd02d4c - Init COMPLETE
23: nodeai04:2417507:2418809 [7] NCCL INFO comm 0x55a36f02fcc0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x7aceba865fd02d4c - Init COMPLETE
 6: nodeai01:2157661:2158959 [6] NCCL INFO Connected all trees
 6: nodeai01:2157661:2158959 [6] NCCL INFO NVLS comm 0x55aa516cd1c0 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 4: nodeai01:2157666:2158957 [4] NCCL INFO Connected all trees
 4: nodeai01:2157666:2158957 [4] NCCL INFO NVLS comm 0x556b87fd4000 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 5: nodeai01:2157665:2158963 [5] NCCL INFO Connected all trees
 5: nodeai01:2157665:2158963 [5] NCCL INFO NVLS comm 0x55f9facf4580 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 2: nodeai01:2157662:2158961 [2] NCCL INFO Connected all trees
 2: nodeai01:2157662:2158961 [2] NCCL INFO NVLS comm 0x556a1ae32840 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 3: nodeai01:2157667:2158958 [3] NCCL INFO Connected all trees
 3: nodeai01:2157667:2158958 [3] NCCL INFO NVLS comm 0x5571536a0180 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
32: nodeai06:2504436:2505735 [0] NCCL INFO Connected NVLS tree
36: nodeai06:2504434:2505739 [4] NCCL INFO Connected NVLS tree
36: nodeai06:2504434:2505739 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
36: nodeai06:2504434:2505739 [4] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
37: nodeai06:2504432:2505738 [5] NCCL INFO Connected NVLS tree
37: nodeai06:2504432:2505738 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
37: nodeai06:2504432:2505738 [5] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
38: nodeai06:2504437:2505740 [6] NCCL INFO Connected NVLS tree
38: nodeai06:2504437:2505740 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
38: nodeai06:2504437:2505740 [6] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
39: nodeai06:2504430:2505741 [7] NCCL INFO Connected NVLS tree
39: nodeai06:2504430:2505741 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
39: nodeai06:2504430:2505741 [7] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
32: nodeai06:2504436:2505735 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
32: nodeai06:2504436:2505735 [0] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
33: nodeai06:2504433:2505737 [1] NCCL INFO Connected NVLS tree
33: nodeai06:2504433:2505737 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
33: nodeai06:2504433:2505737 [1] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
35: nodeai06:2504431:2505736 [3] NCCL INFO Connected NVLS tree
35: nodeai06:2504431:2505736 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
35: nodeai06:2504431:2505736 [3] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
34: nodeai06:2504435:2505742 [2] NCCL INFO Connected NVLS tree
34: nodeai06:2504435:2505742 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
34: nodeai06:2504435:2505742 [2] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
32: nodeai06:2504436:2505735 [0] NCCL INFO comm 0x56050cd86540 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x244ce223fdce877b - Init COMPLETE
33: nodeai06:2504433:2505737 [1] NCCL INFO comm 0x5601eae6d140 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x244ce223fdce877b - Init COMPLETE
34: nodeai06:2504435:2505742 [2] NCCL INFO comm 0x5630f97a2400 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x244ce223fdce877b - Init COMPLETE
35: nodeai06:2504431:2505736 [3] NCCL INFO comm 0x55cbe3456f00 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x244ce223fdce877b - Init COMPLETE
36: nodeai06:2504434:2505739 [4] NCCL INFO comm 0x5619adeaad80 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x244ce223fdce877b - Init COMPLETE
37: nodeai06:2504432:2505738 [5] NCCL INFO comm 0x5622d3b08e40 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x244ce223fdce877b - Init COMPLETE
38: nodeai06:2504437:2505740 [6] NCCL INFO comm 0x55f04b393240 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x244ce223fdce877b - Init COMPLETE
39: nodeai06:2504430:2505741 [7] NCCL INFO comm 0x5654f999a1c0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x244ce223fdce877b - Init COMPLETE
 3: nodeai01:2157667:2158958 [3] NCCL INFO Connected NVLS tree
 3: nodeai01:2157667:2158958 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 3: nodeai01:2157667:2158958 [3] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 7: nodeai01:2157664:2158962 [7] NCCL INFO Connected NVLS tree
 7: nodeai01:2157664:2158962 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 7: nodeai01:2157664:2158962 [7] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 6: nodeai01:2157661:2158959 [6] NCCL INFO Connected NVLS tree
 6: nodeai01:2157661:2158959 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 6: nodeai01:2157661:2158959 [6] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 0: nodeai01:2157660:2158956 [0] NCCL INFO Connected NVLS tree
 0: nodeai01:2157660:2158956 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 0: nodeai01:2157660:2158956 [0] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 4: nodeai01:2157666:2158957 [4] NCCL INFO Connected NVLS tree
 4: nodeai01:2157666:2158957 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 4: nodeai01:2157666:2158957 [4] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 5: nodeai01:2157665:2158963 [5] NCCL INFO Connected NVLS tree
 5: nodeai01:2157665:2158963 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 5: nodeai01:2157665:2158963 [5] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 1: nodeai01:2157663:2158960 [1] NCCL INFO Connected NVLS tree
 1: nodeai01:2157663:2158960 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 1: nodeai01:2157663:2158960 [1] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 2: nodeai01:2157662:2158961 [2] NCCL INFO Connected NVLS tree
 2: nodeai01:2157662:2158961 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
 2: nodeai01:2157662:2158961 [2] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
 0: nodeai01:2157660:2158956 [0] NCCL INFO comm 0x5609149bcfc0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 19000 commId 0x3838e9a601809337 - Init COMPLETE
 1: nodeai01:2157663:2158960 [1] NCCL INFO comm 0x55cb52b76280 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x3838e9a601809337 - Init COMPLETE
 2: nodeai01:2157662:2158961 [2] NCCL INFO comm 0x556a1ae32840 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x3838e9a601809337 - Init COMPLETE
 3: nodeai01:2157667:2158958 [3] NCCL INFO comm 0x5571536a0180 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x3838e9a601809337 - Init COMPLETE
 4: nodeai01:2157666:2158957 [4] NCCL INFO comm 0x556b87fd4000 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x3838e9a601809337 - Init COMPLETE
 5: nodeai01:2157665:2158963 [5] NCCL INFO comm 0x55f9facf4580 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId bb000 commId 0x3838e9a601809337 - Init COMPLETE
 6: nodeai01:2157661:2158959 [6] NCCL INFO comm 0x55aa516cd1c0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId cb000 commId 0x3838e9a601809337 - Init COMPLETE
 7: nodeai01:2157664:2158962 [7] NCCL INFO comm 0x561beaa32e80 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId db000 commId 0x3838e9a601809337 - Init COMPLETE
 0: :::MLLOG {"namespace": "", "time_ms": 1723852701759, "event_type": "POINT_IN_TIME", "key": "opt_lamb_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1015}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852701766, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1016}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852701766, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.6, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1018}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852701766, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.7, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1019}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852701766, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1020}}
42: Torch distributed is available.
42: Torch distributed is initialized.
45: Torch distributed is available.
45: Torch distributed is initialized.
46: Torch distributed is available.
46: Torch distributed is initialized.
40: Torch distributed is available.
40: Torch distributed is initialized.
47: Torch distributed is available.
47: Torch distributed is initialized.
41: Torch distributed is available.
41: Torch distributed is initialized.
23: Torch distributed is available.
23: Torch distributed is initialized.
15: Torch distributed is available.
15: Torch distributed is initialized.
13: Torch distributed is available.
13: Torch distributed is initialized.
22: Torch distributed is available.
22: Torch distributed is initialized.
21: Torch distributed is available.
21: Torch distributed is initialized.
14: Torch distributed is available.
14: Torch distributed is initialized.
43: Torch distributed is available.
43: Torch distributed is initialized.
 9: Torch distributed is available.
 9: Torch distributed is initialized.
 8: Torch distributed is available.
 8: Torch distributed is initialized.
11: Torch distributed is available.
11: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
19: Torch distributed is available.
19: Torch distributed is initialized.
18: Torch distributed is available.
18: Torch distributed is initialized.
16: Torch distributed is available.
16: Torch distributed is initialized.
17: Torch distributed is available.
17: Torch distributed is initialized.
44: Torch distributed is available.
44: Torch distributed is initialized.
20: Torch distributed is available.
20: Torch distributed is initialized.
12: Torch distributed is available.
12: Torch distributed is initialized.
29: Torch distributed is available.
29: Torch distributed is initialized.
30: Torch distributed is available.
30: Torch distributed is initialized.
31: Torch distributed is available.
31: Torch distributed is initialized.
25: Torch distributed is available.
25: Torch distributed is initialized.
26: Torch distributed is available.
26: Torch distributed is initialized.
27: Torch distributed is available.
27: Torch distributed is initialized.
24: Torch distributed is available.
24: Torch distributed is initialized.
28: Torch distributed is available.
28: Torch distributed is initialized.
 0: :::MLLOG {"namespace": "", "time_ms": 1723852702136, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200330.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852702251, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852702252, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": -200000.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
37: Torch distributed is available.
37: Torch distributed is initialized.
39: Torch distributed is available.
39: Torch distributed is initialized.
 1: Torch distributed is available.
 1: Torch distributed is initialized.
36: Torch distributed is available.
36: Torch distributed is initialized.
 2: Torch distributed is available.
 2: Torch distributed is initialized.
 3: Torch distributed is available.
 3: Torch distributed is initialized.
35: Torch distributed is available.
35: Torch distributed is initialized.
34: Torch distributed is available.
34: Torch distributed is initialized.
33: Torch distributed is available.
33: Torch distributed is initialized.
32: Torch distributed is available.
32: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
 6: Torch distributed is available.
 6: Torch distributed is initialized.
 5: Torch distributed is available.
 5: Torch distributed is initialized.
 4: Torch distributed is available.
 4: Torch distributed is initialized.
38: Torch distributed is available.
38: Torch distributed is initialized.
 0: Torch distributed is available.
 0: Torch distributed is initialized.
 0: nodeai01:2157660:2158992 [0] NCCL INFO Using network IBext
 2: nodeai01:2157662:2158993 [2] NCCL INFO Using network IBext
 1: nodeai01:2157663:2158994 [1] NCCL INFO Using network IBext
 3: nodeai01:2157667:2158995 [3] NCCL INFO Using network IBext
46: nodeai07:2435731:2437068 [6] NCCL INFO Using network IBext
24: nodeai05:1971447:1972777 [0] NCCL INFO Using network IBext
20: nodeai04:2417506:2418845 [4] NCCL INFO Using network IBext
28: nodeai05:1971442:1972778 [4] NCCL INFO Using network IBext
27: nodeai05:1971446:1972779 [3] NCCL INFO Using network IBext
30: nodeai05:1971448:1972780 [6] NCCL INFO Using network IBext
17: nodeai04:2417509:2418848 [1] NCCL INFO Using network IBext
25: nodeai05:1971445:1972781 [1] NCCL INFO Using network IBext
26: nodeai05:1971443:1972782 [2] NCCL INFO Using network IBext
19: nodeai04:2417510:2418842 [3] NCCL INFO Using network IBext
23: nodeai04:2417507:2418844 [7] NCCL INFO Using network IBext
16: nodeai04:2417513:2418847 [0] NCCL INFO Using network IBext
18: nodeai04:2417512:2418846 [2] NCCL INFO Using network IBext
21: nodeai04:2417508:2418843 [5] NCCL INFO Using network IBext
41: nodeai07:2435735:2437070 [1] NCCL INFO Using network IBext
 7: nodeai01:2157664:2158996 [7] NCCL INFO Using network IBext
 8: nodeai02:2459395:2460731 [0] NCCL INFO Using network IBext
43: nodeai07:2435733:2437069 [3] NCCL INFO Using network IBext
10: nodeai02:2459393:2460732 [2] NCCL INFO Using network IBext
42: nodeai07:2435730:2437071 [2] NCCL INFO Using network IBext
15: nodeai02:2459391:2460730 [7] NCCL INFO Using network IBext
40: nodeai07:2435734:2437072 [0] NCCL INFO Using network IBext
32: nodeai06:2504436:2505771 [0] NCCL INFO Using network IBext
29: nodeai05:1971444:1972784 [5] NCCL INFO Using network IBext
45: nodeai07:2435732:2437073 [5] NCCL INFO Using network IBext
34: nodeai06:2504435:2505772 [2] NCCL INFO Using network IBext
 9: nodeai02:2459396:2460733 [1] NCCL INFO Using network IBext
47: nodeai07:2435736:2437074 [7] NCCL INFO Using network IBext
35: nodeai06:2504431:2505770 [3] NCCL INFO Using network IBext
31: nodeai05:1971449:1972783 [7] NCCL INFO Using network IBext
44: nodeai07:2435729:2437075 [4] NCCL INFO Using network IBext
 6: nodeai01:2157661:2158997 [6] NCCL INFO Using network IBext
13: nodeai02:2459392:2460734 [5] NCCL INFO Using network IBext
12: nodeai02:2459394:2460735 [4] NCCL INFO Using network IBext
22: nodeai04:2417511:2418849 [6] NCCL INFO Using network IBext
14: nodeai02:2459389:2460736 [6] NCCL INFO Using network IBext
33: nodeai06:2504433:2505777 [1] NCCL INFO Using network IBext
36: nodeai06:2504434:2505776 [4] NCCL INFO Using network IBext
37: nodeai06:2504432:2505773 [5] NCCL INFO Using network IBext
38: nodeai06:2504437:2505774 [6] NCCL INFO Using network IBext
39: nodeai06:2504430:2505775 [7] NCCL INFO Using network IBext
11: nodeai02:2459390:2460737 [3] NCCL INFO Using network IBext
 4: nodeai01:2157666:2158999 [4] NCCL INFO Using network IBext
 5: nodeai01:2157665:2158998 [5] NCCL INFO Using network IBext
31: nodeai05:1971449:1972783 [7] NCCL INFO comm 0x55a0200c3560 rank 31 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init START
32: nodeai06:2504436:2505771 [0] NCCL INFO comm 0x56050ce94190 rank 32 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init START
30: nodeai05:1971448:1972780 [6] NCCL INFO comm 0x557542160f50 rank 30 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init START
33: nodeai06:2504433:2505777 [1] NCCL INFO comm 0x5601eaf7ad90 rank 33 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init START
29: nodeai05:1971444:1972784 [5] NCCL INFO comm 0x56523b615850 rank 29 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init START
34: nodeai06:2504435:2505772 [2] NCCL INFO comm 0x5630f98b3f60 rank 34 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init START
41: nodeai07:2435735:2437070 [1] NCCL INFO comm 0x55cf54665010 rank 41 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init START
35: nodeai06:2504431:2505770 [3] NCCL INFO comm 0x55cbe3564bd0 rank 35 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init START
36: nodeai06:2504434:2505776 [4] NCCL INFO comm 0x5619adfbc960 rank 36 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init START
28: nodeai05:1971442:1972778 [4] NCCL INFO comm 0x560ddfffe120 rank 28 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init START
27: nodeai05:1971446:1972779 [3] NCCL INFO comm 0x55bd9e04b590 rank 27 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init START
23: nodeai04:2417507:2418844 [7] NCCL INFO comm 0x55a375e18400 rank 23 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init START
26: nodeai05:1971443:1972782 [2] NCCL INFO comm 0x55be69282420 rank 26 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init START
42: nodeai07:2435730:2437071 [2] NCCL INFO comm 0x55c42d9c3da0 rank 42 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init START
37: nodeai06:2504432:2505773 [5] NCCL INFO comm 0x5622d3c16a90 rank 37 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init START
38: nodeai06:2504437:2505774 [6] NCCL INFO comm 0x55f04b4a0f10 rank 38 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init START
22: nodeai04:2417511:2418849 [6] NCCL INFO comm 0x560f72ba9d00 rank 22 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init START
40: nodeai07:2435734:2437072 [0] NCCL INFO comm 0x558204ec9940 rank 40 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init START
39: nodeai06:2504430:2505775 [7] NCCL INFO comm 0x5654f9aa7e90 rank 39 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init START
43: nodeai07:2435733:2437069 [3] NCCL INFO comm 0x559b9d1419d0 rank 43 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init START
25: nodeai05:1971445:1972781 [1] NCCL INFO comm 0x55c3e3d22ea0 rank 25 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init START
44: nodeai07:2435729:2437075 [4] NCCL INFO comm 0x564b202ace00 rank 44 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init START
24: nodeai05:1971447:1972777 [0] NCCL INFO comm 0x5643f4facd80 rank 24 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init START
21: nodeai04:2417508:2418843 [5] NCCL INFO comm 0x55875ce00800 rank 21 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init START
45: nodeai07:2435732:2437073 [5] NCCL INFO comm 0x5631562fcc80 rank 45 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init START
46: nodeai07:2435731:2437068 [6] NCCL INFO comm 0x56162eb0a340 rank 46 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init START
47: nodeai07:2435736:2437074 [7] NCCL INFO comm 0x564e1fa936e0 rank 47 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init START
 0: nodeai01:2157660:2158992 [0] NCCL INFO comm 0x560914aceb20 rank 0 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init START
 1: nodeai01:2157663:2158994 [1] NCCL INFO comm 0x55cb58d46d40 rank 1 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init START
20: nodeai04:2417506:2418845 [4] NCCL INFO comm 0x5615b36b3720 rank 20 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init START
10: nodeai02:2459393:2460732 [2] NCCL INFO comm 0x556f37931c10 rank 10 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init START
 2: nodeai01:2157662:2158993 [2] NCCL INFO comm 0x556a1af44420 rank 2 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init START
15: nodeai02:2459391:2460730 [7] NCCL INFO comm 0x5619f3f26260 rank 15 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init START
 3: nodeai01:2157667:2158995 [3] NCCL INFO comm 0x5571537b1d60 rank 3 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init START
16: nodeai04:2417513:2418847 [0] NCCL INFO comm 0x55ea13cd0fe0 rank 16 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init START
 4: nodeai01:2157666:2158999 [4] NCCL INFO comm 0x556b880e1cd0 rank 4 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init START
 8: nodeai02:2459395:2460731 [0] NCCL INFO comm 0x55adf1660d10 rank 8 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init START
17: nodeai04:2417509:2418848 [1] NCCL INFO comm 0x55c6a8266ad0 rank 17 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init START
 5: nodeai01:2157665:2158998 [5] NCCL INFO comm 0x55fa00ec5d80 rank 5 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init START
 9: nodeai02:2459396:2460733 [1] NCCL INFO comm 0x559b0650a9e0 rank 9 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init START
18: nodeai04:2417512:2418846 [2] NCCL INFO comm 0x564c4f4d8a90 rank 18 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init START
 6: nodeai01:2157661:2158997 [6] NCCL INFO comm 0x55aa517dae10 rank 6 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init START
14: nodeai02:2459389:2460736 [6] NCCL INFO comm 0x55ec379eac50 rank 14 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init START
19: nodeai04:2417510:2418842 [3] NCCL INFO comm 0x55e304e57250 rank 19 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init START
 7: nodeai01:2157664:2158996 [7] NCCL INFO comm 0x561beab44a60 rank 7 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init START
11: nodeai02:2459390:2460737 [3] NCCL INFO comm 0x564a9a895710 rank 11 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init START
12: nodeai02:2459394:2460735 [4] NCCL INFO comm 0x55d806ccfd60 rank 12 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init START
13: nodeai02:2459392:2460734 [5] NCCL INFO comm 0x55d986a07340 rank 13 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init START
 4: nodeai01:2157666:2158999 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 4: nodeai01:2157666:2158999 [4] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS multicast support is available on dev 4
35: nodeai06:2504431:2505770 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
35: nodeai06:2504431:2505770 [3] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
35: nodeai06:2504431:2505770 [3] NCCL INFO NVLS multicast support is available on dev 3
 5: nodeai01:2157665:2158998 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 5: nodeai01:2157665:2158998 [5] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS multicast support is available on dev 5
 6: nodeai01:2157661:2158997 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 6: nodeai01:2157661:2158997 [6] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS multicast support is available on dev 6
34: nodeai06:2504435:2505772 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
34: nodeai06:2504435:2505772 [2] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
34: nodeai06:2504435:2505772 [2] NCCL INFO NVLS multicast support is available on dev 2
17: nodeai04:2417509:2418848 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
17: nodeai04:2417509:2418848 [1] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
17: nodeai04:2417509:2418848 [1] NCCL INFO NVLS multicast support is available on dev 1
33: nodeai06:2504433:2505777 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
33: nodeai06:2504433:2505777 [1] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
33: nodeai06:2504433:2505777 [1] NCCL INFO NVLS multicast support is available on dev 1
42: nodeai07:2435730:2437071 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
42: nodeai07:2435730:2437071 [2] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
42: nodeai07:2435730:2437071 [2] NCCL INFO NVLS multicast support is available on dev 2
16: nodeai04:2417513:2418847 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
16: nodeai04:2417513:2418847 [0] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
16: nodeai04:2417513:2418847 [0] NCCL INFO NVLS multicast support is available on dev 0
18: nodeai04:2417512:2418846 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
18: nodeai04:2417512:2418846 [2] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
18: nodeai04:2417512:2418846 [2] NCCL INFO NVLS multicast support is available on dev 2
41: nodeai07:2435735:2437070 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
41: nodeai07:2435735:2437070 [1] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
41: nodeai07:2435735:2437070 [1] NCCL INFO NVLS multicast support is available on dev 1
 0: nodeai01:2157660:2158992 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
 0: nodeai01:2157660:2158992 [0] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS multicast support is available on dev 0
 7: nodeai01:2157664:2158996 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
 7: nodeai01:2157664:2158996 [7] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS multicast support is available on dev 7
 3: nodeai01:2157667:2158995 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
 3: nodeai01:2157667:2158995 [3] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS multicast support is available on dev 3
 1: nodeai01:2157663:2158994 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
 1: nodeai01:2157663:2158994 [1] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS multicast support is available on dev 1
 2: nodeai01:2157662:2158993 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
 2: nodeai01:2157662:2158993 [2] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS multicast support is available on dev 2
43: nodeai07:2435733:2437069 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
43: nodeai07:2435733:2437069 [3] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
43: nodeai07:2435733:2437069 [3] NCCL INFO NVLS multicast support is available on dev 3
19: nodeai04:2417510:2418842 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
19: nodeai04:2417510:2418842 [3] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
19: nodeai04:2417510:2418842 [3] NCCL INFO NVLS multicast support is available on dev 3
 8: nodeai02:2459395:2460731 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
 8: nodeai02:2459395:2460731 [0] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 8: nodeai02:2459395:2460731 [0] NCCL INFO NVLS multicast support is available on dev 0
25: nodeai05:1971445:1972781 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
25: nodeai05:1971445:1972781 [1] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
25: nodeai05:1971445:1972781 [1] NCCL INFO NVLS multicast support is available on dev 1
32: nodeai06:2504436:2505771 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
32: nodeai06:2504436:2505771 [0] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
32: nodeai06:2504436:2505771 [0] NCCL INFO NVLS multicast support is available on dev 0
38: nodeai06:2504437:2505774 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
38: nodeai06:2504437:2505774 [6] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
38: nodeai06:2504437:2505774 [6] NCCL INFO NVLS multicast support is available on dev 6
23: nodeai04:2417507:2418844 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
23: nodeai04:2417507:2418844 [7] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
23: nodeai04:2417507:2418844 [7] NCCL INFO NVLS multicast support is available on dev 7
39: nodeai06:2504430:2505775 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
39: nodeai06:2504430:2505775 [7] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
39: nodeai06:2504430:2505775 [7] NCCL INFO NVLS multicast support is available on dev 7
11: nodeai02:2459390:2460737 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
11: nodeai02:2459390:2460737 [3] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
11: nodeai02:2459390:2460737 [3] NCCL INFO NVLS multicast support is available on dev 3
27: nodeai05:1971446:1972779 [3] NCCL INFO Setting affinity for GPU 3 to 55,55555555,55555555,55555555
27: nodeai05:1971446:1972779 [3] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
27: nodeai05:1971446:1972779 [3] NCCL INFO NVLS multicast support is available on dev 3
 9: nodeai02:2459396:2460733 [1] NCCL INFO Setting affinity for GPU 1 to 55,55555555,55555555,55555555
 9: nodeai02:2459396:2460733 [1] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
 9: nodeai02:2459396:2460733 [1] NCCL INFO NVLS multicast support is available on dev 1
29: nodeai05:1971444:1972784 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
29: nodeai05:1971444:1972784 [5] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
29: nodeai05:1971444:1972784 [5] NCCL INFO NVLS multicast support is available on dev 5
36: nodeai06:2504434:2505776 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
36: nodeai06:2504434:2505776 [4] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
36: nodeai06:2504434:2505776 [4] NCCL INFO NVLS multicast support is available on dev 4
40: nodeai07:2435734:2437072 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
40: nodeai07:2435734:2437072 [0] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
40: nodeai07:2435734:2437072 [0] NCCL INFO NVLS multicast support is available on dev 0
37: nodeai06:2504432:2505773 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
37: nodeai06:2504432:2505773 [5] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
37: nodeai06:2504432:2505773 [5] NCCL INFO NVLS multicast support is available on dev 5
46: nodeai07:2435731:2437068 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
46: nodeai07:2435731:2437068 [6] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
46: nodeai07:2435731:2437068 [6] NCCL INFO NVLS multicast support is available on dev 6
22: nodeai04:2417511:2418849 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
22: nodeai04:2417511:2418849 [6] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
22: nodeai04:2417511:2418849 [6] NCCL INFO NVLS multicast support is available on dev 6
14: nodeai02:2459389:2460736 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
14: nodeai02:2459389:2460736 [6] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
14: nodeai02:2459389:2460736 [6] NCCL INFO NVLS multicast support is available on dev 6
45: nodeai07:2435732:2437073 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
45: nodeai07:2435732:2437073 [5] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
45: nodeai07:2435732:2437073 [5] NCCL INFO NVLS multicast support is available on dev 5
15: nodeai02:2459391:2460730 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
15: nodeai02:2459391:2460730 [7] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
15: nodeai02:2459391:2460730 [7] NCCL INFO NVLS multicast support is available on dev 7
21: nodeai04:2417508:2418843 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
21: nodeai04:2417508:2418843 [5] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
21: nodeai04:2417508:2418843 [5] NCCL INFO NVLS multicast support is available on dev 5
31: nodeai05:1971449:1972783 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
31: nodeai05:1971449:1972783 [7] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
31: nodeai05:1971449:1972783 [7] NCCL INFO NVLS multicast support is available on dev 7
24: nodeai05:1971447:1972777 [0] NCCL INFO Setting affinity for GPU 0 to 55,55555555,55555555,55555555
24: nodeai05:1971447:1972777 [0] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
24: nodeai05:1971447:1972777 [0] NCCL INFO NVLS multicast support is available on dev 0
30: nodeai05:1971448:1972780 [6] NCCL INFO Setting affinity for GPU 6 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
30: nodeai05:1971448:1972780 [6] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
30: nodeai05:1971448:1972780 [6] NCCL INFO NVLS multicast support is available on dev 6
28: nodeai05:1971442:1972778 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
28: nodeai05:1971442:1972778 [4] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
28: nodeai05:1971442:1972778 [4] NCCL INFO NVLS multicast support is available on dev 4
47: nodeai07:2435736:2437074 [7] NCCL INFO Setting affinity for GPU 7 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
47: nodeai07:2435736:2437074 [7] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
47: nodeai07:2435736:2437074 [7] NCCL INFO NVLS multicast support is available on dev 7
20: nodeai04:2417506:2418845 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
20: nodeai04:2417506:2418845 [4] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
20: nodeai04:2417506:2418845 [4] NCCL INFO NVLS multicast support is available on dev 4
44: nodeai07:2435729:2437075 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
44: nodeai07:2435729:2437075 [4] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
44: nodeai07:2435729:2437075 [4] NCCL INFO NVLS multicast support is available on dev 4
10: nodeai02:2459393:2460732 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
10: nodeai02:2459393:2460732 [2] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
10: nodeai02:2459393:2460732 [2] NCCL INFO NVLS multicast support is available on dev 2
26: nodeai05:1971443:1972782 [2] NCCL INFO Setting affinity for GPU 2 to 55,55555555,55555555,55555555
26: nodeai05:1971443:1972782 [2] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
26: nodeai05:1971443:1972782 [2] NCCL INFO NVLS multicast support is available on dev 2
12: nodeai02:2459394:2460735 [4] NCCL INFO Setting affinity for GPU 4 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
12: nodeai02:2459394:2460735 [4] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
12: nodeai02:2459394:2460735 [4] NCCL INFO NVLS multicast support is available on dev 4
13: nodeai02:2459392:2460734 [5] NCCL INFO Setting affinity for GPU 5 to aa,aaaaaaaa,aaaaaaaa,aaaaaaaa
13: nodeai02:2459392:2460734 [5] NCCL INFO NCCL_COLLNET_ENABLE set by environment to 0.
13: nodeai02:2459392:2460734 [5] NCCL INFO NVLS multicast support is available on dev 5
13: nodeai02:2459392:2460734 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->21 [6] -1/-1/-1->13->12 [7] 14/-1/-1->13->12 [8] 14/-1/-1->13->12 [9] 14/-1/-1->13->12 [10] 14/-1/-1->13->12 [11] 14/-1/-1->13->12 [12] 14/-1/-1->13->12 [13] 14/29/5->13->45 [14] -1/-1/-1->13->12 [15] 14/-1/-1->13->12
13: nodeai02:2459392:2460734 [5] NCCL INFO P2P Chunksize set to 131072
14: nodeai02:2459389:2460736 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/-1/-1->14->22 [7] -1/-1/-1->14->13 [8] 15/-1/-1->14->13 [9] 15/-1/-1->14->13 [10] 15/-1/-1->14->13 [11] 15/-1/-1->14->13 [12] 15/-1/-1->14->13 [13] 15/-1/-1->14->13 [14] 15/30/6->14->46 [15] -1/-1/-1->14->13
14: nodeai02:2459389:2460736 [6] NCCL INFO P2P Chunksize set to 131072
15: nodeai02:2459391:2460730 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] 8/-1/-1->15->14 [3] 8/-1/-1->15->14 [4] 8/-1/-1->15->14 [5] 8/-1/-1->15->14 [6] 8/-1/-1->15->14 [7] 8/-1/-1->15->23 [8] -1/-1/-1->15->14 [9] 8/-1/-1->15->14 [10] 8/-1/-1->15->14 [11] 8/-1/-1->15->14 [12] 8/-1/-1->15->14 [13] 8/-1/-1->15->14 [14] 8/-1/-1->15->14 [15] 8/31/7->15->47
15: nodeai02:2459391:2460730 [7] NCCL INFO P2P Chunksize set to 131072
24: nodeai05:1971447:1972777 [0] NCCL INFO Trees [0] 25/-1/-1->24->16 [1] -1/-1/-1->24->31 [2] 25/-1/-1->24->31 [3] 25/-1/-1->24->31 [4] 25/-1/-1->24->31 [5] 25/-1/-1->24->31 [6] 25/-1/-1->24->31 [7] 25/-1/-1->24->31 [8] 25/32/16->24->8 [9] -1/-1/-1->24->31 [10] 25/-1/-1->24->31 [11] 25/-1/-1->24->31 [12] 25/-1/-1->24->31 [13] 25/-1/-1->24->31 [14] 25/-1/-1->24->31 [15] 25/-1/-1->24->31
24: nodeai05:1971447:1972777 [0] NCCL INFO P2P Chunksize set to 131072
32: nodeai06:2504436:2505771 [0] NCCL INFO Trees [0] 33/16/40->32->0 [1] -1/-1/-1->32->39 [2] 33/-1/-1->32->39 [3] 33/-1/-1->32->39 [4] 33/-1/-1->32->39 [5] 33/-1/-1->32->39 [6] 33/-1/-1->32->39 [7] 33/-1/-1->32->39 [8] 33/-1/-1->32->24 [9] -1/-1/-1->32->39 [10] 33/-1/-1->32->39 [11] 33/-1/-1->32->39 [12] 33/-1/-1->32->39 [13] 33/-1/-1->32->39 [14] 33/-1/-1->32->39 [15] 33/-1/-1->32->39
32: nodeai06:2504436:2505771 [0] NCCL INFO P2P Chunksize set to 131072
33: nodeai06:2504433:2505777 [1] NCCL INFO Trees [0] 34/-1/-1->33->32 [1] 34/17/41->33->1 [2] -1/-1/-1->33->32 [3] 34/-1/-1->33->32 [4] 34/-1/-1->33->32 [5] 34/-1/-1->33->32 [6] 34/-1/-1->33->32 [7] 34/-1/-1->33->32 [8] 34/-1/-1->33->32 [9] 34/-1/-1->33->25 [10] -1/-1/-1->33->32 [11] 34/-1/-1->33->32 [12] 34/-1/-1->33->32 [13] 34/-1/-1->33->32 [14] 34/-1/-1->33->32 [15] 34/-1/-1->33->32
33: nodeai06:2504433:2505777 [1] NCCL INFO P2P Chunksize set to 131072
34: nodeai06:2504435:2505772 [2] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 35/-1/-1->34->33 [2] 35/18/42->34->2 [3] -1/-1/-1->34->33 [4] 35/-1/-1->34->33 [5] 35/-1/-1->34->33 [6] 35/-1/-1->34->33 [7] 35/-1/-1->34->33 [8] 35/-1/-1->34->33 [9] 35/-1/-1->34->33 [10] 35/-1/-1->34->26 [11] -1/-1/-1->34->33 [12] 35/-1/-1->34->33 [13] 35/-1/-1->34->33 [14] 35/-1/-1->34->33 [15] 35/-1/-1->34->33
34: nodeai06:2504435:2505772 [2] NCCL INFO P2P Chunksize set to 131072
35: nodeai06:2504431:2505770 [3] NCCL INFO Trees [0] 36/-1/-1->35->34 [1] 36/-1/-1->35->34 [2] 36/-1/-1->35->34 [3] 36/19/43->35->3 [4] -1/-1/-1->35->34 [5] 36/-1/-1->35->34 [6] 36/-1/-1->35->34 [7] 36/-1/-1->35->34 [8] 36/-1/-1->35->34 [9] 36/-1/-1->35->34 [10] 36/-1/-1->35->34 [11] 36/-1/-1->35->27 [12] -1/-1/-1->35->34 [13] 36/-1/-1->35->34 [14] 36/-1/-1->35->34 [15] 36/-1/-1->35->34
35: nodeai06:2504431:2505770 [3] NCCL INFO P2P Chunksize set to 131072
36: nodeai06:2504434:2505776 [4] NCCL INFO Trees [0] 37/-1/-1->36->35 [1] 37/-1/-1->36->35 [2] 37/-1/-1->36->35 [3] 37/-1/-1->36->35 [4] 37/20/44->36->4 [5] -1/-1/-1->36->35 [6] 37/-1/-1->36->35 [7] 37/-1/-1->36->35 [8] 37/-1/-1->36->35 [9] 37/-1/-1->36->35 [10] 37/-1/-1->36->35 [11] 37/-1/-1->36->35 [12] 37/-1/-1->36->28 [13] -1/-1/-1->36->35 [14] 37/-1/-1->36->35 [15] 37/-1/-1->36->35
36: nodeai06:2504434:2505776 [4] NCCL INFO P2P Chunksize set to 131072
37: nodeai06:2504432:2505773 [5] NCCL INFO Trees [0] 38/-1/-1->37->36 [1] 38/-1/-1->37->36 [2] 38/-1/-1->37->36 [3] 38/-1/-1->37->36 [4] 38/-1/-1->37->36 [5] 38/21/45->37->5 [6] -1/-1/-1->37->36 [7] 38/-1/-1->37->36 [8] 38/-1/-1->37->36 [9] 38/-1/-1->37->36 [10] 38/-1/-1->37->36 [11] 38/-1/-1->37->36 [12] 38/-1/-1->37->36 [13] 38/-1/-1->37->29 [14] -1/-1/-1->37->36 [15] 38/-1/-1->37->36
37: nodeai06:2504432:2505773 [5] NCCL INFO P2P Chunksize set to 131072
38: nodeai06:2504437:2505774 [6] NCCL INFO Trees [0] 39/-1/-1->38->37 [1] 39/-1/-1->38->37 [2] 39/-1/-1->38->37 [3] 39/-1/-1->38->37 [4] 39/-1/-1->38->37 [5] 39/-1/-1->38->37 [6] 39/22/46->38->6 [7] -1/-1/-1->38->37 [8] 39/-1/-1->38->37 [9] 39/-1/-1->38->37 [10] 39/-1/-1->38->37 [11] 39/-1/-1->38->37 [12] 39/-1/-1->38->37 [13] 39/-1/-1->38->37 [14] 39/-1/-1->38->30 [15] -1/-1/-1->38->37
38: nodeai06:2504437:2505774 [6] NCCL INFO P2P Chunksize set to 131072
25: nodeai05:1971445:1972781 [1] NCCL INFO Trees [0] 26/-1/-1->25->24 [1] 26/-1/-1->25->17 [2] -1/-1/-1->25->24 [3] 26/-1/-1->25->24 [4] 26/-1/-1->25->24 [5] 26/-1/-1->25->24 [6] 26/-1/-1->25->24 [7] 26/-1/-1->25->24 [8] 26/-1/-1->25->24 [9] 26/33/17->25->9 [10] -1/-1/-1->25->24 [11] 26/-1/-1->25->24 [12] 26/-1/-1->25->24 [13] 26/-1/-1->25->24 [14] 26/-1/-1->25->24 [15] 26/-1/-1->25->24
25: nodeai05:1971445:1972781 [1] NCCL INFO P2P Chunksize set to 131072
39: nodeai06:2504430:2505775 [7] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] 32/-1/-1->39->38 [2] 32/-1/-1->39->38 [3] 32/-1/-1->39->38 [4] 32/-1/-1->39->38 [5] 32/-1/-1->39->38 [6] 32/-1/-1->39->38 [7] 32/23/47->39->7 [8] -1/-1/-1->39->38 [9] 32/-1/-1->39->38 [10] 32/-1/-1->39->38 [11] 32/-1/-1->39->38 [12] 32/-1/-1->39->38 [13] 32/-1/-1->39->38 [14] 32/-1/-1->39->38 [15] 32/-1/-1->39->31
39: nodeai06:2504430:2505775 [7] NCCL INFO P2P Chunksize set to 131072
26: nodeai05:1971443:1972782 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] 27/-1/-1->26->25 [2] 27/-1/-1->26->18 [3] -1/-1/-1->26->25 [4] 27/-1/-1->26->25 [5] 27/-1/-1->26->25 [6] 27/-1/-1->26->25 [7] 27/-1/-1->26->25 [8] 27/-1/-1->26->25 [9] 27/-1/-1->26->25 [10] 27/34/18->26->10 [11] -1/-1/-1->26->25 [12] 27/-1/-1->26->25 [13] 27/-1/-1->26->25 [14] 27/-1/-1->26->25 [15] 27/-1/-1->26->25
26: nodeai05:1971443:1972782 [2] NCCL INFO P2P Chunksize set to 131072
27: nodeai05:1971446:1972779 [3] NCCL INFO Trees [0] 28/-1/-1->27->26 [1] 28/-1/-1->27->26 [2] 28/-1/-1->27->26 [3] 28/-1/-1->27->19 [4] -1/-1/-1->27->26 [5] 28/-1/-1->27->26 [6] 28/-1/-1->27->26 [7] 28/-1/-1->27->26 [8] 28/-1/-1->27->26 [9] 28/-1/-1->27->26 [10] 28/-1/-1->27->26 [11] 28/35/19->27->11 [12] -1/-1/-1->27->26 [13] 28/-1/-1->27->26 [14] 28/-1/-1->27->26 [15] 28/-1/-1->27->26
27: nodeai05:1971446:1972779 [3] NCCL INFO P2P Chunksize set to 131072
16: nodeai04:2417513:2418847 [0] NCCL INFO Trees [0] 17/8/24->16->32 [1] -1/-1/-1->16->23 [2] 17/-1/-1->16->23 [3] 17/-1/-1->16->23 [4] 17/-1/-1->16->23 [5] 17/-1/-1->16->23 [6] 17/-1/-1->16->23 [7] 17/-1/-1->16->23 [8] 17/-1/-1->16->24 [9] -1/-1/-1->16->23 [10] 17/-1/-1->16->23 [11] 17/-1/-1->16->23 [12] 17/-1/-1->16->23 [13] 17/-1/-1->16->23 [14] 17/-1/-1->16->23 [15] 17/-1/-1->16->23
16: nodeai04:2417513:2418847 [0] NCCL INFO P2P Chunksize set to 131072
28: nodeai05:1971442:1972778 [4] NCCL INFO Trees [0] 29/-1/-1->28->27 [1] 29/-1/-1->28->27 [2] 29/-1/-1->28->27 [3] 29/-1/-1->28->27 [4] 29/-1/-1->28->20 [5] -1/-1/-1->28->27 [6] 29/-1/-1->28->27 [7] 29/-1/-1->28->27 [8] 29/-1/-1->28->27 [9] 29/-1/-1->28->27 [10] 29/-1/-1->28->27 [11] 29/-1/-1->28->27 [12] 29/36/20->28->12 [13] -1/-1/-1->28->27 [14] 29/-1/-1->28->27 [15] 29/-1/-1->28->27
28: nodeai05:1971442:1972778 [4] NCCL INFO P2P Chunksize set to 131072
29: nodeai05:1971444:1972784 [5] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] 30/-1/-1->29->28 [3] 30/-1/-1->29->28 [4] 30/-1/-1->29->28 [5] 30/-1/-1->29->21 [6] -1/-1/-1->29->28 [7] 30/-1/-1->29->28 [8] 30/-1/-1->29->28 [9] 30/-1/-1->29->28 [10] 30/-1/-1->29->28 [11] 30/-1/-1->29->28 [12] 30/-1/-1->29->28 [13] 30/37/21->29->13 [14] -1/-1/-1->29->28 [15] 30/-1/-1->29->28
29: nodeai05:1971444:1972784 [5] NCCL INFO P2P Chunksize set to 131072
30: nodeai05:1971448:1972780 [6] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->29 [3] 31/-1/-1->30->29 [4] 31/-1/-1->30->29 [5] 31/-1/-1->30->29 [6] 31/-1/-1->30->22 [7] -1/-1/-1->30->29 [8] 31/-1/-1->30->29 [9] 31/-1/-1->30->29 [10] 31/-1/-1->30->29 [11] 31/-1/-1->30->29 [12] 31/-1/-1->30->29 [13] 31/-1/-1->30->29 [14] 31/38/22->30->14 [15] -1/-1/-1->30->29
30: nodeai05:1971448:1972780 [6] NCCL INFO P2P Chunksize set to 131072
31: nodeai05:1971449:1972783 [7] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] 24/-1/-1->31->30 [2] 24/-1/-1->31->30 [3] 24/-1/-1->31->30 [4] 24/-1/-1->31->30 [5] 24/-1/-1->31->30 [6] 24/-1/-1->31->30 [7] 24/-1/-1->31->23 [8] -1/-1/-1->31->30 [9] 24/-1/-1->31->30 [10] 24/-1/-1->31->30 [11] 24/-1/-1->31->30 [12] 24/-1/-1->31->30 [13] 24/-1/-1->31->30 [14] 24/-1/-1->31->30 [15] 24/39/23->31->15
31: nodeai05:1971449:1972783 [7] NCCL INFO P2P Chunksize set to 131072
17: nodeai04:2417509:2418848 [1] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/9/25->17->33 [2] -1/-1/-1->17->16 [3] 18/-1/-1->17->16 [4] 18/-1/-1->17->16 [5] 18/-1/-1->17->16 [6] 18/-1/-1->17->16 [7] 18/-1/-1->17->16 [8] 18/-1/-1->17->16 [9] 18/-1/-1->17->25 [10] -1/-1/-1->17->16 [11] 18/-1/-1->17->16 [12] 18/-1/-1->17->16 [13] 18/-1/-1->17->16 [14] 18/-1/-1->17->16 [15] 18/-1/-1->17->16
17: nodeai04:2417509:2418848 [1] NCCL INFO P2P Chunksize set to 131072
18: nodeai04:2417512:2418846 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/10/26->18->34 [3] -1/-1/-1->18->17 [4] 19/-1/-1->18->17 [5] 19/-1/-1->18->17 [6] 19/-1/-1->18->17 [7] 19/-1/-1->18->17 [8] 19/-1/-1->18->17 [9] 19/-1/-1->18->17 [10] 19/-1/-1->18->26 [11] -1/-1/-1->18->17 [12] 19/-1/-1->18->17 [13] 19/-1/-1->18->17 [14] 19/-1/-1->18->17 [15] 19/-1/-1->18->17
18: nodeai04:2417512:2418846 [2] NCCL INFO P2P Chunksize set to 131072
19: nodeai04:2417510:2418842 [3] NCCL INFO Trees [0] 20/-1/-1->19->18 [1] 20/-1/-1->19->18 [2] 20/-1/-1->19->18 [3] 20/11/27->19->35 [4] -1/-1/-1->19->18 [5] 20/-1/-1->19->18 [6] 20/-1/-1->19->18 [7] 20/-1/-1->19->18 [8] 20/-1/-1->19->18 [9] 20/-1/-1->19->18 [10] 20/-1/-1->19->18 [11] 20/-1/-1->19->27 [12] -1/-1/-1->19->18 [13] 20/-1/-1->19->18 [14] 20/-1/-1->19->18 [15] 20/-1/-1->19->18
19: nodeai04:2417510:2418842 [3] NCCL INFO P2P Chunksize set to 131072
20: nodeai04:2417506:2418845 [4] NCCL INFO Trees [0] 21/-1/-1->20->19 [1] 21/-1/-1->20->19 [2] 21/-1/-1->20->19 [3] 21/-1/-1->20->19 [4] 21/12/28->20->36 [5] -1/-1/-1->20->19 [6] 21/-1/-1->20->19 [7] 21/-1/-1->20->19 [8] 21/-1/-1->20->19 [9] 21/-1/-1->20->19 [10] 21/-1/-1->20->19 [11] 21/-1/-1->20->19 [12] 21/-1/-1->20->28 [13] -1/-1/-1->20->19 [14] 21/-1/-1->20->19 [15] 21/-1/-1->20->19
20: nodeai04:2417506:2418845 [4] NCCL INFO P2P Chunksize set to 131072
21: nodeai04:2417508:2418843 [5] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] 22/-1/-1->21->20 [3] 22/-1/-1->21->20 [4] 22/-1/-1->21->20 [5] 22/13/29->21->37 [6] -1/-1/-1->21->20 [7] 22/-1/-1->21->20 [8] 22/-1/-1->21->20 [9] 22/-1/-1->21->20 [10] 22/-1/-1->21->20 [11] 22/-1/-1->21->20 [12] 22/-1/-1->21->20 [13] 22/-1/-1->21->29 [14] -1/-1/-1->21->20 [15] 22/-1/-1->21->20
21: nodeai04:2417508:2418843 [5] NCCL INFO P2P Chunksize set to 131072
22: nodeai04:2417511:2418849 [6] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->21 [3] 23/-1/-1->22->21 [4] 23/-1/-1->22->21 [5] 23/-1/-1->22->21 [6] 23/14/30->22->38 [7] -1/-1/-1->22->21 [8] 23/-1/-1->22->21 [9] 23/-1/-1->22->21 [10] 23/-1/-1->22->21 [11] 23/-1/-1->22->21 [12] 23/-1/-1->22->21 [13] 23/-1/-1->22->21 [14] 23/-1/-1->22->30 [15] -1/-1/-1->22->21
22: nodeai04:2417511:2418849 [6] NCCL INFO P2P Chunksize set to 131072
23: nodeai04:2417507:2418844 [7] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] 16/-1/-1->23->22 [2] 16/-1/-1->23->22 [3] 16/-1/-1->23->22 [4] 16/-1/-1->23->22 [5] 16/-1/-1->23->22 [6] 16/-1/-1->23->22 [7] 16/15/31->23->39 [8] -1/-1/-1->23->22 [9] 16/-1/-1->23->22 [10] 16/-1/-1->23->22 [11] 16/-1/-1->23->22 [12] 16/-1/-1->23->22 [13] 16/-1/-1->23->22 [14] 16/-1/-1->23->22 [15] 16/-1/-1->23->31
23: nodeai04:2417507:2418844 [7] NCCL INFO P2P Chunksize set to 131072
46: nodeai07:2435731:2437068 [6] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 47/-1/-1->46->45 [2] 47/-1/-1->46->45 [3] 47/-1/-1->46->45 [4] 47/-1/-1->46->45 [5] 47/-1/-1->46->45 [6] 47/-1/-1->46->38 [7] -1/-1/-1->46->45 [8] 47/-1/-1->46->45 [9] 47/-1/-1->46->45 [10] 47/-1/-1->46->45 [11] 47/-1/-1->46->45 [12] 47/-1/-1->46->45 [13] 47/-1/-1->46->45 [14] 47/14/-1->46->-1 [15] -1/-1/-1->46->45
46: nodeai07:2435731:2437068 [6] NCCL INFO P2P Chunksize set to 131072
47: nodeai07:2435736:2437074 [7] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] 40/-1/-1->47->46 [2] 40/-1/-1->47->46 [3] 40/-1/-1->47->46 [4] 40/-1/-1->47->46 [5] 40/-1/-1->47->46 [6] 40/-1/-1->47->46 [7] 40/-1/-1->47->39 [8] -1/-1/-1->47->46 [9] 40/-1/-1->47->46 [10] 40/-1/-1->47->46 [11] 40/-1/-1->47->46 [12] 40/-1/-1->47->46 [13] 40/-1/-1->47->46 [14] 40/-1/-1->47->46 [15] 40/15/-1->47->-1
47: nodeai07:2435736:2437074 [7] NCCL INFO P2P Chunksize set to 131072
40: nodeai07:2435734:2437072 [0] NCCL INFO Trees [0] 41/-1/-1->40->32 [1] -1/-1/-1->40->47 [2] 41/-1/-1->40->47 [3] 41/-1/-1->40->47 [4] 41/-1/-1->40->47 [5] 41/-1/-1->40->47 [6] 41/-1/-1->40->47 [7] 41/-1/-1->40->47 [8] 41/8/-1->40->-1 [9] -1/-1/-1->40->47 [10] 41/-1/-1->40->47 [11] 41/-1/-1->40->47 [12] 41/-1/-1->40->47 [13] 41/-1/-1->40->47 [14] 41/-1/-1->40->47 [15] 41/-1/-1->40->47
40: nodeai07:2435734:2437072 [0] NCCL INFO P2P Chunksize set to 131072
41: nodeai07:2435735:2437070 [1] NCCL INFO Trees [0] 42/-1/-1->41->40 [1] 42/-1/-1->41->33 [2] -1/-1/-1->41->40 [3] 42/-1/-1->41->40 [4] 42/-1/-1->41->40 [5] 42/-1/-1->41->40 [6] 42/-1/-1->41->40 [7] 42/-1/-1->41->40 [8] 42/-1/-1->41->40 [9] 42/9/-1->41->-1 [10] -1/-1/-1->41->40 [11] 42/-1/-1->41->40 [12] 42/-1/-1->41->40 [13] 42/-1/-1->41->40 [14] 42/-1/-1->41->40 [15] 42/-1/-1->41->40
41: nodeai07:2435735:2437070 [1] NCCL INFO P2P Chunksize set to 131072
42: nodeai07:2435730:2437071 [2] NCCL INFO Trees [0] 43/-1/-1->42->41 [1] 43/-1/-1->42->41 [2] 43/-1/-1->42->34 [3] -1/-1/-1->42->41 [4] 43/-1/-1->42->41 [5] 43/-1/-1->42->41 [6] 43/-1/-1->42->41 [7] 43/-1/-1->42->41 [8] 43/-1/-1->42->41 [9] 43/-1/-1->42->41 [10] 43/10/-1->42->-1 [11] -1/-1/-1->42->41 [12] 43/-1/-1->42->41 [13] 43/-1/-1->42->41 [14] 43/-1/-1->42->41 [15] 43/-1/-1->42->41
42: nodeai07:2435730:2437071 [2] NCCL INFO P2P Chunksize set to 131072
43: nodeai07:2435733:2437069 [3] NCCL INFO Trees [0] 44/-1/-1->43->42 [1] 44/-1/-1->43->42 [2] 44/-1/-1->43->42 [3] 44/-1/-1->43->35 [4] -1/-1/-1->43->42 [5] 44/-1/-1->43->42 [6] 44/-1/-1->43->42 [7] 44/-1/-1->43->42 [8] 44/-1/-1->43->42 [9] 44/-1/-1->43->42 [10] 44/-1/-1->43->42 [11] 44/11/-1->43->-1 [12] -1/-1/-1->43->42 [13] 44/-1/-1->43->42 [14] 44/-1/-1->43->42 [15] 44/-1/-1->43->42
43: nodeai07:2435733:2437069 [3] NCCL INFO P2P Chunksize set to 131072
44: nodeai07:2435729:2437075 [4] NCCL INFO Trees [0] 45/-1/-1->44->43 [1] 45/-1/-1->44->43 [2] 45/-1/-1->44->43 [3] 45/-1/-1->44->43 [4] 45/-1/-1->44->36 [5] -1/-1/-1->44->43 [6] 45/-1/-1->44->43 [7] 45/-1/-1->44->43 [8] 45/-1/-1->44->43 [9] 45/-1/-1->44->43 [10] 45/-1/-1->44->43 [11] 45/-1/-1->44->43 [12] 45/12/-1->44->-1 [13] -1/-1/-1->44->43 [14] 45/-1/-1->44->43 [15] 45/-1/-1->44->43
44: nodeai07:2435729:2437075 [4] NCCL INFO P2P Chunksize set to 131072
45: nodeai07:2435732:2437073 [5] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 46/-1/-1->45->44 [2] 46/-1/-1->45->44 [3] 46/-1/-1->45->44 [4] 46/-1/-1->45->44 [5] 46/-1/-1->45->37 [6] -1/-1/-1->45->44 [7] 46/-1/-1->45->44 [8] 46/-1/-1->45->44 [9] 46/-1/-1->45->44 [10] 46/-1/-1->45->44 [11] 46/-1/-1->45->44 [12] 46/-1/-1->45->44 [13] 46/13/-1->45->-1 [14] -1/-1/-1->45->44 [15] 46/-1/-1->45->44
45: nodeai07:2435732:2437073 [5] NCCL INFO P2P Chunksize set to 131072
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 00/16 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 01/16 :    0   3   2   9  15  14  13  12   8  11  10  17  23  22  21  20  16  19  18  25
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 02/16 :    0   3  10  15  14  13  12   9   8  11  18  23  22  21  20  17  16  19  26  31
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 03/16 :    0  11  15  14  13  12  10   9   8  19  23  22  21  20  18  17  16  27  31  30
 8: nodeai02:2459395:2460731 [0] NCCL INFO Trees [0] 9/-1/-1->8->16 [1] -1/-1/-1->8->15 [2] 9/-1/-1->8->15 [3] 9/-1/-1->8->15 [4] 9/-1/-1->8->15 [5] 9/-1/-1->8->15 [6] 9/-1/-1->8->15 [7] 9/-1/-1->8->15 [8] 9/24/0->8->40 [9] -1/-1/-1->8->15 [10] 9/-1/-1->8->15 [11] 9/-1/-1->8->15 [12] 9/-1/-1->8->15 [13] 9/-1/-1->8->15 [14] 9/-1/-1->8->15 [15] 9/-1/-1->8->15
 8: nodeai02:2459395:2460731 [0] NCCL INFO P2P Chunksize set to 131072
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 1: nodeai01:2157663:2158994 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/33/-1->1->-1 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->9 [10] -1/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0
 1: nodeai01:2157663:2158994 [1] NCCL INFO P2P Chunksize set to 131072
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 04/16 :    0   7   6   5  12  11  10   9   8  15  14  13  20  19  18  17  16  23  22  21
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 05/16 :    0   4   7   6  13  11  10   9   8  12  15  14  21  19  18  17  16  20  23  22
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 06/16 :    0   5   4   7  14  11  10   9   8  13  12  15  22  19  18  17  16  21  20  23
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 07/16 :    0   6   5   4  15  11  10   9   8  14  13  12  23  19  18  17  16  22  21  20
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 08/16 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 09/16 :    0   3   2   9  15  14  13  12   8  11  10  17  23  22  21  20  16  19  18  25
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 10/16 :    0   3  10  15  14  13  12   9   8  11  18  23  22  21  20  17  16  19  26  31
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 11/16 :    0  11  15  14  13  12  10   9   8  19  23  22  21  20  18  17  16  27  31  30
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 12/16 :    0   7   6   5  12  11  10   9   8  15  14  13  20  19  18  17  16  23  22  21
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 13/16 :    0   4   7   6  13  11  10   9   8  12  15  14  21  19  18  17  16  20  23  22
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 14/16 :    0   5   4   7  14  11  10   9   8  13  12  15  22  19  18  17  16  21  20  23
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 15/16 :    0   6   5   4  15  11  10   9   8  14  13  12  23  19  18  17  16  22  21  20
 0: nodeai01:2157660:2158992 [0] NCCL INFO Trees [0] 1/32/-1->0->-1 [1] -1/-1/-1->0->7 [2] 1/-1/-1->0->7 [3] 1/-1/-1->0->7 [4] 1/-1/-1->0->7 [5] 1/-1/-1->0->7 [6] 1/-1/-1->0->7 [7] 1/-1/-1->0->7 [8] 1/-1/-1->0->8 [9] -1/-1/-1->0->7 [10] 1/-1/-1->0->7 [11] 1/-1/-1->0->7 [12] 1/-1/-1->0->7 [13] 1/-1/-1->0->7 [14] 1/-1/-1->0->7 [15] 1/-1/-1->0->7
 0: nodeai01:2157660:2158992 [0] NCCL INFO P2P Chunksize set to 131072
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 2: nodeai01:2157662:2158993 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/34/-1->2->-1 [3] -1/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->10 [11] -1/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1
 2: nodeai01:2157662:2158993 [2] NCCL INFO P2P Chunksize set to 131072
 9: nodeai02:2459396:2460733 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->17 [2] -1/-1/-1->9->8 [3] 10/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] 10/-1/-1->9->8 [7] 10/-1/-1->9->8 [8] 10/-1/-1->9->8 [9] 10/25/1->9->41 [10] -1/-1/-1->9->8 [11] 10/-1/-1->9->8 [12] 10/-1/-1->9->8 [13] 10/-1/-1->9->8 [14] 10/-1/-1->9->8 [15] 10/-1/-1->9->8
 9: nodeai02:2459396:2460733 [1] NCCL INFO P2P Chunksize set to 131072
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 3: nodeai01:2157667:2158995 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/35/-1->3->-1 [4] -1/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->11 [12] -1/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2
 3: nodeai01:2157667:2158995 [3] NCCL INFO P2P Chunksize set to 131072
10: nodeai02:2459393:2460732 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->18 [3] -1/-1/-1->10->9 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->9 [7] 11/-1/-1->10->9 [8] 11/-1/-1->10->9 [9] 11/-1/-1->10->9 [10] 11/26/2->10->42 [11] -1/-1/-1->10->9 [12] 11/-1/-1->10->9 [13] 11/-1/-1->10->9 [14] 11/-1/-1->10->9 [15] 11/-1/-1->10->9
10: nodeai02:2459393:2460732 [2] NCCL INFO P2P Chunksize set to 131072
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 4: nodeai01:2157666:2158999 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/36/-1->4->-1 [5] -1/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->12 [13] -1/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3
 4: nodeai01:2157666:2158999 [4] NCCL INFO P2P Chunksize set to 131072
11: nodeai02:2459390:2460737 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->19 [4] -1/-1/-1->11->10 [5] 12/-1/-1->11->10 [6] 12/-1/-1->11->10 [7] 12/-1/-1->11->10 [8] 12/-1/-1->11->10 [9] 12/-1/-1->11->10 [10] 12/-1/-1->11->10 [11] 12/27/3->11->43 [12] -1/-1/-1->11->10 [13] 12/-1/-1->11->10 [14] 12/-1/-1->11->10 [15] 12/-1/-1->11->10
11: nodeai02:2459390:2460737 [3] NCCL INFO P2P Chunksize set to 131072
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 5: nodeai01:2157665:2158998 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/37/-1->5->-1 [6] -1/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->13 [14] -1/-1/-1->5->4 [15] 6/-1/-1->5->4
 5: nodeai01:2157665:2158998 [5] NCCL INFO P2P Chunksize set to 131072
12: nodeai02:2459394:2460735 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11 [4] 13/-1/-1->12->20 [5] -1/-1/-1->12->11 [6] 13/-1/-1->12->11 [7] 13/-1/-1->12->11 [8] 13/-1/-1->12->11 [9] 13/-1/-1->12->11 [10] 13/-1/-1->12->11 [11] 13/-1/-1->12->11 [12] 13/28/4->12->44 [13] -1/-1/-1->12->11 [14] 13/-1/-1->12->11 [15] 13/-1/-1->12->11
12: nodeai02:2459394:2460735 [4] NCCL INFO P2P Chunksize set to 131072
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 6: nodeai01:2157661:2158997 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/38/-1->6->-1 [7] -1/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->14 [15] -1/-1/-1->6->5
 6: nodeai01:2157661:2158997 [6] NCCL INFO P2P Chunksize set to 131072
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  0:  0  8 16 24 32 40
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  1:  1  9 17 25 33 41
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  2:  2 10 18 26 34 42
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  3:  3 11 19 27 35 43
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  4:  4 12 20 28 36 44
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  5:  5 13 21 29 37 45
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  6:  6 14 22 30 38 46
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS Head  7:  7 15 23 31 39 47
 7: nodeai01:2157664:2158996 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] 0/-1/-1->7->6 [3] 0/-1/-1->7->6 [4] 0/-1/-1->7->6 [5] 0/-1/-1->7->6 [6] 0/-1/-1->7->6 [7] 0/39/-1->7->-1 [8] -1/-1/-1->7->6 [9] 0/-1/-1->7->6 [10] 0/-1/-1->7->6 [11] 0/-1/-1->7->6 [12] 0/-1/-1->7->6 [13] 0/-1/-1->7->6 [14] 0/-1/-1->7->6 [15] 0/-1/-1->7->15
 7: nodeai01:2157664:2158996 [7] NCCL INFO P2P Chunksize set to 131072
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 00/0 : 23[7] -> 24[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 23[7] -> 24[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 15[7] -> 16[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 15[7] -> 16[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[1] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 31[7] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 31[7] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 00/0 : 39[7] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 08/0 : 39[7] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 7[7] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 00/0 : 47[7] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 08/0 : 47[7] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 8[0] -> 9[1] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 16[0] -> 17[1] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 00/0 : 14[6] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 00/0 : 15[7] -> 16[0] [send] via NET/IBext/0(8)/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 08/0 : 14[6] -> 15[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 08/0 : 15[7] -> 16[0] [send] via NET/IBext/0(8)/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 17[1] -> 18[2] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 08/0 : 13[5] -> 14[6] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 00/0 : 12[4] -> 13[5] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 33[1] -> 34[2] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 00/0 : 29[5] -> 30[6] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 08/0 : 12[4] -> 13[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 00/0 : 28[4] -> 29[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 17[1] -> 18[2] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[3] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 08/0 : 29[5] -> 30[6] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 34[2] -> 35[3] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 33[1] -> 34[2] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 08/0 : 9[1] -> 10[2] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 00/0 : 30[6] -> 31[7] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 00/0 : 7[7] -> 8[0] [send] via NET/IBext/0(0)/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 08/0 : 28[4] -> 29[5] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 08/0 : 7[7] -> 8[0] [send] via NET/IBext/0(0)/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 38[6] -> 39[7] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 23[7] -> 24[0] [send] via NET/IBext/0(16)/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 23[7] -> 24[0] [send] via NET/IBext/0(16)/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 34[2] -> 35[3] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 08/0 : 30[6] -> 31[7] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 38[6] -> 39[7] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 37[5] -> 38[6] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 35[3] -> 36[4] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 00/0 : 27[3] -> 28[4] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 36[4] -> 37[5] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 35[3] -> 36[4] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 37[5] -> 38[6] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 00/0 : 43[3] -> 44[4] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 18[2] -> 19[3] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[4] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 19[3] -> 20[4] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 01/0 : 16[0] -> 19[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 36[4] -> 37[5] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 24[0] -> 25[1] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 21[5] -> 22[6] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 00/0 : 44[4] -> 45[5] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 00/0 : 45[5] -> 46[6] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 08/0 : 25[1] -> 26[2] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 08/0 : 26[2] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 08/0 : 27[3] -> 28[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 32[0] -> 33[1] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 00/0 : 41[1] -> 42[2] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 00/0 : 42[2] -> 43[3] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 08/0 : 40[0] -> 41[1] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 00/0 : 46[6] -> 47[7] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 08/0 : 44[4] -> 45[5] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 08/0 : 45[5] -> 46[6] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 22[6] -> 23[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 00/0 : 31[7] -> 32[0] [send] via NET/IBext/0(24)/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 08/0 : 31[7] -> 32[0] [send] via NET/IBext/0(24)/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 08/0 : 11[3] -> 12[4] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 8[0] -> 11[3] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 17[1] -> 23[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 16[0] -> 19[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 28[4] -> 31[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 34[2] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 05/0 : 36[4] -> 39[7] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 08/0 : 41[1] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 08/0 : 46[6] -> 47[7] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 22[6] -> 23[7] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 39[7] -> 40[0] [send] via NET/IBext/0(32)/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 24[0] -> 27[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 39[7] -> 40[0] [send] via NET/IBext/0(32)/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 20[4] -> 21[5] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 33[1] -> 39[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 01/0 : 32[0] -> 35[3] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 25[1] -> 31[7] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 02/0 : 26[2] -> 31[7] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 09/0 : 17[1] -> 23[7] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 21[5] -> 22[6] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 08/0 : 43[3] -> 44[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 34[2] -> 39[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 32[0] -> 35[3] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 09/0 : 33[1] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 36[4] -> 39[7] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 08/0 : 42[2] -> 43[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 00/0 : 47[7] -> 0[0] [send] via NET/IBext/0(40)/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 08/0 : 47[7] -> 0[0] [send] via NET/IBext/0(40)/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 08/0 : 10[2] -> 11[3] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 12[4] -> 15[7] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 20[4] -> 21[5] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 02/0 : 24[0] -> 27[3] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 09/0 : 16[0] -> 19[3] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 24[0] -> 27[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 06/0 : 28[4] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 25[1] -> 31[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 05/0 : 4[4] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 09/0 : 32[0] -> 35[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 13/0 : 36[4] -> 39[7] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 05/0 : 44[4] -> 47[7] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 9[1] -> 15[7] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 02/0 : 10[2] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 06/0 : 12[4] -> 15[7] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 10[2] -> 15[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 02/0 : 2[2] -> 7[7] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 01/0 : 1[1] -> 7[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 01/0 : 40[0] -> 43[3] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 02/0 : 42[2] -> 47[7] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 01/0 : 41[1] -> 47[7] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 28[4] -> 31[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 10/0 : 24[0] -> 27[3] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 26[2] -> 31[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 32[0] -> 35[3] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 06/0 : 44[4] -> 47[7] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 09/0 : 41[1] -> 47[7] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 10/0 : 42[2] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 02/0 : 40[0] -> 43[3] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 8[0] -> 11[3] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 9[1] -> 15[7] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 10/0 : 8[0] -> 11[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 12[4] -> 15[7] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 10/0 : 2[2] -> 7[7] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 36[4] -> 39[7] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 09/0 : 1[1] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 06/0 : 4[4] -> 7[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 14/0 : 28[4] -> 31[7] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 18[2] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 16[0] -> 19[3] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 09/0 : 40[0] -> 43[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 05/0 : 20[4] -> 23[7] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 13/0 : 44[4] -> 47[7] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 14/0 : 44[4] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 10/0 : 40[0] -> 43[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 20[4] -> 23[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 05/0 : 40[0] -> 44[4] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 03/0 : 43[3] -> 47[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 14/0 : 12[4] -> 15[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 13/0 : 4[4] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 05/0 : 32[0] -> 36[4] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 35[3] -> 39[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[4] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 27[3] -> 31[7] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 13/0 : 20[4] -> 23[7] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[4] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 11[3] -> 15[7] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 20[4] -> 23[7] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 8[0] -> 12[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 13/0 : 32[0] -> 36[4] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 11/0 : 35[3] -> 39[7] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 11[3] -> 15[7] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 11/0 : 43[3] -> 47[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 13/0 : 40[0] -> 44[4] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 18[2] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[4] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 19[3] -> 23[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 24[0] -> 28[4] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[4] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 14/0 : 4[4] -> 7[7] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 27[3] -> 31[7] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 11/0 : 19[3] -> 23[7] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 13/0 : 16[0] -> 20[4] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 35[3] -> 42[2] [send] via NET/IBext/2(34)/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 35[3] -> 42[2] [send] via NET/IBext/2(34)/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 06/0 : 24[0] -> 29[5] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 11/0 : 3[3] -> 7[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 32[0] -> 37[5] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 16[0] -> 21[5] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 04/0 : 21[5] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 02/0 : 27[3] -> 34[2] [send] via NET/IBext/2(26)/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 21[5] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 10/0 : 27[3] -> 34[2] [send] via NET/IBext/2(26)/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 29[5] -> 36[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 29[5] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 06/0 : 8[0] -> 13[5] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 13[5] -> 20[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 13[5] -> 20[4] [receive] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 14/0 : 24[0] -> 29[5] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 13/0 : 0[0] -> 4[4] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [receive] via NET/IBext/4/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 32[0] -> 37[5] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 06/0 : 40[0] -> 45[5] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 02/0 : 43[3] -> 2[2] [send] via NET/IBext/2(42)/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 10/0 : 43[3] -> 2[2] [send] via NET/IBext/2(42)/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 14/0 : 8[0] -> 13[5] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 04/0 : 37[5] -> 44[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 12/0 : 37[5] -> 44[4] [receive] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 24[0] -> 30[6] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 27[3] -> 34[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 27[3] -> 34[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 02/0 : 19[3] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 19[3] -> 26[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 19[3] -> 26[2] [send] via NET/IBext/2(18)/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 19[3] -> 26[2] [send] via NET/IBext/2(18)/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 14/0 : 40[0] -> 45[5] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 02/0 : 11[3] -> 18[2] [send] via NET/IBext/2(10)/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 10/0 : 11[3] -> 18[2] [send] via NET/IBext/2(10)/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 16[0] -> 21[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 22[6] -> 29[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 22[6] -> 29[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 04/0 : 29[5] -> 36[4] [send] via NET/IBext/4(28)/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 12/0 : 29[5] -> 36[4] [send] via NET/IBext/4(28)/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 01/0 : 34[2] -> 41[1] [send] via NET/IBext/1(33)/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 09/0 : 34[2] -> 41[1] [send] via NET/IBext/1(33)/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 26[2] -> 33[1] [send] via NET/IBext/1(25)/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 26[2] -> 33[1] [send] via NET/IBext/1(25)/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 8[0] -> 14[6] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 24[0] -> 30[6] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 16[0] -> 27[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 16[0] -> 27[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 02/0 : 35[3] -> 42[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 10/0 : 35[3] -> 42[2] [receive] via NET/IBext/2/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 07/0 : 32[0] -> 38[6] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 04/0 : 13[5] -> 20[4] [send] via NET/IBext/4(12)/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 12/0 : 13[5] -> 20[4] [send] via NET/IBext/4(12)/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 07/0 : 16[0] -> 22[6] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 01/0 : 42[2] -> 1[1] [send] via NET/IBext/1(41)/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 09/0 : 42[2] -> 1[1] [send] via NET/IBext/1(41)/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 30[6] -> 37[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 13/0 : 30[6] -> 37[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 37[5] -> 44[4] [send] via NET/IBext/4(36)/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 37[5] -> 44[4] [send] via NET/IBext/4(36)/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 8[0] -> 14[6] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [receive] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 14[6] -> 21[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 13/0 : 14[6] -> 21[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 21[5] -> 28[4] [send] via NET/IBext/4(20)/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 18[2] -> 25[1] [receive] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 21[5] -> 28[4] [send] via NET/IBext/4(20)/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 18[2] -> 25[1] [receive] via NET/IBext/1/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 07/0 : 40[0] -> 46[6] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 24[0] -> 35[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 11/0 : 24[0] -> 35[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 8[0] -> 19[3] [receive] via NET/IBext/3/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 15/0 : 32[0] -> 38[6] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 06/0 : 0[0] -> 5[5] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 10[2] -> 17[1] [send] via NET/IBext/1(9)/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 10[2] -> 17[1] [send] via NET/IBext/1(9)/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 15/0 : 16[0] -> 22[6] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 05/0 : 38[6] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 13/0 : 38[6] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 04/0 : 45[5] -> 4[4] [send] via NET/IBext/4(44)/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 12/0 : 45[5] -> 4[4] [send] via NET/IBext/4(44)/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 04/0 : 45[5] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 12/0 : 45[5] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 04/0 : 8[0] -> 15[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 04/0 : 24[0] -> 31[7] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 11/0 : 8[0] -> 19[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 07/0 : 36[4] -> 47[7] [send] via NET/IBext/7(39)/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 32[0] -> 39[7] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 15/0 : 36[4] -> 47[7] [send] via NET/IBext/7(39)/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 26[2] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 09/0 : 26[2] -> 33[1] [receive] via NET/IBext/1/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 15/0 : 40[0] -> 46[6] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 01/0 : 34[2] -> 41[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 09/0 : 34[2] -> 41[1] [receive] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 14[6] -> 21[5] [send] via NET/IBext/5(13)/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 14[6] -> 21[5] [send] via NET/IBext/5(13)/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 32[0] -> 39[7] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 21[5] -> 19[3] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 31[7] -> 38[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 07/0 : 20[4] -> 31[7] [send] via NET/IBext/7(23)/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 31[7] -> 38[6] [receive] via NET/IBext/6/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 16[0] -> 23[7] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 15/0 : 20[4] -> 31[7] [send] via NET/IBext/7(23)/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 05/0 : 38[6] -> 45[5] [send] via NET/IBext/5(37)/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 13/0 : 38[6] -> 45[5] [send] via NET/IBext/5(37)/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 06/0 : 23[7] -> 30[6] [receive] via NET/IBext/6/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 07/0 : 4[4] -> 15[7] [send] via NET/IBext/7(7)/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 11[3] -> 18[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 23[7] -> 30[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 11[3] -> 18[2] [receive] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 13/0 : 21[5] -> 19[3] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 30[6] -> 37[5] [send] via NET/IBext/5(29)/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 02/0 : 3[3] -> 10[2] [send] via NET/IBext/2(2)/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 30[6] -> 37[5] [send] via NET/IBext/5(29)/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 10/0 : 3[3] -> 10[2] [send] via NET/IBext/2(2)/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 05/0 : 45[5] -> 43[3] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 12/0 : 8[0] -> 15[7] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 37[5] -> 35[3] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 12/0 : 24[0] -> 31[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 04/0 : 40[0] -> 47[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 14/0 : 0[0] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 01/0 : 18[2] -> 25[1] [send] via NET/IBext/1(17)/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 09/0 : 18[2] -> 25[1] [send] via NET/IBext/1(17)/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 15[7] -> 22[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 15[7] -> 22[6] [receive] via NET/IBext/6/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 12/0 : 40[0] -> 47[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 03/0 : 40[0] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 11/0 : 40[0] -> 3[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 39[7] -> 46[6] [send] via NET/IBext/6(38)/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 39[7] -> 46[6] [send] via NET/IBext/6(38)/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 03/0 : 32[0] -> 43[3] [send] via NET/IBext/3(35)/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 11/0 : 32[0] -> 43[3] [send] via NET/IBext/3(35)/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 16[0] -> 23[7] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 06/0 : 39[7] -> 46[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 14/0 : 39[7] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [receive] via NET/IBext/1/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 0[0] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [receive] via NET/IBext/1/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 0[0] -> 11[3] [receive] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 05/0 : 22[6] -> 29[5] [send] via NET/IBext/5(21)/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 13/0 : 22[6] -> 29[5] [send] via NET/IBext/5(21)/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 10[2] -> 17[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 09/0 : 10[2] -> 17[1] [receive] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 29[5] -> 27[3] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 13/0 : 45[5] -> 43[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 15/0 : 4[4] -> 15[7] [send] via NET/IBext/7(7)/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 07/0 : 44[4] -> 7[7] [send] via NET/IBext/7(47)/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 28[4] -> 39[7] [send] via NET/IBext/7(31)/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 28[4] -> 39[7] [send] via NET/IBext/7(31)/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 06/0 : 31[7] -> 38[6] [send] via NET/IBext/6(30)/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 14/0 : 31[7] -> 38[6] [send] via NET/IBext/6(30)/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 38[6] -> 35[3] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 24[0] -> 35[3] [send] via NET/IBext/3(27)/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 20[4] -> 31[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 24[0] -> 35[3] [send] via NET/IBext/3(27)/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 13/0 : 37[5] -> 35[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 20[4] -> 31[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 31[7] -> 27[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 01/0 : 20[4] -> 16[0] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 31[7] -> 27[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 06/0 : 15[7] -> 22[6] [send] via NET/IBext/6(14)/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 14/0 : 15[7] -> 22[6] [send] via NET/IBext/6(14)/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 8[0] -> 19[3] [send] via NET/IBext/3(11)/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 22[6] -> 19[3] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 8[0] -> 19[3] [send] via NET/IBext/3(11)/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 02/0 : 43[3] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 10/0 : 43[3] -> 2[2] [receive] via NET/IBext/2/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 22[6] -> 19[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 23[7] -> 30[6] [send] via NET/IBext/6(22)/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 23[7] -> 30[6] [send] via NET/IBext/6(22)/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 07/0 : 0[0] -> 6[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 4[4] -> 15[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 05/0 : 46[6] -> 5[5] [send] via NET/IBext/5(45)/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 15/0 : 44[4] -> 7[7] [send] via NET/IBext/7(47)/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 13/0 : 46[6] -> 5[5] [send] via NET/IBext/5(45)/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 4[4] -> 15[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 06/0 : 47[7] -> 6[6] [send] via NET/IBext/6(46)/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 06/0 : 30[6] -> 27[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 14/0 : 47[7] -> 6[6] [send] via NET/IBext/6(46)/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 12[4] -> 23[7] [receive] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 15/0 : 12[4] -> 23[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 28[4] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 15/0 : 28[4] -> 39[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 15[7] -> 11[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 01/0 : 4[4] -> 0[0] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 39[7] -> 35[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 28[4] -> 24[0] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 38[6] -> 35[3] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 30[6] -> 27[3] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 29[5] -> 27[3] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 01/0 : 2[2] -> 9[1] [send] via NET/IBext/1(1)/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 09/0 : 2[2] -> 9[1] [send] via NET/IBext/1(1)/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 15[7] -> 11[3] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 05/0 : 46[6] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 13/0 : 46[6] -> 5[5] [receive] via NET/IBext/5/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 03/0 : 40[0] -> 3[3] [send] via NET/IBext/3(43)/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 03/0 : 32[0] -> 43[3] [receive] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 11/0 : 40[0] -> 3[3] [send] via NET/IBext/3(43)/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 11/0 : 32[0] -> 43[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 09/0 : 20[4] -> 16[0] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 01/0 : 42[2] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 15/0 : 0[0] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 09/0 : 4[4] -> 0[0] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 09/0 : 42[2] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 04/0 : 5[5] -> 12[4] [send] via NET/IBext/4(4)/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 03/0 : 16[0] -> 27[3] [send] via NET/IBext/3(19)/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 11/0 : 16[0] -> 27[3] [send] via NET/IBext/3(19)/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 04/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 12/0 : 5[5] -> 12[4] [send] via NET/IBext/4(4)/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 15/0 : 39[7] -> 35[3] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 06/0 : 46[6] -> 43[3] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 05/0 : 5[5] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 28[4] -> 24[0] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 06/0 : 47[7] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 14/0 : 47[7] -> 6[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 12[4] -> 23[7] [send] via NET/IBext/7(15)/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 12[4] -> 23[7] [send] via NET/IBext/7(15)/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 23[7] -> 19[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 12[4] -> 8[0] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 06/0 : 7[7] -> 14[6] [send] via NET/IBext/6(6)/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 14/0 : 7[7] -> 14[6] [send] via NET/IBext/6(6)/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 06/0 : 14[6] -> 11[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 02/0 : 28[4] -> 25[1] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 15/0 : 23[7] -> 19[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 20[4] -> 17[1] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 12[4] -> 8[0] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 14/0 : 46[6] -> 43[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 10/0 : 28[4] -> 25[1] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 31[7] -> 30[6] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 12/0 : 0[0] -> 7[7] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 01/0 : 39[7] -> 38[6] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 13/0 : 5[5] -> 3[3] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 14[6] -> 11[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 20[4] -> 17[1] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 01/0 : 23[7] -> 22[6] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 03/0 : 20[4] -> 18[2] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 28[4] -> 26[2] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 39[7] -> 38[6] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 03/0 : 0[0] -> 11[3] [send] via NET/IBext/3(3)/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 28[4] -> 26[2] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 11/0 : 0[0] -> 11[3] [send] via NET/IBext/3(3)/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 07/0 : 44[4] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 15/0 : 44[4] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 07/0 : 7[7] -> 3[3] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 05/0 : 6[6] -> 13[5] [send] via NET/IBext/5(5)/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 13/0 : 6[6] -> 13[5] [send] via NET/IBext/5(5)/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 01/0 : 44[4] -> 40[0] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 02/0 : 12[4] -> 9[1] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 07/0 : 36[4] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 15/0 : 36[4] -> 47[7] [receive] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 06/0 : 6[6] -> 3[3] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 13[5] -> 11[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 23[7] -> 22[6] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 01/0 : 36[4] -> 32[0] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 07/0 : 47[7] -> 43[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 03/0 : 23[7] -> 22[6] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 11/0 : 20[4] -> 18[2] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 03/0 : 39[7] -> 38[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 02/0 : 31[7] -> 30[6] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 09/0 : 36[4] -> 32[0] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 10/0 : 12[4] -> 9[1] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 15/0 : 47[7] -> 43[3] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 09/0 : 44[4] -> 40[0] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 23[7] -> 22[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 39[7] -> 38[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 05/0 : 39[7] -> 38[6] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 12[4] -> 10[2] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 13[5] -> 11[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 36[4] -> 33[1] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 01/0 : 47[7] -> 46[6] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 05/0 : 23[7] -> 22[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 31[7] -> 30[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 09/0 : 39[7] -> 38[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 04/0 : 31[7] -> 30[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 02/0 : 47[7] -> 46[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 39[7] -> 38[6] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 02/0 : 44[4] -> 41[1] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 15[7] -> 14[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 03/0 : 47[7] -> 46[6] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 10/0 : 44[4] -> 41[1] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 09/0 : 23[7] -> 22[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 15/0 : 7[7] -> 3[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 02/0 : 4[4] -> 1[1] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 23[7] -> 22[6] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 11/0 : 23[7] -> 22[6] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 12[4] -> 10[2] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 31[7] -> 30[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 31[7] -> 30[6] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 36[4] -> 33[1] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 10/0 : 31[7] -> 30[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 11/0 : 39[7] -> 38[6] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 01/0 : 35[3] -> 34[2] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 03/0 : 36[4] -> 34[2] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 04/0 : 47[7] -> 46[6] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 23[7] -> 22[6] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 13/0 : 23[7] -> 22[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 02/0 : 15[7] -> 14[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 15[7] -> 14[6] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 14/0 : 6[6] -> 3[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 11/0 : 36[4] -> 34[2] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 10/0 : 4[4] -> 1[1] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 39[7] -> 38[6] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 04/0 : 35[3] -> 34[2] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 03/0 : 44[4] -> 42[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 01/0 : 19[3] -> 18[2] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 04/0 : 15[7] -> 14[6] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 03/0 : 4[4] -> 2[2] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 13/0 : 39[7] -> 38[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 05/0 : 47[7] -> 46[6] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 11/0 : 44[4] -> 42[2] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 09/0 : 47[7] -> 46[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 31[7] -> 30[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 15[7] -> 14[6] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 11/0 : 4[4] -> 2[2] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 15[7] -> 14[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 12/0 : 31[7] -> 30[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 31[7] -> 30[6] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 04/0 : 19[3] -> 18[2] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 05/0 : 35[3] -> 34[2] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 10/0 : 15[7] -> 14[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 05/0 : 19[3] -> 18[2] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 15[7] -> 14[6] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 35[3] -> 34[2] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 04/0 : 28[4] -> 27[3] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 19[3] -> 18[2] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 10/0 : 47[7] -> 46[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 11/0 : 47[7] -> 46[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 27[3] -> 26[2] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 28[4] -> 27[3] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 07/0 : 19[3] -> 18[2] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 07/0 : 35[3] -> 34[2] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 01/0 : 38[6] -> 37[5] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 12/0 : 47[7] -> 46[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 12/0 : 15[7] -> 14[6] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 04/0 : 27[3] -> 26[2] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 20[4] -> 19[3] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 01/0 : 22[6] -> 21[5] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 09/0 : 35[3] -> 34[2] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 01/0 : 43[3] -> 42[2] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 03/0 : 26[2] -> 25[1] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 22[6] -> 21[5] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 01/0 : 37[5] -> 36[4] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 27[3] -> 26[2] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 03/0 : 22[6] -> 21[5] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 37[5] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 38[6] -> 37[5] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 03/0 : 18[2] -> 17[1] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 12/0 : 35[3] -> 34[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 09/0 : 19[3] -> 18[2] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 13/0 : 47[7] -> 46[6] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 04/0 : 43[3] -> 42[2] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 15[7] -> 14[6] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 03/0 : 38[6] -> 37[5] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 03/0 : 37[5] -> 36[4] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 01/0 : 21[5] -> 20[4] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 20[4] -> 19[3] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 38[6] -> 37[5] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 06/0 : 37[5] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 07/0 : 38[6] -> 37[5] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 13/0 : 35[3] -> 34[2] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 11[3] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 06/0 : 27[3] -> 26[2] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 27[3] -> 26[2] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 04/0 : 25[1] -> 24[0] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 04/0 : 26[2] -> 25[1] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 07/0 : 37[5] -> 36[4] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 22[6] -> 21[5] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 09/0 : 37[5] -> 36[4] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 12/0 : 11[3] -> 10[2] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 30[6] -> 29[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 11[3] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 04/0 : 12[4] -> 11[3] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 02/0 : 30[6] -> 29[5] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 27[3] -> 26[2] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 25[1] -> 24[0] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 30[6] -> 29[5] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 12/0 : 27[3] -> 26[2] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 06/0 : 25[1] -> 24[0] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 07/0 : 22[6] -> 21[5] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 21[5] -> 20[4] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 09/0 : 38[6] -> 37[5] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 36[4] -> 35[3] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 38[6] -> 37[5] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 37[5] -> 36[4] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 09/0 : 22[6] -> 21[5] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 36[4] -> 35[3] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 02/0 : 17[1] -> 16[0] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 18[2] -> 17[1] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 11/0 : 38[6] -> 37[5] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 04/0 : 44[4] -> 43[3] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 14/0 : 11[3] -> 10[2] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[4] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 25[1] -> 24[0] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 29[5] -> 28[4] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 04/0 : 30[6] -> 29[5] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 10/0 : 25[1] -> 24[0] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 03/0 : 21[5] -> 20[4] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 12/0 : 19[3] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 06/0 : 21[5] -> 20[4] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 22[6] -> 21[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 03/0 : 17[1] -> 16[0] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 13/0 : 19[3] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 07/0 : 21[5] -> 20[4] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 11/0 : 37[5] -> 36[4] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 35[3] -> 34[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 12/0 : 44[4] -> 43[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 05/0 : 43[3] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 01/0 : 46[6] -> 45[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 02/0 : 29[5] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 26[2] -> 25[1] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 12[4] -> 11[3] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 27[3] -> 26[2] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 11[3] -> 10[2] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 10/0 : 9[1] -> 8[0] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 11/0 : 22[6] -> 21[5] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 38[6] -> 37[5] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 15/0 : 35[3] -> 34[2] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 05/0 : 18[2] -> 17[1] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 15/0 : 38[6] -> 37[5] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 22[6] -> 21[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 17[1] -> 16[0] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 02/0 : 13[5] -> 12[4] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 13[5] -> 12[4] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 03/0 : 10[2] -> 9[1] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 07/0 : 30[6] -> 29[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 29[5] -> 28[4] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 25[1] -> 24[0] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 14/0 : 37[5] -> 36[4] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 09/0 : 21[5] -> 20[4] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 19[3] -> 18[2] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 18[2] -> 17[1] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 15/0 : 22[6] -> 21[5] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 15/0 : 37[5] -> 36[4] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 02/0 : 33[1] -> 32[0] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 9[1] -> 8[0] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 02/0 : 46[6] -> 45[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 12/0 : 9[1] -> 8[0] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 06/0 : 43[3] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 03/0 : 46[6] -> 45[5] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 21[5] -> 20[4] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 30[6] -> 29[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 05/0 : 17[1] -> 16[0] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 14/0 : 27[3] -> 26[2] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 15/0 : 19[3] -> 18[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 06/0 : 26[2] -> 25[1] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 10/0 : 30[6] -> 29[5] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 27[3] -> 26[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 11/0 : 21[5] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 17[1] -> 16[0] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 9[1] -> 8[0] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 04/0 : 46[6] -> 45[5] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 01/0 : 45[5] -> 44[4] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 06/0 : 13[5] -> 12[4] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 14/0 : 9[1] -> 8[0] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 02/0 : 41[1] -> 40[0] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 07/0 : 43[3] -> 42[2] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 06/0 : 29[5] -> 28[4] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 07/0 : 18[2] -> 17[1] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 29[5] -> 28[4] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 12/0 : 25[1] -> 24[0] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 11/0 : 18[2] -> 17[1] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 14/0 : 21[5] -> 20[4] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 02/0 : 45[5] -> 44[4] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 03/0 : 45[5] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 03/0 : 41[1] -> 40[0] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 09/0 : 43[3] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 07/0 : 46[6] -> 45[5] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 06/0 : 45[5] -> 44[4] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 13[5] -> 12[4] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 14[6] -> 13[5] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 03/0 : 33[1] -> 32[0] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 30[6] -> 29[5] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 26[2] -> 25[1] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 9[1] -> 8[0] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 04/0 : 41[1] -> 40[0] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 09/0 : 46[6] -> 45[5] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 15/0 : 21[5] -> 20[4] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 03/0 : 42[2] -> 41[1] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 06/0 : 10[2] -> 9[1] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 13[5] -> 12[4] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 29[5] -> 28[4] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 12/0 : 30[6] -> 29[5] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 25[1] -> 24[0] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 10/0 : 29[5] -> 28[4] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 15/0 : 30[6] -> 29[5] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 14/0 : 25[1] -> 24[0] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 10/0 : 46[6] -> 45[5] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 02/0 : 14[6] -> 13[5] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 12/0 : 43[3] -> 42[2] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 10/0 : 13[5] -> 12[4] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 10[2] -> 9[1] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 11/0 : 26[2] -> 25[1] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 18[2] -> 17[1] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 03/0 : 34[2] -> 33[1] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 11/0 : 46[6] -> 45[5] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 04/0 : 42[2] -> 41[1] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 07/0 : 45[5] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 05/0 : 41[1] -> 40[0] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 29[5] -> 28[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 14/0 : 29[5] -> 28[4] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 12/0 : 26[2] -> 25[1] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 07/0 : 17[1] -> 16[0] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 12/0 : 46[6] -> 45[5] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 14[6] -> 13[5] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 09/0 : 45[5] -> 44[4] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 13[5] -> 12[4] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 11/0 : 10[2] -> 9[1] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 04/0 : 14[6] -> 13[5] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 25[1] -> 24[0] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 29[5] -> 28[4] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 05/0 : 42[2] -> 41[1] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 13/0 : 43[3] -> 42[2] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 15/0 : 46[6] -> 45[5] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 06/0 : 42[2] -> 41[1] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 14/0 : 43[3] -> 42[2] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 12/0 : 10[2] -> 9[1] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 06/0 : 41[1] -> 40[0] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 10/0 : 45[5] -> 44[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 10[2] -> 9[1] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 07/0 : 14[6] -> 13[5] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 14/0 : 10[2] -> 9[1] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 14/0 : 13[5] -> 12[4] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 14[6] -> 13[5] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 26[2] -> 25[1] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 11/0 : 45[5] -> 44[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 07/0 : 42[2] -> 41[1] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 10[2] -> 9[1] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 13[5] -> 12[4] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 13/0 : 18[2] -> 17[1] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 15/0 : 43[3] -> 42[2] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 07/0 : 41[1] -> 40[0] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 14/0 : 45[5] -> 44[4] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 10/0 : 14[6] -> 13[5] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 14[6] -> 13[5] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 10/0 : 41[1] -> 40[0] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 15/0 : 45[5] -> 44[4] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 10/0 : 17[1] -> 16[0] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 14/0 : 26[2] -> 25[1] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 11/0 : 42[2] -> 41[1] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 12/0 : 42[2] -> 41[1] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 11/0 : 41[1] -> 40[0] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 12/0 : 14[6] -> 13[5] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 33[1] -> 32[0] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 18[2] -> 17[1] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 13/0 : 42[2] -> 41[1] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 15/0 : 18[2] -> 17[1] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 12/0 : 41[1] -> 40[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 34[2] -> 33[1] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 11/0 : 17[1] -> 16[0] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 14/0 : 42[2] -> 41[1] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 05/0 : 33[1] -> 32[0] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 17[1] -> 16[0] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 13/0 : 41[1] -> 40[0] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 14/0 : 41[1] -> 40[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 05/0 : 34[2] -> 33[1] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 26[2] -> 25[1] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 15/0 : 14[6] -> 13[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 13/0 : 17[1] -> 16[0] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 33[1] -> 32[0] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 15/0 : 42[2] -> 41[1] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 15/0 : 41[1] -> 40[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 34[2] -> 33[1] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 07/0 : 33[1] -> 32[0] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 10/0 : 33[1] -> 32[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 07/0 : 34[2] -> 33[1] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 17[1] -> 16[0] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 15/0 : 17[1] -> 16[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 11/0 : 34[2] -> 33[1] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 11/0 : 33[1] -> 32[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 34[2] -> 33[1] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 33[1] -> 32[0] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 13/0 : 33[1] -> 32[0] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 33[1] -> 32[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 13/0 : 34[2] -> 33[1] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 15/0 : 33[1] -> 32[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 34[2] -> 33[1] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 15/0 : 34[2] -> 33[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Connected all rings
17: nodeai04:2417509:2418848 [1] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460731 [0] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 16[0] -> 17[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Connected all rings
45: nodeai07:2435732:2437073 [5] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 10/0 : 8[0] -> 9[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 8[0] -> 9[1] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 05/0 : 16[0] -> 17[1] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 12/0 : 8[0] -> 9[1] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 8[0] -> 9[1] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 16[0] -> 17[1] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Connected all rings
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 14/0 : 8[0] -> 9[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 8[0] -> 9[1] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 07/0 : 16[0] -> 17[1] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 16[0] -> 17[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Connected all rings
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[1] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 11/0 : 16[0] -> 17[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 04/0 : 40[0] -> 41[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 01/0 : 19[3] -> 20[4] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Connected all rings
12: nodeai02:2459394:2460735 [4] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 16[0] -> 17[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 19[3] -> 20[4] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 13/0 : 16[0] -> 17[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 19[3] -> 20[4] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 05/0 : 40[0] -> 41[1] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 16[0] -> 17[1] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Connected all rings
 4: nodeai01:2157666:2158999 [4] NCCL INFO Connected all rings
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 15/0 : 16[0] -> 17[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 05/0 : 19[3] -> 20[4] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Connected all rings
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Connected all rings
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 19[3] -> 20[4] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 01/0 : 20[4] -> 21[5] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 20[4] -> 21[5] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 03/0 : 20[4] -> 21[5] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Connected all rings
39: nodeai06:2504430:2505775 [7] NCCL INFO Connected all rings
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 07/0 : 19[3] -> 20[4] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 20[4] -> 21[5] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 06/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 07/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 10/0 : 40[0] -> 41[1] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Connected all rings
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[1] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Connected all rings
33: nodeai06:2504433:2505777 [1] NCCL INFO Connected all rings
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Connected all rings
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 17[1] -> 18[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 09/0 : 19[3] -> 20[4] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 20[4] -> 21[5] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Connected all rings
30: nodeai05:1971448:1972780 [6] NCCL INFO Connected all rings
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 9[1] -> 10[2] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Connected all rings
46: nodeai07:2435731:2437068 [6] NCCL INFO Connected all rings
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[1] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Connected all rings
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 03/0 : 17[1] -> 18[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 19[3] -> 20[4] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 07/0 : 20[4] -> 21[5] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Connected all rings
14: nodeai02:2459389:2460736 [6] NCCL INFO Connected all rings
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 32[0] -> 33[1] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 01/0 : 36[4] -> 37[5] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Connected all rings
15: nodeai02:2459391:2460730 [7] NCCL INFO Connected all rings
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 11/0 : 40[0] -> 41[1] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 01/0 : 21[5] -> 22[6] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 17[1] -> 18[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 11/0 : 19[3] -> 20[4] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 12/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 13/0 : 40[0] -> 41[1] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Connected all rings
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 14/0 : 40[0] -> 41[1] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 15/0 : 40[0] -> 41[1] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Connected all rings
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 21[5] -> 22[6] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Connected all rings
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 03/0 : 21[5] -> 22[6] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 09/0 : 20[4] -> 21[5] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 36[4] -> 37[5] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 05/0 : 32[0] -> 33[1] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 06/0 : 9[1] -> 10[2] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 9[1] -> 10[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 21[5] -> 22[6] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 13/0 : 19[3] -> 20[4] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 03/0 : 36[4] -> 37[5] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Connected all rings
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 05/0 : 17[1] -> 18[2] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 20[4] -> 21[5] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 21[5] -> 22[6] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 01/0 : 46[6] -> 47[7] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 02/0 : 46[6] -> 47[7] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 03/0 : 46[6] -> 47[7] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 01/0 : 44[4] -> 45[5] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 32[0] -> 33[1] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 04/0 : 46[6] -> 47[7] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 02/0 : 44[4] -> 45[5] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 36[4] -> 37[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 9[1] -> 10[2] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Connected all rings
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 9[1] -> 10[2] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 01/0 : 22[6] -> 23[7] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 29[5] -> 30[6] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 10[2] -> 11[3] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 17[1] -> 18[2] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 22[6] -> 23[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 07/0 : 32[0] -> 33[1] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 32[0] -> 33[1] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 05/0 : 46[6] -> 47[7] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 36[4] -> 37[5] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 11/0 : 32[0] -> 33[1] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Connected all rings
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 07/0 : 21[5] -> 22[6] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 11/0 : 20[4] -> 21[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 12/0 : 9[1] -> 10[2] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 12[4] -> 13[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 02/0 : 29[5] -> 30[6] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 19[3] -> 20[4] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 07/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 09/0 : 21[5] -> 22[6] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 29[5] -> 30[6] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 07/0 : 36[4] -> 37[5] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 32[0] -> 33[1] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 9[1] -> 10[2] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 10[2] -> 11[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 20[4] -> 21[5] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 14/0 : 9[1] -> 10[2] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 12/0 : 10[2] -> 11[3] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 06/0 : 46[6] -> 47[7] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[4] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 9[1] -> 10[2] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 09/0 : 46[6] -> 47[7] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 03/0 : 22[6] -> 23[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 10/0 : 46[6] -> 47[7] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 11/0 : 46[6] -> 47[7] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 20[4] -> 21[5] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 15/0 : 19[3] -> 20[4] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 12/0 : 46[6] -> 47[7] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 01/0 : 43[3] -> 44[4] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 03/0 : 44[4] -> 45[5] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 13/0 : 46[6] -> 47[7] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 02/0 : 43[3] -> 44[4] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 04/0 : 44[4] -> 45[5] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 14/0 : 46[6] -> 47[7] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 03/0 : 43[3] -> 44[4] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 06/0 : 44[4] -> 45[5] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 05/0 : 43[3] -> 44[4] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 07/0 : 44[4] -> 45[5] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 02/0 : 11[3] -> 12[4] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 10[2] -> 11[3] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 22[6] -> 23[7] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 11[3] -> 12[4] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 14/0 : 10[2] -> 11[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 02/0 : 12[4] -> 13[5] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 15/0 : 20[4] -> 21[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 09/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 21[5] -> 22[6] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 05/0 : 22[6] -> 23[7] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[3] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 09/0 : 44[4] -> 45[5] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 06/0 : 43[3] -> 44[4] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 28[4] -> 29[5] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 09/0 : 36[4] -> 37[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 04/0 : 29[5] -> 30[6] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 01/0 : 41[1] -> 42[2] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 10/0 : 44[4] -> 45[5] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 07/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 03/0 : 41[1] -> 42[2] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 11/0 : 44[4] -> 45[5] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 09/0 : 43[3] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 01/0 : 42[2] -> 43[3] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 04/0 : 41[1] -> 42[2] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 12/0 : 44[4] -> 45[5] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 10/0 : 43[3] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 02/0 : 42[2] -> 43[3] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 05/0 : 41[1] -> 42[2] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 36[4] -> 37[5] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 13/0 : 32[0] -> 33[1] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 14/0 : 44[4] -> 45[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 11/0 : 17[1] -> 18[2] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 11/0 : 43[3] -> 44[4] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 06/0 : 41[1] -> 42[2] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 04/0 : 42[2] -> 43[3] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 10[2] -> 11[3] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 15/0 : 44[4] -> 45[5] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 11[3] -> 12[4] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 13/0 : 43[3] -> 44[4] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 8[0] -> 15[7] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 05/0 : 42[2] -> 43[3] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 11/0 : 21[5] -> 22[6] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 07/0 : 41[1] -> 42[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 14/0 : 43[3] -> 44[4] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 06/0 : 42[2] -> 43[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 12[4] -> 13[5] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 21[5] -> 22[6] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 09/0 : 41[1] -> 42[2] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 15/0 : 43[3] -> 44[4] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 07/0 : 42[2] -> 43[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 11/0 : 36[4] -> 37[5] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 02/0 : 28[4] -> 29[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 29[5] -> 30[6] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 06/0 : 11[3] -> 12[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 36[4] -> 37[5] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 32[0] -> 33[1] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 36[4] -> 37[5] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 22[6] -> 23[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 15/0 : 32[0] -> 33[1] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 11/0 : 41[1] -> 42[2] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 11[3] -> 12[4] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 13/0 : 17[1] -> 18[2] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 12/0 : 41[1] -> 42[2] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 18[2] -> 19[3] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 09/0 : 42[2] -> 43[3] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 09/0 : 22[6] -> 23[7] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 13/0 : 41[1] -> 42[2] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 02/0 : 8[0] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 04/0 : 12[4] -> 13[5] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 13/0 : 21[5] -> 22[6] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 01/0 : 37[5] -> 38[6] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 14/0 : 41[1] -> 42[2] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 10/0 : 42[2] -> 43[3] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 15/0 : 41[1] -> 42[2] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 15/0 : 36[4] -> 37[5] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 22[6] -> 23[7] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 11[3] -> 12[4] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 28[4] -> 29[5] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 11/0 : 22[6] -> 23[7] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 17[1] -> 18[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 15/0 : 21[5] -> 22[6] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 12/0 : 42[2] -> 43[3] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 01/0 : 45[5] -> 46[6] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 01/0 : 40[0] -> 47[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 02/0 : 45[5] -> 46[6] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 29[5] -> 30[6] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Connected all rings
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 10/0 : 11[3] -> 12[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 8[0] -> 15[7] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 29[5] -> 30[6] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 22[6] -> 23[7] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 37[5] -> 38[6] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 11[3] -> 12[4] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 8[0] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 06/0 : 12[4] -> 13[5] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 15/0 : 17[1] -> 18[2] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 13/0 : 22[6] -> 23[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 02/0 : 40[0] -> 47[7] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 13/0 : 42[2] -> 43[3] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 10/0 : 29[5] -> 30[6] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 04/0 : 28[4] -> 29[5] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 03/0 : 37[5] -> 38[6] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 18[2] -> 19[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 11[3] -> 12[4] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 37[5] -> 38[6] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 06/0 : 8[0] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 12[4] -> 13[5] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[6] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 14[6] -> 15[7] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Connected all rings
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 03/0 : 45[5] -> 46[6] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 04/0 : 45[5] -> 46[6] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 30[6] -> 31[7] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 14/0 : 11[3] -> 12[4] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 22[6] -> 23[7] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 02/0 : 30[6] -> 31[7] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 05/0 : 18[2] -> 19[3] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 37[5] -> 38[6] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 11[3] -> 12[4] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 33[1] -> 34[2] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 14/0 : 42[2] -> 43[3] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 29[5] -> 30[6] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 01/0 : 16[0] -> 23[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 06/0 : 28[4] -> 29[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 12/0 : 29[5] -> 30[6] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 8[0] -> 15[7] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 02/0 : 14[6] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 12[4] -> 13[5] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 02/0 : 13[5] -> 14[6] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 07/0 : 37[5] -> 38[6] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 05/0 : 45[5] -> 46[6] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 16[0] -> 23[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 07/0 : 45[5] -> 46[6] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 15/0 : 42[2] -> 43[3] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 18[2] -> 19[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 25[1] -> 26[2] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 30[6] -> 31[7] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 13/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 01/0 : 38[6] -> 39[7] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 03/0 : 33[1] -> 34[2] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 03/0 : 16[0] -> 23[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 03/0 : 40[0] -> 47[7] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 28[4] -> 29[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 29[5] -> 30[6] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 07/0 : 18[2] -> 19[3] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 09/0 : 37[5] -> 38[6] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 8[0] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 10/0 : 12[4] -> 13[5] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 09/0 : 45[5] -> 46[6] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 05/0 : 16[0] -> 23[7] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 01/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 02/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 03/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 38[6] -> 39[7] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 13[5] -> 14[6] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 28[4] -> 29[5] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 14[6] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 12[4] -> 13[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 29[5] -> 30[6] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 05/0 : 40[0] -> 47[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 01/0 : 0[0] -> 7[7] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 09/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 09/0 : 18[2] -> 19[3] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 04/0 : 30[6] -> 31[7] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 10/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 10/0 : 8[0] -> 15[7] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 01/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 02/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 25[1] -> 26[2] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 02/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 37[5] -> 38[6] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 04/0 : 25[1] -> 26[2] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 06/0 : 30[6] -> 31[7] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 10/0 : 28[4] -> 29[5] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 25[1] -> 26[2] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 11/0 : 37[5] -> 38[6] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 33[1] -> 34[2] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 06/0 : 40[0] -> 47[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 10/0 : 45[5] -> 46[6] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 18[2] -> 19[3] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 03/0 : 38[6] -> 39[7] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 28[4] -> 29[5] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 8[0] -> 15[7] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 38[6] -> 39[7] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 28[4] -> 29[5] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 06/0 : 25[1] -> 26[2] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 04/0 : 14[6] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 04/0 : 13[5] -> 14[6] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[7] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 18[2] -> 19[3] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 05/0 : 0[0] -> 7[7] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 13/0 : 18[2] -> 19[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 14/0 : 28[4] -> 29[5] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 28[4] -> 29[5] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 10/0 : 30[6] -> 31[7] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 37[5] -> 38[6] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 30[6] -> 31[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 11/0 : 45[5] -> 46[6] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 05/0 : 33[1] -> 34[2] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 14[6] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 12[4] -> 13[5] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 07/0 : 40[0] -> 47[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 16[0] -> 23[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Connected all rings
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 8[0] -> 15[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 09/0 : 40[0] -> 47[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 12/0 : 45[5] -> 46[6] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 13/0 : 37[5] -> 38[6] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 13[5] -> 14[6] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 05/0 : 38[6] -> 39[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 06/0 : 0[0] -> 7[7] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 07/0 : 0[0] -> 7[7] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 07/0 : 16[0] -> 23[7] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 12/0 : 30[6] -> 31[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 13/0 : 45[5] -> 46[6] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 10/0 : 40[0] -> 47[7] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 15/0 : 37[5] -> 38[6] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 14/0 : 8[0] -> 15[7] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 14/0 : 12[4] -> 13[5] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 13[5] -> 14[6] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 33[1] -> 34[2] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 18[2] -> 19[3] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 30[6] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 25[1] -> 26[2] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 06/0 : 14[6] -> 15[7] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 8[0] -> 15[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 15/0 : 45[5] -> 46[6] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 13[5] -> 14[6] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 09/0 : 0[0] -> 7[7] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 15/0 : 18[2] -> 19[3] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 38[6] -> 39[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 25[1] -> 26[2] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 10/0 : 0[0] -> 7[7] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 30[6] -> 31[7] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 09/0 : 16[0] -> 23[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 25[1] -> 26[2] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 11/0 : 40[0] -> 47[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 04/0 : 24[0] -> 25[1] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 09/0 : 38[6] -> 39[7] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 07/0 : 33[1] -> 34[2] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 12[4] -> 13[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 12/0 : 25[1] -> 26[2] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 26[2] -> 27[3] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 06/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 14[6] -> 15[7] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 10/0 : 13[5] -> 14[6] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 13/0 : 40[0] -> 47[7] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 01/0 : 34[2] -> 35[3] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 10/0 : 14[6] -> 15[7] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 16[0] -> 23[7] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 24[0] -> 25[1] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 11/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 09/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 13[5] -> 14[6] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 11/0 : 0[0] -> 7[7] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 09/0 : 33[1] -> 34[2] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 38[6] -> 39[7] via P2P/IPC
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 13/0 : 0[0] -> 7[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 14/0 : 40[0] -> 47[7] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 11/0 : 38[6] -> 39[7] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 11/0 : 33[1] -> 34[2] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 14[6] -> 15[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 25[1] -> 26[2] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 04/0 : 26[2] -> 27[3] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 05/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 04/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 15/0 : 40[0] -> 47[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 11/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 14/0 : 0[0] -> 7[7] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 03/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 12/0 : 13[5] -> 14[6] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 12/0 : 14[6] -> 15[7] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 34[2] -> 35[3] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 11/0 : 16[0] -> 23[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 06/0 : 24[0] -> 25[1] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 13[5] -> 14[6] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 14[6] -> 15[7] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 00/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 07/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 15/0 : 0[0] -> 7[7] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 13/0 : 16[0] -> 23[7] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 38[6] -> 39[7] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 26[2] -> 27[3] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 33[1] -> 34[2] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 14/0 : 25[1] -> 26[2] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 34[2] -> 35[3] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 06/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 24[0] -> 25[1] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 13[5] -> 14[6] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 13/0 : 38[6] -> 39[7] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 14[6] -> 15[7] via P2P/IPC
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 16[0] -> 23[7] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 13/0 : 33[1] -> 34[2] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 06/0 : 26[2] -> 27[3] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 38[6] -> 39[7] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 06/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 04/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 15/0 : 16[0] -> 23[7] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 05/0 : 34[2] -> 35[3] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 01/0 : 35[3] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 10/0 : 24[0] -> 25[1] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 13/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 14/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 13/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 15/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 33[1] -> 34[2] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 34[2] -> 35[3] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 24[0] -> 25[1] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 25[1] -> 26[2] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 26[2] -> 27[3] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 15/0 : 33[1] -> 34[2] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 35[3] -> 36[4] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 07/0 : 34[2] -> 35[3] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 13/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 12/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 14/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 05/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 12/0 : 24[0] -> 25[1] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 04/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 06/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 05/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 06/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 13/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 14/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 13/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 05/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 06/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 06/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 06/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 06/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 13/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 14/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 06/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 24[0] -> 25[1] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 09/0 : 34[2] -> 35[3] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 35[3] -> 36[4] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 26[2] -> 27[3] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 01/0 : 32[0] -> 39[7] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 13/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 34[2] -> 35[3] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 05/0 : 35[3] -> 36[4] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 05/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 13/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 26[2] -> 27[3] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 34[2] -> 35[3] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 35[3] -> 36[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 03/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 13/0 : 34[2] -> 35[3] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 38[6] -> 37[5] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 00/0 : 14[6] -> 13[5] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 00/0 : 46[6] -> 45[5] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 14/0 : 24[0] -> 25[1] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 27[3] -> 28[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 05/0 : 32[0] -> 39[7] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 07/0 : 35[3] -> 36[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 05/0 : 46[6] -> 45[5] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 14[6] -> 13[5] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 22[6] -> 21[5] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 08/0 : 46[6] -> 45[5] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 00/0 : 30[6] -> 29[5] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 24[0] -> 25[1] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 02/0 : 27[3] -> 28[4] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 34[2] -> 35[3] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 32[0] -> 39[7] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 05/0 : 22[6] -> 21[5] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 05/0 : 38[6] -> 37[5] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 09/0 : 35[3] -> 36[4] via P2P/IPC
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 13/0 : 46[6] -> 45[5] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 00/0 : 45[5] -> 44[4] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 08/0 : 14[6] -> 13[5] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 12/0 : 26[2] -> 27[3] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[4] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 26[2] -> 27[3] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 22[6] -> 21[5] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 38[6] -> 37[5] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 35[3] -> 36[4] via P2P/IPC
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 13/0 : 38[6] -> 37[5] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 07/0 : 32[0] -> 39[7] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 15/0 : 34[2] -> 35[3] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 21[5] -> 20[4] via P2P/IPC
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 14[6] -> 13[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 00/0 : 29[5] -> 28[4] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 27[3] -> 28[4] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 30[6] -> 29[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 04/0 : 29[5] -> 28[4] via P2P/IPC
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 13/0 : 22[6] -> 21[5] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 24[0] -> 31[7] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 37[5] -> 36[4] via P2P/IPC
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 21[5] -> 20[4] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 11/0 : 35[3] -> 36[4] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 04/0 : 45[5] -> 44[4] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 04/0 : 13[5] -> 12[4] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 14/0 : 26[2] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 27[3] -> 28[4] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 21[5] -> 20[4] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 37[5] -> 36[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 09/0 : 32[0] -> 39[7] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 08/0 : 30[6] -> 29[5] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 08/0 : 29[5] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 02/0 : 24[0] -> 31[7] via P2P/IPC
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 08/0 : 13[5] -> 12[4] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 08/0 : 45[5] -> 44[4] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 09/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 09/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 09/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 12/0 : 13[5] -> 12[4] via P2P/IPC
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 21[5] -> 20[4] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 37[5] -> 36[4] via P2P/IPC
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 12/0 : 45[5] -> 44[4] via P2P/IPC
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 30[6] -> 29[5] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 26[2] -> 27[3] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 06/0 : 27[3] -> 28[4] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 13/0 : 35[3] -> 36[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 32[0] -> 39[7] via P2P/IPC
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 37[5] -> 36[4] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 35[3] -> 36[4] via P2P/IPC
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 12/0 : 29[5] -> 28[4] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 24[0] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 01/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 07/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 09/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 01/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 09/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 09/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 09/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 01/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 15/0 : 35[3] -> 36[4] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 27[3] -> 28[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 11/0 : 32[0] -> 39[7] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 11/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 10/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 12/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 12/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 11/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 10/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 11/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 24[0] -> 31[7] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 10/0 : 27[3] -> 28[4] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 13/0 : 32[0] -> 39[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 06/0 : 24[0] -> 31[7] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 24[0] -> 31[7] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 32[0] -> 39[7] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 27[3] -> 28[4] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 24[0] -> 31[7] via P2P/IPC
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 15/0 : 32[0] -> 39[7] via P2P/IPC
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 00/0 : 41[1] -> 40[0] via P2P/IPC
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 27[3] -> 28[4] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 17[1] -> 16[0] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 10/0 : 24[0] -> 31[7] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 14/0 : 27[3] -> 28[4] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 33[1] -> 32[0] via P2P/IPC
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 15/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 08/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 08/0 : 9[1] -> 8[0] via P2P/IPC
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 08/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 15/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 15/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 08/0 : 41[1] -> 40[0] via P2P/IPC
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 17[1] -> 16[0] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 24[0] -> 31[7] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 27[3] -> 28[4] via P2P/IPC
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 33[1] -> 32[0] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 24[0] -> 31[7] via P2P/IPC
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 08/0 : 25[1] -> 24[0] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 04/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 04/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 04/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 02/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 03/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 04/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 11/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 11/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 02/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 14/0 : 24[0] -> 31[7] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 03/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 11/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 04/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 12/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 04/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 03/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 04/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 11/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 10/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 24[0] -> 31[7] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 02/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 07/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 00/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 15/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 15/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 07/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 01/0 : 47[7] -> 40[0] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 01/0 : 39[7] -> 32[0] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 15[7] -> 8[0] via P2P/IPC
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 01/0 : 23[7] -> 16[0] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 31[7] -> 24[0] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 08/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 00/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 02/0 : 31[7] -> 24[0] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 02/0 : 15[7] -> 8[0] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 00/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 15[7] -> 8[0] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 04/0 : 15[7] -> 8[0] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 02/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 15[7] -> 8[0] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 08/0 : 10[2] -> 9[1] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 06/0 : 15[7] -> 8[0] via P2P/IPC
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 10[2] -> 9[1] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 00/0 : 28[4] -> 27[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 23[7] -> 16[0] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 39[7] -> 32[0] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 02/0 : 47[7] -> 40[0] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 20[4] -> 19[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 15[7] -> 8[0] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 01/0 : 20[4] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 18[2] -> 17[1] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 28[4] -> 27[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 02/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 31[7] -> 24[0] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 03/0 : 39[7] -> 32[0] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 03/0 : 47[7] -> 40[0] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 03/0 : 23[7] -> 16[0] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 23[7] -> 16[0] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 20[4] -> 19[3] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 01/0 : 18[2] -> 17[1] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 36[4] -> 35[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 01/0 : 36[4] -> 35[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 04/0 : 47[7] -> 40[0] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 06/0 : 12[4] -> 11[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 15[7] -> 8[0] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 05/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 00/0 : 44[4] -> 43[3] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 00/0 : 42[2] -> 41[1] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 05/0 : 23[7] -> 16[0] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 28[4] -> 27[3] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 04/0 : 31[7] -> 24[0] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 39[7] -> 32[0] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 05/0 : 39[7] -> 32[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 01/0 : 44[4] -> 43[3] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 34[2] -> 33[1] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 01/0 : 42[2] -> 41[1] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 06/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 02/0 : 44[4] -> 43[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 23[7] -> 16[0] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 28[4] -> 27[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 07/0 : 23[7] -> 16[0] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 03/0 : 20[4] -> 19[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 39[7] -> 32[0] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 39[7] -> 32[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 01/0 : 34[2] -> 33[1] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 10/0 : 15[7] -> 8[0] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 12[4] -> 11[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 36[4] -> 35[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 09/0 : 39[7] -> 32[0] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 08/0 : 42[2] -> 41[1] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 31[7] -> 24[0] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 18[2] -> 17[1] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 39[7] -> 32[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 34[2] -> 33[1] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 03/0 : 44[4] -> 43[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 07/0 : 47[7] -> 40[0] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 06/0 : 28[4] -> 27[3] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 15[7] -> 8[0] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 08/0 : 26[2] -> 25[1] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 08/0 : 12[4] -> 11[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 09/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 05/0 : 44[4] -> 43[3] via P2P/IPC
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 26[2] -> 25[1] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 28[4] -> 27[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 05/0 : 20[4] -> 19[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 09/0 : 23[7] -> 16[0] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 03/0 : 36[4] -> 35[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 11/0 : 39[7] -> 32[0] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 05/0 : 36[4] -> 35[3] via P2P/IPC
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 09/0 : 42[2] -> 41[1] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 20[4] -> 19[3] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 08/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 06/0 : 31[7] -> 24[0] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 12/0 : 15[7] -> 8[0] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 28[4] -> 27[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 07/0 : 20[4] -> 19[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 39[7] -> 32[0] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 09/0 : 34[2] -> 33[1] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 12[4] -> 11[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 36[4] -> 35[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 13/0 : 39[7] -> 32[0] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 23[7] -> 16[0] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 11/0 : 23[7] -> 16[0] via P2P/IPC
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 09/0 : 18[2] -> 17[1] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 07/0 : 36[4] -> 35[3] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 31[7] -> 24[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 06/0 : 44[4] -> 43[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 10/0 : 47[7] -> 40[0] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 20[4] -> 19[3] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 07/0 : 44[4] -> 43[3] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 10/0 : 12[4] -> 11[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 09/0 : 20[4] -> 19[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 36[4] -> 35[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 11/0 : 47[7] -> 40[0] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 15[7] -> 8[0] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 23[7] -> 16[0] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 12/0 : 47[7] -> 40[0] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 10/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 31[7] -> 24[0] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 13/0 : 23[7] -> 16[0] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 39[7] -> 32[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 08/0 : 44[4] -> 43[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 12[4] -> 11[3] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 09/0 : 44[4] -> 43[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 20[4] -> 19[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 13/0 : 47[7] -> 40[0] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 10/0 : 31[7] -> 24[0] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 14/0 : 15[7] -> 8[0] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 11/0 : 20[4] -> 19[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 23[7] -> 16[0] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 15/0 : 39[7] -> 32[0] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 09/0 : 36[4] -> 35[3] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 08/0 : 11[3] -> 10[2] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 12[4] -> 11[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 36[4] -> 35[3] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 10/0 : 11[3] -> 10[2] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 13/0 : 20[4] -> 19[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 14/0 : 12[4] -> 11[3] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 15/0 : 23[7] -> 16[0] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 20[4] -> 19[3] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 19[3] -> 18[2] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 31[7] -> 24[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 10/0 : 44[4] -> 43[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 14/0 : 47[7] -> 40[0] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 15/0 : 15[7] -> 8[0] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 11/0 : 44[4] -> 43[3] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 15/0 : 47[7] -> 40[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 13/0 : 44[4] -> 43[3] via P2P/IPC
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 12[4] -> 11[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 14/0 : 28[4] -> 27[3] via P2P/IPC
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 15/0 : 20[4] -> 19[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 11/0 : 36[4] -> 35[3] via P2P/IPC
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 28[4] -> 27[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 12/0 : 31[7] -> 24[0] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 14/0 : 44[4] -> 43[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 13/0 : 36[4] -> 35[3] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 00/0 : 43[3] -> 42[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 19[3] -> 18[2] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 23[7] -> 22[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 31[7] -> 24[0] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 15/0 : 44[4] -> 43[3] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 36[4] -> 35[3] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 19[3] -> 18[2] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 23[7] -> 22[6] via P2P/IPC
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 15/0 : 36[4] -> 35[3] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 35[3] -> 34[2] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 02/0 : 43[3] -> 42[2] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 39[7] -> 38[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 14/0 : 31[7] -> 24[0] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 35[3] -> 34[2] via P2P/IPC
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 19[3] -> 18[2] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 23[7] -> 22[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 15/0 : 31[7] -> 24[0] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 08/0 : 43[3] -> 42[2] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 35[3] -> 34[2] via P2P/IPC
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 10/0 : 43[3] -> 42[2] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 39[7] -> 38[6] via P2P/IPC
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 23[7] -> 22[6] via P2P/IPC
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 00/0 : 15[7] -> 14[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 35[3] -> 34[2] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 39[7] -> 38[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 00/0 : 47[7] -> 46[6] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 08/0 : 27[3] -> 26[2] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 00/0 : 31[7] -> 30[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 10/0 : 27[3] -> 26[2] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 06/0 : 15[7] -> 14[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 06/0 : 47[7] -> 46[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 06/0 : 31[7] -> 30[6] via P2P/IPC
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 39[7] -> 38[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 08/0 : 31[7] -> 30[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 14/0 : 31[7] -> 30[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 08/0 : 15[7] -> 14[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 08/0 : 47[7] -> 46[6] via P2P/IPC
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 14/0 : 15[7] -> 14[6] via P2P/IPC
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 14/0 : 47[7] -> 46[6] via P2P/IPC
 2: nodeai01:2157662:2158993 [2] NCCL INFO Connected all trees
 2: nodeai01:2157662:2158993 [2] NCCL INFO NVLS comm 0x556a1af44420 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
42: nodeai07:2435730:2437071 [2] NCCL INFO Connected all trees
42: nodeai07:2435730:2437071 [2] NCCL INFO NVLS comm 0x55c42d9c3da0 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
45: nodeai07:2435732:2437073 [5] NCCL INFO Connected all trees
45: nodeai07:2435732:2437073 [5] NCCL INFO NVLS comm 0x5631562fcc80 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
47: nodeai07:2435736:2437074 [7] NCCL INFO Connected all trees
47: nodeai07:2435736:2437074 [7] NCCL INFO NVLS comm 0x564e1fa936e0 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
43: nodeai07:2435733:2437069 [3] NCCL INFO Connected all trees
43: nodeai07:2435733:2437069 [3] NCCL INFO NVLS comm 0x559b9d1419d0 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
44: nodeai07:2435729:2437075 [4] NCCL INFO Connected all trees
44: nodeai07:2435729:2437075 [4] NCCL INFO NVLS comm 0x564b202ace00 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 7: nodeai01:2157664:2158996 [7] NCCL INFO Connected all trees
 7: nodeai01:2157664:2158996 [7] NCCL INFO NVLS comm 0x561beab44a60 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
10: nodeai02:2459393:2460732 [2] NCCL INFO Connected all trees
10: nodeai02:2459393:2460732 [2] NCCL INFO NVLS comm 0x556f37931c10 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
41: nodeai07:2435735:2437070 [1] NCCL INFO Connected all trees
41: nodeai07:2435735:2437070 [1] NCCL INFO NVLS comm 0x55cf54665010 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 5: nodeai01:2157665:2158998 [5] NCCL INFO Connected all trees
 5: nodeai01:2157665:2158998 [5] NCCL INFO NVLS comm 0x55fa00ec5d80 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
40: nodeai07:2435734:2437072 [0] NCCL INFO Connected all trees
40: nodeai07:2435734:2437072 [0] NCCL INFO NVLS comm 0x558204ec9940 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 3: nodeai01:2157667:2158995 [3] NCCL INFO Connected all trees
 4: nodeai01:2157666:2158999 [4] NCCL INFO Connected all trees
 3: nodeai01:2157667:2158995 [3] NCCL INFO NVLS comm 0x5571537b1d60 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 4: nodeai01:2157666:2158999 [4] NCCL INFO NVLS comm 0x556b880e1cd0 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
46: nodeai07:2435731:2437068 [6] NCCL INFO Connected all trees
46: nodeai07:2435731:2437068 [6] NCCL INFO NVLS comm 0x56162eb0a340 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
13: nodeai02:2459392:2460734 [5] NCCL INFO Connected all trees
13: nodeai02:2459392:2460734 [5] NCCL INFO NVLS comm 0x55d986a07340 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
11: nodeai02:2459390:2460737 [3] NCCL INFO Connected all trees
11: nodeai02:2459390:2460737 [3] NCCL INFO NVLS comm 0x564a9a895710 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
15: nodeai02:2459391:2460730 [7] NCCL INFO Connected all trees
15: nodeai02:2459391:2460730 [7] NCCL INFO NVLS comm 0x5619f3f26260 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
12: nodeai02:2459394:2460735 [4] NCCL INFO Connected all trees
 9: nodeai02:2459396:2460733 [1] NCCL INFO Connected all trees
12: nodeai02:2459394:2460735 [4] NCCL INFO NVLS comm 0x55d806ccfd60 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 9: nodeai02:2459396:2460733 [1] NCCL INFO NVLS comm 0x559b0650a9e0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 1: nodeai01:2157663:2158994 [1] NCCL INFO Connected all trees
 1: nodeai01:2157663:2158994 [1] NCCL INFO NVLS comm 0x55cb58d46d40 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 8: nodeai02:2459395:2460731 [0] NCCL INFO Connected all trees
 8: nodeai02:2459395:2460731 [0] NCCL INFO NVLS comm 0x55adf1660d10 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
14: nodeai02:2459389:2460736 [6] NCCL INFO Connected all trees
14: nodeai02:2459389:2460736 [6] NCCL INFO NVLS comm 0x55ec379eac50 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
21: nodeai04:2417508:2418843 [5] NCCL INFO Connected all trees
21: nodeai04:2417508:2418843 [5] NCCL INFO NVLS comm 0x55875ce00800 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 0: nodeai01:2157660:2158992 [0] NCCL INFO Connected all trees
 0: nodeai01:2157660:2158992 [0] NCCL INFO NVLS comm 0x560914aceb20 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 6: nodeai01:2157661:2158997 [6] NCCL INFO Connected all trees
 6: nodeai01:2157661:2158997 [6] NCCL INFO NVLS comm 0x55aa517dae10 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
29: nodeai05:1971444:1972784 [5] NCCL INFO Connected all trees
29: nodeai05:1971444:1972784 [5] NCCL INFO NVLS comm 0x56523b615850 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
18: nodeai04:2417512:2418846 [2] NCCL INFO Connected all trees
18: nodeai04:2417512:2418846 [2] NCCL INFO NVLS comm 0x564c4f4d8a90 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
37: nodeai06:2504432:2505773 [5] NCCL INFO Connected all trees
37: nodeai06:2504432:2505773 [5] NCCL INFO NVLS comm 0x5622d3c16a90 headRank 5 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
19: nodeai04:2417510:2418842 [3] NCCL INFO Connected all trees
20: nodeai04:2417506:2418845 [4] NCCL INFO Connected all trees
19: nodeai04:2417510:2418842 [3] NCCL INFO NVLS comm 0x55e304e57250 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
20: nodeai04:2417506:2418845 [4] NCCL INFO NVLS comm 0x5615b36b3720 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
22: nodeai04:2417511:2418849 [6] NCCL INFO Connected all trees
22: nodeai04:2417511:2418849 [6] NCCL INFO NVLS comm 0x560f72ba9d00 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
17: nodeai04:2417509:2418848 [1] NCCL INFO Connected all trees
17: nodeai04:2417509:2418848 [1] NCCL INFO NVLS comm 0x55c6a8266ad0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
16: nodeai04:2417513:2418847 [0] NCCL INFO Connected all trees
16: nodeai04:2417513:2418847 [0] NCCL INFO NVLS comm 0x55ea13cd0fe0 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
23: nodeai04:2417507:2418844 [7] NCCL INFO Connected all trees
23: nodeai04:2417507:2418844 [7] NCCL INFO NVLS comm 0x55a375e18400 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 02/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 00/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 02/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 00/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 00/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 02/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 00/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 02/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 00/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 02/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 00/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 00/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 02/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 04/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 04/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 04/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 04/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 06/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 04/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 02/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 04/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 06/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 06/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 06/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 06/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 08/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 10/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 08/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 10/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 08/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 10/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 04/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 06/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 08/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 10/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 12/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 08/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 10/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 08/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 10/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 12/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 06/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 08/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 10/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 12/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 12/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 12/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 08/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 12/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 14/0 : 36[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 14/0 : 38[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 12/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 14/0 : 33[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 10/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 14/0 : 32[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 14/0 : 34[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 14/0 : 37[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 14/0 : 39[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 12/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 14/0 : 35[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Connected all trees
28: nodeai05:1971442:1972778 [4] NCCL INFO NVLS comm 0x560ddfffe120 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
36: nodeai06:2504434:2505776 [4] NCCL INFO Connected all trees
25: nodeai05:1971445:1972781 [1] NCCL INFO Connected all trees
25: nodeai05:1971445:1972781 [1] NCCL INFO NVLS comm 0x55c3e3d22ea0 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
36: nodeai06:2504434:2505776 [4] NCCL INFO NVLS comm 0x5619adfbc960 headRank 4 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
33: nodeai06:2504433:2505777 [1] NCCL INFO Connected all trees
33: nodeai06:2504433:2505777 [1] NCCL INFO NVLS comm 0x5601eaf7ad90 headRank 1 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
34: nodeai06:2504435:2505772 [2] NCCL INFO Connected all trees
34: nodeai06:2504435:2505772 [2] NCCL INFO NVLS comm 0x5630f98b3f60 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
27: nodeai05:1971446:1972779 [3] NCCL INFO Connected all trees
27: nodeai05:1971446:1972779 [3] NCCL INFO NVLS comm 0x55bd9e04b590 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
35: nodeai06:2504431:2505770 [3] NCCL INFO Connected all trees
35: nodeai06:2504431:2505770 [3] NCCL INFO NVLS comm 0x55cbe3564bd0 headRank 3 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 00/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 02/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 00/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 02/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 04/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 00/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 02/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 04/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 06/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 04/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 00/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 08/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 04/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 06/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 00/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 02/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 10/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 06/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 00/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 10/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 08/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 02/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 06/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 08/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 12/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 02/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 12/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 04/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 08/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 14[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 00/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 10/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 04/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 14/0 : 8[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 12/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 06/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 10/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 02/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 12/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 06/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 14/0 : 10[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 08/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 14/0 : 13[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 04/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 08/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 10/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 14/0 : 12[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 06/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 10/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 12/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 08/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 12/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 14/0 : 11[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 10/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 14/0 : 15[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 12/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 14/0 : 9[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Connected all trees
39: nodeai06:2504430:2505775 [7] NCCL INFO NVLS comm 0x5654f9aa7e90 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
24: nodeai05:1971447:1972777 [0] NCCL INFO Connected all trees
26: nodeai05:1971443:1972782 [2] NCCL INFO Connected all trees
24: nodeai05:1971447:1972777 [0] NCCL INFO NVLS comm 0x5643f4facd80 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
26: nodeai05:1971443:1972782 [2] NCCL INFO NVLS comm 0x55be69282420 headRank 2 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
31: nodeai05:1971449:1972783 [7] NCCL INFO Connected all trees
31: nodeai05:1971449:1972783 [7] NCCL INFO NVLS comm 0x55a0200c3560 headRank 7 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
32: nodeai06:2504436:2505771 [0] NCCL INFO Connected all trees
32: nodeai06:2504436:2505771 [0] NCCL INFO NVLS comm 0x56050ce94190 headRank 0 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
30: nodeai05:1971448:1972780 [6] NCCL INFO Connected all trees
30: nodeai05:1971448:1972780 [6] NCCL INFO NVLS comm 0x557542160f50 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 01/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 01/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 03/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 01/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 03/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 01/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 05/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 03/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 03/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 01/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 01/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 05/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 05/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 07/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 05/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 03/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 03/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 07/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 07/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 09/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 07/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 05/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 05/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 09/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 09/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 11/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 09/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 07/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 07/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 11/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 15/0 : 5[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 11/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 11/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 09/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 09/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 13/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 13/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 01/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 11/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 13/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 11/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 15/0 : 4[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 15/0 : 6[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 03/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 13/0 : 7[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 15/0 : 0[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 13/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 05/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 15/0 : 2[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 07/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 00/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 11/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 00/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 02/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 02/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 13/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 00/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 04/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 02/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 00/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 04/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 15/0 : 1[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 00/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 02/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 06/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 06/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 02/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 06/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 04/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 04/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 08/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 04/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 08/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 08/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 08/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 06/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 10/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 06/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 10/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 10/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 10/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 08/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 12/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 12/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 08/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 12/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 00/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 12/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 10/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 14/0 : 37[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 10/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 14/0 : 36[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 14/0 : 32[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 02/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 14/0 : 38[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 12/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 12/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 04/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 06/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 14/0 : 34[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 14/0 : 39[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 08/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 10/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 12/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 14/0 : 33[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 03/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 05/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 07/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 09/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 13/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 15/0 : 3[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 00/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 02/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 04/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 06/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 08/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 10/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 12/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 14/0 : 35[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Connected all trees
38: nodeai06:2504437:2505774 [6] NCCL INFO NVLS comm 0x55f04b4a0f10 headRank 6 nHeads 8 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 1610612736
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 06/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 02/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 04/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 10[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 14/0 : 13[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 8[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 10/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 12/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 01/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 11[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 01/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 14[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 15[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 9[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 03/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 05/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 02/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 05/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 01/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 04/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 03/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 01/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 01/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 03/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 06/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 07/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 05/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 03/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 03/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 07/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 07/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 12[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 05/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 09/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 05/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 05/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 09/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 07/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 09/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 01/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 07/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 11/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 07/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 11/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 09/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 11/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 13/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 03/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 09/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 09/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 13/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 10/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 05/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 12/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 14/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 11/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 11/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 11/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 13/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 15/0 : 18[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 15/0 : 21[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 15/0 : 16[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 07/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 13/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 13/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 15/0 : 19[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 13/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 23[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 09/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 15/0 : 22[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 15/0 : 17[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 11/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 13/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 15/0 : 20[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 03/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 45[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 11/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 43[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 47[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 40[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 07/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 42[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 03/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 13[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 41[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 15/0 : 46[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 44[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 15[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 11/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 11[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 8[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 07/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 10[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 9[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 15/0 : 14[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 12[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 02/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 00/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 02/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 00/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 00/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 00/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 02/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 02/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 04/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 00/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 04/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 04/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 00/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 04/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 00/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 04/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 06/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 02/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 06/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 06/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 07/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 04/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 02/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 02/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 06/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 08/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 10/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 08/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 06/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 04/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 08/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 08/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 10/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 11/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 10/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 06/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 06/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 10/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 12/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 08/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 12/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 12/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 12/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 08/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 08/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 12/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 14/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 10/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 14/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 14/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 18[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 15/0 : 22[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 17[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 12/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 16[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 10/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 10/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 14/0 : 23[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 03/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 14/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 12/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 19[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 14/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 14/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 07/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 21[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 20[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 11/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 25[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 26[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 24[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 15/0 : 30[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 31[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 27[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 29[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 28[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 02/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 18[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 04/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 10/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 22[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 17[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 06/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 16[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 12/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 23[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 19[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 20[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 14/0 : 21[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 01/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 03/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 01/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 05/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 03/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 01/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 01/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 07/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 05/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 01/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 03/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 09/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 05/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 05/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 03/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 09/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 11/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 07/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 03/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 07/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 01/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 05/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 09/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 13/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 09/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 11/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 03/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 07/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 13/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 11/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 15/0 : 26[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 07/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 13/0 : 31[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 05/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 11/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 15/0 : 27[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 13/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 09/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 07/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 13/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 15/0 : 30[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 11/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 09/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 15/0 : 25[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 15/0 : 29[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 04/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 11/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 13/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 02/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 15/0 : 28[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 34[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 12/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 06/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 39[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 35[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 38[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 10/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 33[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 14/0 : 37[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 36[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 03/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 07/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 11/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 02/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 04/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 10[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 15[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 15/0 : 14[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 11[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 9[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 18[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 13[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 01/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 03/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 05/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 01/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 03/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 01/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 03/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 22[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 07/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 05/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 01/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 01/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 05/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 23[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 06/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 10/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 12/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 03/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 05/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 09/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 11/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 13/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 01/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 03/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 05/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 03/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 05/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 19[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 07/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 09/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 11/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 07/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 09/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 11/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 17[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 07/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 01/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 07/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 15/0 : 42[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 07/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 03/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 13/0 : 47[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 11/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 09/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 13/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 09/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 05/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 13/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 11/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 15/0 : 46[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 07/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 13/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 14/0 : 21[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 15/0 : 41[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 02/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 15/0 : 45[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 20[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 09/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 04/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 34[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 15/0 : 43[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 03/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 11/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 38[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 13/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 12[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 11/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 07/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 39[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 15/0 : 44[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 10/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 06/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 12/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 26[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 31[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 01/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 25[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 35[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 33[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 03/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 27[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 15/0 : 30[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 29[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 01/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 01/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 01/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 05/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 36[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 14/0 : 37[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 03/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 03/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 03/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 01/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 07/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 05/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 01/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 05/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 05/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 03/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 09/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 01/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 07/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 07/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 01/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 07/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 03/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 05/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 11/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 03/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 09/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 09/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 11/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 05/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 07/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 38[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 05/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 13/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 34[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 04/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 13/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 11/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 02/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 09/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 11/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 07/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 07/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 15/0 : 10[2] -> 42[2] [receive] via NET/IBext/2/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 13/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 02/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 15/0 : 9[1] -> 41[1] [receive] via NET/IBext/1/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 13/0 : 15[7] -> 47[7] [receive] via NET/IBext/7/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 09/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 09/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 11/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 07/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 15/0 : 12[4] -> 44[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 11/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 13/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 13/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 00/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 04/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 06/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 6[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 28[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 00/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 02/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 04/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 08/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 10/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 00/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 02/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 00/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 15/0 : 13[5] -> 45[5] [receive] via NET/IBext/5/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 12/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 04/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 04/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 15/0 : 14[6] -> 46[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 08/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 15/0 : 11[3] -> 43[3] [receive] via NET/IBext/3/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 02/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 39[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 14/0 : 6[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 06/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 10/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 12/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 04/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 2[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 08/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 10/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 00/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 06/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 12/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 10/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 35[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 00/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 06/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 10/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 02/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 14/0 : 2[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 06/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 02/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 12/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 33[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 04/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 7[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 08/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 00/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 06/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 14/0 : 7[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 06/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 10/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 02/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 08/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 08/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 12/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 10/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 04/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 1[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 10/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 14/0 : 5[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 38[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 12/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 14/0 : 5[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 06/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 12/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 12/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 14/0 : 4[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 08/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 14/0 : 37[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 02/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 36[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 3[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 14/0 : 1[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 4[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 10/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 29[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 15/0 : 30[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 28[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 25[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 12/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 34[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 07/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 06/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 14/0 : 3[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 13[5] -> 45[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 9[1] -> 41[1] [send] via NET/IBext/1/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 15/0 : 14[6] -> 46[6] [send] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 01/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 12[4] -> 44[4] [send] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 03/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 27[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 10/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 04/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 39[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 00/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 00/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 00/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 01/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 33[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 02/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 00/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 02/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 00/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 00/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 11/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 00/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 04/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 00/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 14/0 : 37[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 02/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 12/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 04/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 36[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 04/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 04/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 35[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 02/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 08/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 01/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 03/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 06/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 00/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 06/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 06/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 06/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 10/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 02/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 02/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 00/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 08/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 08/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 02/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 12/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 02/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 08/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 04/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 04/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 08/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 6: nodeai01:2157661:2158997 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 00/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 02/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 00/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 10/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 06/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 04/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 7: nodeai01:2157664:2158996 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [receive] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 26[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 10/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 06/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 10/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 10/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 05/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
 2: nodeai01:2157662:2158993 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [receive] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 12/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 08/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 14/0 : 46[6] -> 38[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 12/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 00/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 12/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 02/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 05/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 04/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 01/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 01/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 05/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
45: nodeai07:2435732:2437073 [5] NCCL INFO Channel 14/0 : 45[5] -> 37[5] [send] via NET/IBext/5/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 12/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 01/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 10/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 06/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 07/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
41: nodeai07:2435735:2437070 [1] NCCL INFO Channel 14/0 : 41[1] -> 33[1] [send] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 06/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 02/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 03/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 06/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 02/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 33[1] -> 25[1] [receive] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
44: nodeai07:2435729:2437075 [4] NCCL INFO Channel 14/0 : 44[4] -> 36[4] [send] via NET/IBext/4/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 14/0 : 42[2] -> 34[2] [receive] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 02/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 37[5] -> 29[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 08/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 03/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 12/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 08/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 03/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 08/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 08/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 07/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 07/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 04/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 08/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 00/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 03/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
46: nodeai07:2435731:2437068 [6] NCCL INFO Channel 14/0 : 46[6] -> 38[6] [send] via NET/IBext/6/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 05/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 00/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 00/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 04/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 09/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 10/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 10/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 03/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 01/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 10/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 11[3] -> 43[3] [send] via NET/IBext/3/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 02/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 05/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 01/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 10/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 36[4] -> 28[4] [receive] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 07/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 10/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 06/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 08/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 02/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 12/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 03/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 05/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 00/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 09/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 00/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 03/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 05/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 12/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 04/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 12/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 01/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 01/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
 5: nodeai01:2157665:2158998 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 02/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 10/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 15/0 : 38[6] -> 30[6] [receive] via NET/IBext/6/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
 1: nodeai01:2157663:2158994 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 03/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 14/0 : 21[5] -> 13[5] [receive] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 12/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 01/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 11/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 03/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 11/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 07/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
 4: nodeai01:2157666:2158999 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [receive] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 09/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 05/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 12/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 01/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 08/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 02/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 07/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 07/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 03/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 04/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 14/0 : 44[4] -> 36[4] [receive] via NET/IBext/4/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 11/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 14/0 : 47[7] -> 39[7] [receive] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 05/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 10/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
 3: nodeai01:2157667:2158995 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 06/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 04/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 09/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 01/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 04/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 09/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 13/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 05/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 13/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 02/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 13/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 01/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 11/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 00/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 07/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 06/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 09/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 12/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 06/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 03/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 03/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 04/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 06/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 01/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 07/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 15/0 : 30[6] -> 22[6] [receive] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 07/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 05/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 10/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 02/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 08/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 06/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 08/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 11/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 06/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 05/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 03/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 00/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 09/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 15/0 : 26[2] -> 18[2] [receive] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 11/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 10/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 11/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 07/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 07/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 04/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 02/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
13: nodeai02:2459392:2460734 [5] NCCL INFO Channel 15/0 : 13[5] -> 5[5] [send] via NET/IBext/5/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 10/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 11/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 08/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 00/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 05/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 13/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 11/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 12/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 12/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 14/0 : 17[1] -> 9[1] [receive] via NET/IBext/1/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 09/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 07/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 14/0 : 22[6] -> 14[6] [receive] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 01/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 04/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 01/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 13/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 03/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 07/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 11/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 04/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 09/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 13/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 08/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 31[7] -> 23[7] [receive] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 08/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
38: nodeai06:2504437:2505774 [6] NCCL INFO Channel 15/0 : 38[6] -> 30[6] [send] via NET/IBext/6/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 03/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 09/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 05/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 14/0 : 41[1] -> 33[1] [receive] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 10/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 06/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 05/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 15/0 : 28[4] -> 20[4] [receive] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 08/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 11/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 07/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 07/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 10/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 01/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 00/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 12/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 10/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 09/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 13/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 08/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 09/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 00/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 09/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 02/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 03/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 11/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 14/0 : 45[5] -> 37[5] [receive] via NET/IBext/5/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 12/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 12/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 15/0 : 25[1] -> 17[1] [receive] via NET/IBext/1/GDRDMA
34: nodeai06:2504435:2505772 [2] NCCL INFO Channel 15/0 : 34[2] -> 26[2] [send] via NET/IBext/2/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 11/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 11/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 05/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 01/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 14/0 : 43[3] -> 35[3] [receive] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 13/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
39: nodeai06:2504430:2505775 [7] NCCL INFO Channel 13/0 : 39[7] -> 31[7] [send] via NET/IBext/7/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 07/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 03/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 01/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
36: nodeai06:2504434:2505776 [4] NCCL INFO Channel 15/0 : 36[4] -> 28[4] [send] via NET/IBext/4/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 11/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 05/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 00/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 03/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 13/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 12/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 13/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 02/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 10/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 07/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 08/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 05/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 14/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 11/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 09/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 14/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
29: nodeai05:1971444:1972784 [5] NCCL INFO Channel 15/0 : 29[5] -> 21[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 13/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
25: nodeai05:1971445:1972781 [1] NCCL INFO Channel 15/0 : 25[1] -> 17[1] [send] via NET/IBext/1/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 10/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 09/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 14/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 11/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
28: nodeai05:1971442:1972778 [4] NCCL INFO Channel 15/0 : 28[4] -> 20[4] [send] via NET/IBext/4/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 12/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 03/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 04/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
33: nodeai06:2504433:2505777 [1] NCCL INFO Channel 15/0 : 33[1] -> 25[1] [send] via NET/IBext/1/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 07/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 13/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 03/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
30: nodeai05:1971448:1972780 [6] NCCL INFO Channel 15/0 : 30[6] -> 22[6] [send] via NET/IBext/6/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 11/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 05/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 14/0 : 20[4] -> 12[4] [receive] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 05/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 06/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 07/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 10[2] -> 42[2] [send] via NET/IBext/2/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 09/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 01/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 07/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 00/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 11/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 03/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 09/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 02/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
37: nodeai06:2504432:2505773 [5] NCCL INFO Channel 15/0 : 37[5] -> 29[5] [send] via NET/IBext/5/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 13/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 05/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 00/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 11/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
 9: nodeai02:2459396:2460733 [1] NCCL INFO Channel 15/0 : 9[1] -> 1[1] [send] via NET/IBext/1/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 04/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 07/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 13/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 08/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 13/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 10/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 04/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 02/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 04/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 00/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 10/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
22: nodeai04:2417511:2418849 [6] NCCL INFO Channel 14/0 : 22[6] -> 14[6] [send] via NET/IBext/6/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 06/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
35: nodeai06:2504431:2505770 [3] NCCL INFO Channel 15/0 : 35[3] -> 27[3] [send] via NET/IBext/3/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 12/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 12/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 06/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 14/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 02/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 08/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
18: nodeai04:2417512:2418846 [2] NCCL INFO Channel 14/0 : 18[2] -> 10[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 13/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 08/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 15/0 : 29[5] -> 21[5] [receive] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 04/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 10/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 06/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 10/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 00/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 06/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 12/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 12/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 15/0 : 27[3] -> 19[3] [receive] via NET/IBext/3/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 12/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 02/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
20: nodeai04:2417506:2418845 [4] NCCL INFO Channel 14/0 : 20[4] -> 12[4] [send] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 06/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 08/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 09/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
23: nodeai04:2417507:2418844 [7] NCCL INFO Channel 14/0 : 23[7] -> 15[7] [send] via NET/IBext/7/GDRDMA
14: nodeai02:2459389:2460736 [6] NCCL INFO Channel 15/0 : 14[6] -> 6[6] [send] via NET/IBext/6/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 00/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 00/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 04/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 11/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 10/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 04/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 06/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 08/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 02/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 13/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 12/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 06/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 08/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 08/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
12: nodeai02:2459394:2460735 [4] NCCL INFO Channel 15/0 : 12[4] -> 4[4] [send] via NET/IBext/4/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 10/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 03/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
43: nodeai07:2435733:2437069 [3] NCCL INFO Channel 14/0 : 43[3] -> 35[3] [send] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 04/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 10/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
17: nodeai04:2417509:2418848 [1] NCCL INFO Channel 14/0 : 17[1] -> 9[1] [send] via NET/IBext/1/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 10/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 35[3] -> 27[3] [receive] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 06/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 00/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 11/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 01/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 02/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 12/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 12/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 08/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
21: nodeai04:2417508:2418843 [5] NCCL INFO Channel 14/0 : 21[5] -> 13[5] [send] via NET/IBext/5/GDRDMA
42: nodeai07:2435730:2437071 [2] NCCL INFO Channel 14/0 : 42[2] -> 34[2] [send] via NET/IBext/2/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 10/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 12/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
19: nodeai04:2417510:2418842 [3] NCCL INFO Channel 14/0 : 19[3] -> 11[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 08/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 12/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 10/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 14/0 : 19[3] -> 11[3] [receive] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 12/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 14/0 : 18[2] -> 10[2] [receive] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 03/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 01/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 05/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 03/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 07/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 05/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 09/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 07/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 13/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 09/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 11/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
11: nodeai02:2459390:2460737 [3] NCCL INFO Channel 15/0 : 11[3] -> 3[3] [send] via NET/IBext/3/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 13/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
10: nodeai02:2459393:2460732 [2] NCCL INFO Channel 15/0 : 10[2] -> 2[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 04/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 34[2] -> 26[2] [receive] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 05/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 00/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 06/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 01/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 07/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 08/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 04/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 09/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 05/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 10/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 06/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 12/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 07/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 13/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 08/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 14/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 09/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
27: nodeai05:1971446:1972779 [3] NCCL INFO Channel 15/0 : 27[3] -> 19[3] [send] via NET/IBext/3/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 11/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 12/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 13/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 14/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
26: nodeai05:1971443:1972782 [2] NCCL INFO Channel 15/0 : 26[2] -> 18[2] [send] via NET/IBext/2/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 03/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 05/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 07/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 09/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 11/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 13/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 15/0 : 24[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 32[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 01/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 03/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 05/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 07/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 09/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 8[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 11/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 16[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 13/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 15/0 : 40[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 01/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 03/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 05/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 32[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 07/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 09/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 24[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 11/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 13/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 02/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 15/0 : 8[0] -> 40[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 24[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 04/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 06/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 08/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 10/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 12/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 14/0 : 0[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 32[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 0[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 8[0] -> 40[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 32[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 02/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 02/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 04/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 04/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 06/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 02/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 06/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 08/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 04/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 08/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 10/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 10/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 06/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 03/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 32[0] -> 24[0] [receive] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 12/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 08/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 12/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Channel 14/0 : 40[0] -> 32[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 10/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 14/0 : 16[0] -> 8[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 05/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 02/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 12/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 01/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 03/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 14/0 : 40[0] -> 32[0] [receive] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 04/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 07/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 0: nodeai01:2157660:2158992 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 03/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 01/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 05/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 09/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 05/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 03/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 06/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 07/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 31[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 05/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 07/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 11/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 07/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 09/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 09/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 10/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 13/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 11/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 11/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 09/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 13/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 12/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 11/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 13/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
 8: nodeai02:2459395:2460731 [0] NCCL INFO Channel 15/0 : 8[0] -> 0[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 15/0 : 24[0] -> 16[0] [receive] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
32: nodeai06:2504436:2505771 [0] NCCL INFO Channel 15/0 : 32[0] -> 24[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 13/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 02/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 15[7] -> 47[7] [send] via NET/IBext/7/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 14/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 04/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
24: nodeai05:1971447:1972777 [0] NCCL INFO Channel 15/0 : 24[0] -> 16[0] [send] via NET/IBext/0/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 06/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 08/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 00/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 10/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 02/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 12/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 04/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 07/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
16: nodeai04:2417513:2418847 [0] NCCL INFO Channel 14/0 : 16[0] -> 8[0] [send] via NET/IBext/0/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 00/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 06/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 02/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 39[7] -> 31[7] [receive] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 08/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 04/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 00/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 06/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 10/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 01/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 08/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 12/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 02/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 10/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 14/0 : 23[7] -> 15[7] [receive] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 03/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 12/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 01/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 04/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 03/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
47: nodeai07:2435736:2437074 [7] NCCL INFO Channel 14/0 : 47[7] -> 39[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 05/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 05/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 06/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 07/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 08/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 09/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 09/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 11/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 10/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
15: nodeai02:2459391:2460730 [7] NCCL INFO Channel 13/0 : 15[7] -> 7[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 11/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 12/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 13/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
31: nodeai05:1971449:1972783 [7] NCCL INFO Channel 14/0 : 31[7] -> 23[7] [send] via NET/IBext/7/GDRDMA
40: nodeai07:2435734:2437072 [0] NCCL INFO Connected NVLS tree
40: nodeai07:2435734:2437072 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
40: nodeai07:2435734:2437072 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
45: nodeai07:2435732:2437073 [5] NCCL INFO Connected NVLS tree
45: nodeai07:2435732:2437073 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
45: nodeai07:2435732:2437073 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
47: nodeai07:2435736:2437074 [7] NCCL INFO Connected NVLS tree
47: nodeai07:2435736:2437074 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
47: nodeai07:2435736:2437074 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
43: nodeai07:2435733:2437069 [3] NCCL INFO Connected NVLS tree
43: nodeai07:2435733:2437069 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
43: nodeai07:2435733:2437069 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
41: nodeai07:2435735:2437070 [1] NCCL INFO Connected NVLS tree
41: nodeai07:2435735:2437070 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
41: nodeai07:2435735:2437070 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
42: nodeai07:2435730:2437071 [2] NCCL INFO Connected NVLS tree
42: nodeai07:2435730:2437071 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
42: nodeai07:2435730:2437071 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
44: nodeai07:2435729:2437075 [4] NCCL INFO Connected NVLS tree
44: nodeai07:2435729:2437075 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
44: nodeai07:2435729:2437075 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
46: nodeai07:2435731:2437068 [6] NCCL INFO Connected NVLS tree
46: nodeai07:2435731:2437068 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
46: nodeai07:2435731:2437068 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
40: nodeai07:2435734:2437072 [0] NCCL INFO comm 0x558204ec9940 rank 40 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init COMPLETE
41: nodeai07:2435735:2437070 [1] NCCL INFO comm 0x55cf54665010 rank 41 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init COMPLETE
42: nodeai07:2435730:2437071 [2] NCCL INFO comm 0x55c42d9c3da0 rank 42 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init COMPLETE
43: nodeai07:2435733:2437069 [3] NCCL INFO comm 0x559b9d1419d0 rank 43 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init COMPLETE
44: nodeai07:2435729:2437075 [4] NCCL INFO comm 0x564b202ace00 rank 44 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init COMPLETE
45: nodeai07:2435732:2437073 [5] NCCL INFO comm 0x5631562fcc80 rank 45 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init COMPLETE
46: nodeai07:2435731:2437068 [6] NCCL INFO comm 0x56162eb0a340 rank 46 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init COMPLETE
47: nodeai07:2435736:2437074 [7] NCCL INFO comm 0x564e1fa936e0 rank 47 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init COMPLETE
 7: nodeai01:2157664:2158996 [7] NCCL INFO Connected NVLS tree
 7: nodeai01:2157664:2158996 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 7: nodeai01:2157664:2158996 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 3: nodeai01:2157667:2158995 [3] NCCL INFO Connected NVLS tree
 3: nodeai01:2157667:2158995 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 3: nodeai01:2157667:2158995 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 5: nodeai01:2157665:2158998 [5] NCCL INFO Connected NVLS tree
 5: nodeai01:2157665:2158998 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 5: nodeai01:2157665:2158998 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 1: nodeai01:2157663:2158994 [1] NCCL INFO Connected NVLS tree
 1: nodeai01:2157663:2158994 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 1: nodeai01:2157663:2158994 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 4: nodeai01:2157666:2158999 [4] NCCL INFO Connected NVLS tree
 4: nodeai01:2157666:2158999 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 4: nodeai01:2157666:2158999 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
13: nodeai02:2459392:2460734 [5] NCCL INFO Connected NVLS tree
13: nodeai02:2459392:2460734 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
13: nodeai02:2459392:2460734 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 2: nodeai01:2157662:2158993 [2] NCCL INFO Connected NVLS tree
 2: nodeai01:2157662:2158993 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 2: nodeai01:2157662:2158993 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
38: nodeai06:2504437:2505774 [6] NCCL INFO Connected NVLS tree
38: nodeai06:2504437:2505774 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
38: nodeai06:2504437:2505774 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
10: nodeai02:2459393:2460732 [2] NCCL INFO Connected NVLS tree
10: nodeai02:2459393:2460732 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
10: nodeai02:2459393:2460732 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
12: nodeai02:2459394:2460735 [4] NCCL INFO Connected NVLS tree
12: nodeai02:2459394:2460735 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
12: nodeai02:2459394:2460735 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
36: nodeai06:2504434:2505776 [4] NCCL INFO Connected NVLS tree
36: nodeai06:2504434:2505776 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
36: nodeai06:2504434:2505776 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
27: nodeai05:1971446:1972779 [3] NCCL INFO Connected NVLS tree
27: nodeai05:1971446:1972779 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
27: nodeai05:1971446:1972779 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
28: nodeai05:1971442:1972778 [4] NCCL INFO Connected NVLS tree
28: nodeai05:1971442:1972778 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
28: nodeai05:1971442:1972778 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
14: nodeai02:2459389:2460736 [6] NCCL INFO Connected NVLS tree
14: nodeai02:2459389:2460736 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
14: nodeai02:2459389:2460736 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
35: nodeai06:2504431:2505770 [3] NCCL INFO Connected NVLS tree
35: nodeai06:2504431:2505770 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
35: nodeai06:2504431:2505770 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 8: nodeai02:2459395:2460731 [0] NCCL INFO Connected NVLS tree
 8: nodeai02:2459395:2460731 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 8: nodeai02:2459395:2460731 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
30: nodeai05:1971448:1972780 [6] NCCL INFO Connected NVLS tree
30: nodeai05:1971448:1972780 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
30: nodeai05:1971448:1972780 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 6: nodeai01:2157661:2158997 [6] NCCL INFO Connected NVLS tree
 6: nodeai01:2157661:2158997 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 6: nodeai01:2157661:2158997 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
32: nodeai06:2504436:2505771 [0] NCCL INFO Connected NVLS tree
32: nodeai06:2504436:2505771 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
32: nodeai06:2504436:2505771 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 0: nodeai01:2157660:2158992 [0] NCCL INFO Connected NVLS tree
 0: nodeai01:2157660:2158992 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 0: nodeai01:2157660:2158992 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
21: nodeai04:2417508:2418843 [5] NCCL INFO Connected NVLS tree
21: nodeai04:2417508:2418843 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
21: nodeai04:2417508:2418843 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
22: nodeai04:2417511:2418849 [6] NCCL INFO Connected NVLS tree
22: nodeai04:2417511:2418849 [6] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
22: nodeai04:2417511:2418849 [6] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
20: nodeai04:2417506:2418845 [4] NCCL INFO Connected NVLS tree
20: nodeai04:2417506:2418845 [4] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
20: nodeai04:2417506:2418845 [4] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
26: nodeai05:1971443:1972782 [2] NCCL INFO Connected NVLS tree
26: nodeai05:1971443:1972782 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
26: nodeai05:1971443:1972782 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
34: nodeai06:2504435:2505772 [2] NCCL INFO Connected NVLS tree
34: nodeai06:2504435:2505772 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
34: nodeai06:2504435:2505772 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
25: nodeai05:1971445:1972781 [1] NCCL INFO Connected NVLS tree
25: nodeai05:1971445:1972781 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
25: nodeai05:1971445:1972781 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
33: nodeai06:2504433:2505777 [1] NCCL INFO Connected NVLS tree
33: nodeai06:2504433:2505777 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
33: nodeai06:2504433:2505777 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
18: nodeai04:2417512:2418846 [2] NCCL INFO Connected NVLS tree
18: nodeai04:2417512:2418846 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
18: nodeai04:2417512:2418846 [2] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 0: nodeai01:2157660:2158992 [0] NCCL INFO comm 0x560914aceb20 rank 0 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init COMPLETE
31: nodeai05:1971449:1972783 [7] NCCL INFO Connected NVLS tree
31: nodeai05:1971449:1972783 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
31: nodeai05:1971449:1972783 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 1: nodeai01:2157663:2158994 [1] NCCL INFO comm 0x55cb58d46d40 rank 1 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init COMPLETE
 2: nodeai01:2157662:2158993 [2] NCCL INFO comm 0x556a1af44420 rank 2 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init COMPLETE
 3: nodeai01:2157667:2158995 [3] NCCL INFO comm 0x5571537b1d60 rank 3 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init COMPLETE
 4: nodeai01:2157666:2158999 [4] NCCL INFO comm 0x556b880e1cd0 rank 4 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init COMPLETE
 5: nodeai01:2157665:2158998 [5] NCCL INFO comm 0x55fa00ec5d80 rank 5 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init COMPLETE
 6: nodeai01:2157661:2158997 [6] NCCL INFO comm 0x55aa517dae10 rank 6 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init COMPLETE
 7: nodeai01:2157664:2158996 [7] NCCL INFO comm 0x561beab44a60 rank 7 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init COMPLETE
11: nodeai02:2459390:2460737 [3] NCCL INFO Connected NVLS tree
11: nodeai02:2459390:2460737 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
11: nodeai02:2459390:2460737 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
16: nodeai04:2417513:2418847 [0] NCCL INFO Connected NVLS tree
16: nodeai04:2417513:2418847 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
16: nodeai04:2417513:2418847 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
29: nodeai05:1971444:1972784 [5] NCCL INFO Connected NVLS tree
29: nodeai05:1971444:1972784 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
29: nodeai05:1971444:1972784 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
37: nodeai06:2504432:2505773 [5] NCCL INFO Connected NVLS tree
37: nodeai06:2504432:2505773 [5] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
37: nodeai06:2504432:2505773 [5] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
24: nodeai05:1971447:1972777 [0] NCCL INFO Connected NVLS tree
24: nodeai05:1971447:1972777 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
24: nodeai05:1971447:1972777 [0] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
39: nodeai06:2504430:2505775 [7] NCCL INFO Connected NVLS tree
39: nodeai06:2504430:2505775 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
39: nodeai06:2504430:2505775 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
19: nodeai04:2417510:2418842 [3] NCCL INFO Connected NVLS tree
19: nodeai04:2417510:2418842 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
19: nodeai04:2417510:2418842 [3] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 9: nodeai02:2459396:2460733 [1] NCCL INFO Connected NVLS tree
 9: nodeai02:2459396:2460733 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
 9: nodeai02:2459396:2460733 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
15: nodeai02:2459391:2460730 [7] NCCL INFO Connected NVLS tree
15: nodeai02:2459391:2460730 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
15: nodeai02:2459391:2460730 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
32: nodeai06:2504436:2505771 [0] NCCL INFO comm 0x56050ce94190 rank 32 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init COMPLETE
33: nodeai06:2504433:2505777 [1] NCCL INFO comm 0x5601eaf7ad90 rank 33 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init COMPLETE
34: nodeai06:2504435:2505772 [2] NCCL INFO comm 0x5630f98b3f60 rank 34 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init COMPLETE
35: nodeai06:2504431:2505770 [3] NCCL INFO comm 0x55cbe3564bd0 rank 35 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init COMPLETE
36: nodeai06:2504434:2505776 [4] NCCL INFO comm 0x5619adfbc960 rank 36 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init COMPLETE
37: nodeai06:2504432:2505773 [5] NCCL INFO comm 0x5622d3c16a90 rank 37 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init COMPLETE
38: nodeai06:2504437:2505774 [6] NCCL INFO comm 0x55f04b4a0f10 rank 38 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init COMPLETE
39: nodeai06:2504430:2505775 [7] NCCL INFO comm 0x5654f9aa7e90 rank 39 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init COMPLETE
24: nodeai05:1971447:1972777 [0] NCCL INFO comm 0x5643f4facd80 rank 24 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init COMPLETE
26: nodeai05:1971443:1972782 [2] NCCL INFO comm 0x55be69282420 rank 26 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init COMPLETE
28: nodeai05:1971442:1972778 [4] NCCL INFO comm 0x560ddfffe120 rank 28 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init COMPLETE
29: nodeai05:1971444:1972784 [5] NCCL INFO comm 0x56523b615850 rank 29 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init COMPLETE
30: nodeai05:1971448:1972780 [6] NCCL INFO comm 0x557542160f50 rank 30 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init COMPLETE
25: nodeai05:1971445:1972781 [1] NCCL INFO comm 0x55c3e3d22ea0 rank 25 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init COMPLETE
27: nodeai05:1971446:1972779 [3] NCCL INFO comm 0x55bd9e04b590 rank 27 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init COMPLETE
31: nodeai05:1971449:1972783 [7] NCCL INFO comm 0x55a0200c3560 rank 31 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init COMPLETE
17: nodeai04:2417509:2418848 [1] NCCL INFO Connected NVLS tree
17: nodeai04:2417509:2418848 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
17: nodeai04:2417509:2418848 [1] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
23: nodeai04:2417507:2418844 [7] NCCL INFO Connected NVLS tree
23: nodeai04:2417507:2418844 [7] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 512 | 512
23: nodeai04:2417507:2418844 [7] NCCL INFO 16 coll channels, 16 nvls channels, 16 p2p channels, 2 p2p channels per peer
 8: nodeai02:2459395:2460731 [0] NCCL INFO comm 0x55adf1660d10 rank 8 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init COMPLETE
 9: nodeai02:2459396:2460733 [1] NCCL INFO comm 0x559b0650a9e0 rank 9 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init COMPLETE
10: nodeai02:2459393:2460732 [2] NCCL INFO comm 0x556f37931c10 rank 10 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init COMPLETE
11: nodeai02:2459390:2460737 [3] NCCL INFO comm 0x564a9a895710 rank 11 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init COMPLETE
12: nodeai02:2459394:2460735 [4] NCCL INFO comm 0x55d806ccfd60 rank 12 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init COMPLETE
13: nodeai02:2459392:2460734 [5] NCCL INFO comm 0x55d986a07340 rank 13 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init COMPLETE
14: nodeai02:2459389:2460736 [6] NCCL INFO comm 0x55ec379eac50 rank 14 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init COMPLETE
15: nodeai02:2459391:2460730 [7] NCCL INFO comm 0x5619f3f26260 rank 15 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init COMPLETE
 1: Torch distributed is available.
 1: Torch distributed is initialized.
 0: Torch distributed is available.
 0: Torch distributed is initialized.
 3: Torch distributed is available.
 3: Torch distributed is initialized.
 2: Torch distributed is available.
 2: Torch distributed is initialized.
 6: Torch distributed is available.
 6: Torch distributed is initialized.
 5: Torch distributed is available.
 5: Torch distributed is initialized.
 7: Torch distributed is available.
 7: Torch distributed is initialized.
 4: Torch distributed is available.
 4: Torch distributed is initialized.
16: nodeai04:2417513:2418847 [0] NCCL INFO comm 0x55ea13cd0fe0 rank 16 nranks 48 cudaDev 0 nvmlDev 0 busId 19000 commId 0x588b065b9d7bba39 - Init COMPLETE
19: nodeai04:2417510:2418842 [3] NCCL INFO comm 0x55e304e57250 rank 19 nranks 48 cudaDev 3 nvmlDev 3 busId 5d000 commId 0x588b065b9d7bba39 - Init COMPLETE
23: nodeai04:2417507:2418844 [7] NCCL INFO comm 0x55a375e18400 rank 23 nranks 48 cudaDev 7 nvmlDev 7 busId db000 commId 0x588b065b9d7bba39 - Init COMPLETE
17: nodeai04:2417509:2418848 [1] NCCL INFO comm 0x55c6a8266ad0 rank 17 nranks 48 cudaDev 1 nvmlDev 1 busId 3b000 commId 0x588b065b9d7bba39 - Init COMPLETE
18: nodeai04:2417512:2418846 [2] NCCL INFO comm 0x564c4f4d8a90 rank 18 nranks 48 cudaDev 2 nvmlDev 2 busId 4c000 commId 0x588b065b9d7bba39 - Init COMPLETE
20: nodeai04:2417506:2418845 [4] NCCL INFO comm 0x5615b36b3720 rank 20 nranks 48 cudaDev 4 nvmlDev 4 busId 9b000 commId 0x588b065b9d7bba39 - Init COMPLETE
21: nodeai04:2417508:2418843 [5] NCCL INFO comm 0x55875ce00800 rank 21 nranks 48 cudaDev 5 nvmlDev 5 busId bb000 commId 0x588b065b9d7bba39 - Init COMPLETE
22: nodeai04:2417511:2418849 [6] NCCL INFO comm 0x560f72ba9d00 rank 22 nranks 48 cudaDev 6 nvmlDev 6 busId cb000 commId 0x588b065b9d7bba39 - Init COMPLETE
 8: Torch distributed is available.
 8: Torch distributed is initialized.
11: Torch distributed is available.
11: Torch distributed is initialized.
 9: Torch distributed is available.
 9: Torch distributed is initialized.
10: Torch distributed is available.
10: Torch distributed is initialized.
12: Torch distributed is available.
12: Torch distributed is initialized.
13: Torch distributed is available.
13: Torch distributed is initialized.
14: Torch distributed is available.
14: Torch distributed is initialized.
15: Torch distributed is available.
15: Torch distributed is initialized.
32: Torch distributed is available.
32: Torch distributed is initialized.
33: Torch distributed is available.
33: Torch distributed is initialized.
34: Torch distributed is available.
34: Torch distributed is initialized.
35: Torch distributed is available.
35: Torch distributed is initialized.
36: Torch distributed is available.
36: Torch distributed is initialized.
37: Torch distributed is available.
37: Torch distributed is initialized.
38: Torch distributed is available.
38: Torch distributed is initialized.
39: Torch distributed is available.
39: Torch distributed is initialized.
18: Torch distributed is available.
18: Torch distributed is initialized.
16: Torch distributed is available.
16: Torch distributed is initialized.
19: Torch distributed is available.
17: Torch distributed is available.
19: Torch distributed is initialized.
17: Torch distributed is initialized.
20: Torch distributed is available.
20: Torch distributed is initialized.
21: Torch distributed is available.
21: Torch distributed is initialized.
22: Torch distributed is available.
22: Torch distributed is initialized.
23: Torch distributed is available.
23: Torch distributed is initialized.
41: Torch distributed is available.
41: Torch distributed is initialized.
40: Torch distributed is available.
40: Torch distributed is initialized.
42: Torch distributed is available.
42: Torch distributed is initialized.
43: Torch distributed is available.
43: Torch distributed is initialized.
44: Torch distributed is available.
44: Torch distributed is initialized.
45: Torch distributed is available.
45: Torch distributed is initialized.
46: Torch distributed is available.
46: Torch distributed is initialized.
47: Torch distributed is available.
47: Torch distributed is initialized.
27: Torch distributed is available.
27: Torch distributed is initialized.
24: Torch distributed is available.
24: Torch distributed is initialized.
25: Torch distributed is available.
25: Torch distributed is initialized.
26: Torch distributed is available.
26: Torch distributed is initialized.
28: Torch distributed is available.
28: Torch distributed is initialized.
29: Torch distributed is available.
29: Torch distributed is initialized.
30: Torch distributed is available.
30: Torch distributed is initialized.
31: Torch distributed is available.
31: Torch distributed is initialized.
 5: Enabling make_graphed_callables for encoder!!
13: Enabling make_graphed_callables for encoder!!
 7: Enabling make_graphed_callables for encoder!!
 3: Enabling make_graphed_callables for encoder!!
15: Enabling make_graphed_callables for encoder!!
 6: Enabling make_graphed_callables for encoder!!
 0: Enabling make_graphed_callables for encoder!!
10: Enabling make_graphed_callables for encoder!!
 1: Enabling make_graphed_callables for encoder!!
 4: Enabling make_graphed_callables for encoder!!
 8: Enabling make_graphed_callables for encoder!!
 9: Enabling make_graphed_callables for encoder!!
14: Enabling make_graphed_callables for encoder!!
37: Enabling make_graphed_callables for encoder!!
11: Enabling make_graphed_callables for encoder!!
12: Enabling make_graphed_callables for encoder!!
33: Enabling make_graphed_callables for encoder!!
32: Enabling make_graphed_callables for encoder!!
 2: Enabling make_graphed_callables for encoder!!
35: Enabling make_graphed_callables for encoder!!
38: Enabling make_graphed_callables for encoder!!
21: Enabling make_graphed_callables for encoder!!
22: Enabling make_graphed_callables for encoder!!
47: Enabling make_graphed_callables for encoder!!
45: Enabling make_graphed_callables for encoder!!
19: Enabling make_graphed_callables for encoder!!
34: Enabling make_graphed_callables for encoder!!
18: Enabling make_graphed_callables for encoder!!
44: Enabling make_graphed_callables for encoder!!
36: Enabling make_graphed_callables for encoder!!
46: Enabling make_graphed_callables for encoder!!
43: Enabling make_graphed_callables for encoder!!
39: Enabling make_graphed_callables for encoder!!
23: Enabling make_graphed_callables for encoder!!
17: Enabling make_graphed_callables for encoder!!
40: Enabling make_graphed_callables for encoder!!
25: Enabling make_graphed_callables for encoder!!
24: Enabling make_graphed_callables for encoder!!
29: Enabling make_graphed_callables for encoder!!
26: Enabling make_graphed_callables for encoder!!
16: Enabling make_graphed_callables for encoder!!
42: Enabling make_graphed_callables for encoder!!
41: Enabling make_graphed_callables for encoder!!
28: Enabling make_graphed_callables for encoder!!
20: Enabling make_graphed_callables for encoder!!
31: Enabling make_graphed_callables for encoder!!
27: Enabling make_graphed_callables for encoder!!
30: Enabling make_graphed_callables for encoder!!
 7: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 7:   out = jit_dropout_add(x, residual, prob)
 5: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 5:   out = jit_dropout_add(x, residual, prob)
 0: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 0:   out = jit_dropout_add(x, residual, prob)
 1: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 1:   out = jit_dropout_add(x, residual, prob)
 3: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 3:   out = jit_dropout_add(x, residual, prob)
 2: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 2:   out = jit_dropout_add(x, residual, prob)
 4: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 4:   out = jit_dropout_add(x, residual, prob)
 6: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 6:   out = jit_dropout_add(x, residual, prob)
26: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
26:   out = jit_dropout_add(x, residual, prob)
24: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
24:   out = jit_dropout_add(x, residual, prob)
27: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
27:   out = jit_dropout_add(x, residual, prob)
25: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
25:   out = jit_dropout_add(x, residual, prob)
31: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
31:   out = jit_dropout_add(x, residual, prob)
29: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
29:   out = jit_dropout_add(x, residual, prob)
 8: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 8:   out = jit_dropout_add(x, residual, prob)
15: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
15:   out = jit_dropout_add(x, residual, prob)
11: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
11:   out = jit_dropout_add(x, residual, prob)
14: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
14:   out = jit_dropout_add(x, residual, prob)
12: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
12:   out = jit_dropout_add(x, residual, prob)
13: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
13:   out = jit_dropout_add(x, residual, prob)
28: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
28:   out = jit_dropout_add(x, residual, prob)
 9: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
 9:   out = jit_dropout_add(x, residual, prob)
10: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
10:   out = jit_dropout_add(x, residual, prob)
30: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
30:   out = jit_dropout_add(x, residual, prob)
18: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
18:   out = jit_dropout_add(x, residual, prob)
21: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
21:   out = jit_dropout_add(x, residual, prob)
22: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
22:   out = jit_dropout_add(x, residual, prob)
16: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
16:   out = jit_dropout_add(x, residual, prob)
17: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
17:   out = jit_dropout_add(x, residual, prob)
19: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
19:   out = jit_dropout_add(x, residual, prob)
23: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
23:   out = jit_dropout_add(x, residual, prob)
20: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
20:   out = jit_dropout_add(x, residual, prob)
41: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
41:   out = jit_dropout_add(x, residual, prob)
42: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
42:   out = jit_dropout_add(x, residual, prob)
40: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
40:   out = jit_dropout_add(x, residual, prob)
46: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
46:   out = jit_dropout_add(x, residual, prob)
44: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
44:   out = jit_dropout_add(x, residual, prob)
45: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
45:   out = jit_dropout_add(x, residual, prob)
47: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
47:   out = jit_dropout_add(x, residual, prob)
43: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
43:   out = jit_dropout_add(x, residual, prob)
35: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
35:   out = jit_dropout_add(x, residual, prob)
34: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
34:   out = jit_dropout_add(x, residual, prob)
37: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
37:   out = jit_dropout_add(x, residual, prob)
32: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
32:   out = jit_dropout_add(x, residual, prob)
36: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
36:   out = jit_dropout_add(x, residual, prob)
39: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
39:   out = jit_dropout_add(x, residual, prob)
38: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
38:   out = jit_dropout_add(x, residual, prob)
33: /workspace/bert/modeling.py:206: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)
33:   out = jit_dropout_add(x, residual, prob)
24: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
24:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
25:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
30:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
31:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
27:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
28:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
26:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
29:   warnings.warn(
40: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
40:   warnings.warn(
42: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
42:   warnings.warn(
43: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
43:   warnings.warn(
41: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
41:   warnings.warn(
44: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
44:   warnings.warn(
45: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
45:   warnings.warn(
46: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
46:   warnings.warn(
47: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
47:   warnings.warn(
18: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
18:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
16:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
17:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
19:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
20:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
22:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
23:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
21:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
32: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
32:   warnings.warn(
36: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
36:   warnings.warn(
38: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
38:   warnings.warn(
34: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
34:   warnings.warn(
37: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
37:   warnings.warn(
39: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
39:   warnings.warn(
35: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
35:   warnings.warn(
33: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
33:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
 0: :::MLLOG {"namespace": "", "time_ms": 1723852728757, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1679}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852728758, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1679}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852728781, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1697, "epoch_num": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852728783, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1699, "first_epoch_num": 1, "epoch_count": 1}}
 0: parsed args:
 0: Namespace(input_dir='/workspace/data_phase2', packed_samples=True, order_samples=False, max_pack_factor=3, average_packing_rate=2, synthetic_input=False, bert_model='bert-large-uncased', cuda_graph_mode='segmented', max_iterations_per_graph=4, output_dir='/results', eval_dir='/workspace/evaldata', eval_iter_start_samples=200000, eval_iter_samples=200000, num_eval_examples=10000, cache_eval_data=True, load_eval_synchronously=False, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, max_seq_length=512, max_predictions_per_seq=76, train_batch_size=72, eval_batch_size=16, learning_rate=0.00258, weight_decay_rate=0.1, opt_lamb_beta_1=0.6, opt_lamb_beta_2=0.7, max_steps=700.0, sustained_training_time=0, max_samples_termination=4500000.0, warmup_proportion=0.0, warmup_steps=200330.0, start_warmup_step=-200000.0, local_rank=0, seed=25011, gradient_accumulation_steps=1, fp16=True, loss_scale=0.0, log_freq=0.0, checkpoint_activations=False, resume_from_checkpoint=False, keep_n_most_recent
 0: _checkpoints=20, num_samples_per_checkpoint=500000, min_samples_to_start_checkpoints=3000000, skip_checkpoint=True, phase2=True, allreduce_post_accumulation=False, allreduce_post_accumulation_fp16=False, do_train=True, exchange_padding=False, unpad=False, unpad_fmha=False, pad_fmha=True, pad=False, enable_fuse_dropout=False, disable_fuse_mask=False, disable_fuse_scale=False, disable_fuse_qkv=False, disable_apex_softmax=False, enable_stream=False, fused_gemm_gelu=True, fused_mha=False, fused_gelu_bias=False, fused_dropout_add=True, fused_bias_mha=True, fused_bias_fc=True, fused_bias_fc_loss_head=False, dense_seq_output=True, use_env=False, bert_config_path='/workspace/phase1/bert_config.json', target_mlm_accuracy=0.72, train_mlm_accuracy_window_size=0, num_epochs_to_generate_seeds_for=2, use_cuda_graph=True, use_ddp=False, ddp_type='apex', use_gradient_as_bucket_view=False, bypass_amp=False, distributed_lamb=True, dwu_group_size=0, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_num_ar_pg=1, dwu_num_a
 0: g_pg=1, dwu_overlap_reductions=False, dwu_e5m2_allgather=False, use_transformer_engine2=True, n_gpu=48, device=device(type='cuda', index=0), resume_step=0)
 0: epoch: 1
 0: :::MLLOG {"namespace": "", "time_ms": 1723852728783, "event_type": "POINT_IN_TIME", "key": "data_file", "value": "/workspace/data_phase2/part_03277.hdf5", "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1734}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852734429, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 35438.90936092815}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 200072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852734429, "event_type": "INTERVAL_START", "key": "eval_start", "value": 200072, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 200072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852736085, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 200072, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 200072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852736086, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3582659065723419, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 200072}}
 0: {'global_steps': 29, 'eval_loss': 4.246779918670654, 'eval_mlm_accuracy': 0.3582659065723419}
18: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
18:   warnings.warn(
45: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
45:   warnings.warn(
36: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
36:   warnings.warn(
21: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
21:   warnings.warn(
35: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
35:   warnings.warn(
22: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
22:   warnings.warn(
37: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
37:   warnings.warn(
46: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
46:   warnings.warn(
20: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
20:   warnings.warn(
41: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
41:   warnings.warn(
24: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
24:   warnings.warn(
38: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
38:   warnings.warn(
31: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
31:   warnings.warn(
25: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
25:   warnings.warn(
 5: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 5:   warnings.warn(
 6: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 6:   warnings.warn(
 1: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 1:   warnings.warn(
 2: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 2:   warnings.warn(
28: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
28:   warnings.warn(
44: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
44:   warnings.warn(
 3: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 3:   warnings.warn(
 9: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 9:   warnings.warn(
10: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
10:   warnings.warn(
 8: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 8:   warnings.warn(
12: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
12:   warnings.warn(
19: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
19:   warnings.warn(
32: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
32:   warnings.warn(
40: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
40:   warnings.warn(
 4: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 4:   warnings.warn(
16: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
16:   warnings.warn(
33: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
33:   warnings.warn(
13: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
13:   warnings.warn(
14: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
14:   warnings.warn(
43: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
43:   warnings.warn(
11: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
11:   warnings.warn(
34: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
34:   warnings.warn(
23: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
23:   warnings.warn(
26: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
26:   warnings.warn(
15: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
15:   warnings.warn(
42: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
42:   warnings.warn(
47: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
47:   warnings.warn(
30: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
30:   warnings.warn(
27: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
27:   warnings.warn(
17: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
17:   warnings.warn(
39: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
39:   warnings.warn(
 7: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 7:   warnings.warn(
 0: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
 0:   warnings.warn(
29: /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:378: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
29:   warnings.warn(
 0: :::MLLOG {"namespace": "", "time_ms": 1723852739995, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 36017.50213326321}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 400535}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852739995, "event_type": "INTERVAL_START", "key": "eval_start", "value": 400535, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 400535}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852740199, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 400535, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 400535}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852740199, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.36787745356559753, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 400535}}
 0: {'global_steps': 58, 'eval_loss': 4.181979656219482, 'eval_mlm_accuracy': 0.36787745356559753}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852744110, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48539.136818467035}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 600291}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852744110, "event_type": "INTERVAL_START", "key": "eval_start", "value": 600291, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 600291}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852744317, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 600291, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 600291}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852744317, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3769005239009857, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 600291}}
 0: {'global_steps': 87, 'eval_loss': 4.130434513092041, 'eval_mlm_accuracy': 0.3769005239009857}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852748239, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48506.28818734666}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 800560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852748239, "event_type": "INTERVAL_START", "key": "eval_start", "value": 800560, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 800560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852748460, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 800560, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 800560}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852748460, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.3855873644351959, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 800560}}
 0: {'global_steps': 116, 'eval_loss': 3.9966750144958496, 'eval_mlm_accuracy': 0.3855873644351959}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852752383, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48281.09492727965}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 1000654}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852752391, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1000654, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 1000654}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852752590, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1000654, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 1000654}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852752590, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4077480733394623, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 1000654}}
 0: {'global_steps': 145, 'eval_loss': 3.7914466857910156, 'eval_mlm_accuracy': 0.4077480733394623}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852756510, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48459.159314318196}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 1200647}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852756510, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1200647, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 1200647}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852756714, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1200647, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 1200647}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852756714, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.43793943524360657, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 1200647}}
 0: {'global_steps': 174, 'eval_loss': 3.501830816268921, 'eval_mlm_accuracy': 0.43793943524360657}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852760632, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48515.01163518871}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 1400629}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852760632, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1400629, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 1400629}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852760836, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1400629, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 1400629}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852760836, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.5269116163253784, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 1400629}}
 0: {'global_steps': 203, 'eval_loss': 2.7237062454223633, 'eval_mlm_accuracy': 0.5269116163253784}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852764763, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48430.31815493559}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 1600689}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852764763, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1600689, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 1600689}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852764967, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1600689, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 1600689}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852764967, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6540754437446594, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 1600689}}
 0: {'global_steps': 232, 'eval_loss': 1.7572568655014038, 'eval_mlm_accuracy': 0.6540754437446594}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852769648, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 41011.72554117337}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 1801044}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852769649, "event_type": "INTERVAL_START", "key": "eval_start", "value": 1801044, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 1801044}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852769856, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 1801044, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 1801044}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852769856, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.697920560836792, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 1801044}}
 0: {'global_steps': 261, 'eval_loss': 1.44447922706604, 'eval_mlm_accuracy': 0.697920560836792}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852773781, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48397.96425772023}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 2001061}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852773781, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2001061, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 2001061}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852773985, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2001061, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 2001061}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852773986, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7062057256698608, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 2001061}}
 0: {'global_steps': 290, 'eval_loss': 1.3938894271850586, 'eval_mlm_accuracy': 0.7062057256698608}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852777910, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48607.508144319}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 2201752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852777910, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2201752, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 2201752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852778116, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2201752, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 2201752}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852778116, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7086225748062134, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 2201752}}
 0: {'global_steps': 319, 'eval_loss': 1.3782951831817627, 'eval_mlm_accuracy': 0.7086225748062134}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852782046, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48367.46458393001}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 2401824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852782047, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2401824, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 2401824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852782252, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2401824, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 2401824}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852782252, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7140588760375977, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 2401824}}
 0: {'global_steps': 348, 'eval_loss': 1.3407127857208252, 'eval_mlm_accuracy': 0.7140588760375977}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852786183, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48476.818088187894}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 2602370}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852786183, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2602370, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 2602370}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852786385, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2602370, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 2602370}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852786385, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7156000733375549, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 2602370}}
 0: {'global_steps': 377, 'eval_loss': 1.3317317962646484, 'eval_mlm_accuracy': 0.7156000733375549}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852790319, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48573.80744864488}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 2803256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852790319, "event_type": "INTERVAL_START", "key": "eval_start", "value": 2803256, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 2803256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852790522, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 2803256, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 2803256}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852790523, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.716347336769104, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 2803256}}
 0: {'global_steps': 406, 'eval_loss': 1.3253462314605713, 'eval_mlm_accuracy': 0.716347336769104}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852794454, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48426.055721251345}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 3003520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852794455, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3003520, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 3003520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852794659, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3003520, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 3003520}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852794659, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7175335884094238, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 3003520}}
 0: {'global_steps': 435, 'eval_loss': 1.3192429542541504, 'eval_mlm_accuracy': 0.7175335884094238}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852798454, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48372.15857629309}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 3197008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852798455, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3197008, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 3197008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852798658, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3197008, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 3197008}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852798658, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7182177901268005, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 3197008}}
 0: {'global_steps': 463, 'eval_loss': 1.314746618270874, 'eval_mlm_accuracy': 0.7182177901268005}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852802593, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48442.17299829525}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 3397505}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852802594, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3397505, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 3397505}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852802797, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3397505, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 3397505}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852802797, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7189487218856812, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 3397505}}
 0: {'global_steps': 492, 'eval_loss': 1.31033456325531, 'eval_mlm_accuracy': 0.7189487218856812}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852807521, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 40730.61580406892}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 3598192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852807521, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3598192, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 3598192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852807729, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3598192, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 3598192}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852807729, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7197309732437134, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 3598192}}
 0: {'global_steps': 521, 'eval_loss': 1.3069921731948853, 'eval_mlm_accuracy': 0.7197309732437134}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852811665, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48396.71548270754}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 3798789}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852811666, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3798789, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 3798789}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852811872, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3798789, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 3798789}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852811872, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7199201583862305, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 3798789}}
 0: {'global_steps': 550, 'eval_loss': 1.3029621839523315, 'eval_mlm_accuracy': 0.7199201583862305}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852815807, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 48297.10484260283}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1908, "epoch_num": 3998818}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852815814, "event_type": "INTERVAL_START", "key": "eval_start", "value": 3998818, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1914, "epoch_num": 3998818}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816012, "event_type": "INTERVAL_END", "key": "eval_stop", "value": 3998818, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1920, "epoch_num": 3998818}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816012, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7207444310188293, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1924, "epoch_num": 3998818}}
 0: {'global_steps': 579, 'eval_loss': 1.300707459449768, 'eval_mlm_accuracy': 0.7207444310188293}
 0: 0.720744 > 0.720000, Target MLM Accuracy reached at 579
 0: Training runs 1.6946819742520651 mins sustained_training_time 0
 0: (1, 579.0) {'final_loss': 0.0}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816013, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2043, "first_epoch_num": 1}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816013, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2046, "epoch_num": 3998818}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816013, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3998818, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2048}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816013, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2051}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816014, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2054, "status": "success"}}
 0: :::MLLOG {"namespace": "", "time_ms": 1723852816015, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 45828.91310138794, "epoch_num": 3998818}, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 2093, "step": [2, 579]}}
 0: {'e2e_time': 141.90511798858643, 'training_sequences_per_second': 47584.142670620284, 'final_loss': 0.0, 'raw_train_time': 101.68093252182007}
 4: +--------------------------------------------------------------------------+------------+
 4: |                                 Modules                                  | Parameters |
 4: +--------------------------------------------------------------------------+------------+
 4: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 4: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 4: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 4: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 4: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 4: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 4: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 4: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 4: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 4: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 4: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 4: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 4: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 4: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 4: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 4: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 4: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 4: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 4: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 4: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 4: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 4: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 4: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 4: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 4: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 4: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 4: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 4: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 4: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 4: +--------------------------------------------------------------------------+------------+
 4: Total Trainable Params: 336232258
37: +--------------------------------------------------------------------------+------------+
37: |                                 Modules                                  | Parameters |
37: +--------------------------------------------------------------------------+------------+
37: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
37: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
37: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
37: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
37: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
37: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
37: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
37: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
37: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
37: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
37: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
37: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
37: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
37: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
37: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
37: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
37: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
37: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
37: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
37: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
37: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
37: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
37: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
37: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
37: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
37: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
37: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
37: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
37: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
37: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
37: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
37: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
37: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
37: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
37: +--------------------------------------------------------------------------+------------+
37: Total Trainable Params: 336232258
 0: +--------------------------------------------------------------------------+------------+
 0: |                                 Modules                                  | Parameters |
 0: +--------------------------------------------------------------------------+------------+
 0: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 0: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 0: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 0: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 0: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
12: +--------------------------------------------------------------------------+------------+
12: |                                 Modules                                  | Parameters |
12: +--------------------------------------------------------------------------+------------+
12: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
12: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
12: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
12: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
12: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
12: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 7: +--------------------------------------------------------------------------+------------+
 7: |                                 Modules                                  | Parameters |
 7: +--------------------------------------------------------------------------+------------+
 7: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 7: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 7: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 7: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 7: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
12: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
12: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
12: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
12: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
12: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
12: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
12: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
12: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
12: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
12: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 7: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 7: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
12: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
45: +--------------------------------------------------------------------------+------------+
45: |                                 Modules                                  | Parameters |
45: +--------------------------------------------------------------------------+------------+
45: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
45: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
45: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
45: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
45: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
45: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
12: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
12: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
12: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
12: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
12: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
12: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
12: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
12: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
12: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 7: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
12: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
12: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
12: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
12: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
12: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
12: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
12: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
12: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
12: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
12: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
12: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
12: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
12: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
12: +--------------------------------------------------------------------------+------------+
12: Total Trainable Params: 336232258
45: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 0: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
45: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
45: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
45: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 0: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
45: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
45: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 7: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
45: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 7: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
45: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
45: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
45: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
45: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
45: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
45: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
45: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
45: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 0: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 0: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 0: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
45: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 0: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
45: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
45: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 2: +--------------------------------------------------------------------------+------------+
 2: |                                 Modules                                  | Parameters |
 2: +--------------------------------------------------------------------------+------------+
 2: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 2: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 2: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 2: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 2: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
45: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
45: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
45: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
45: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
45: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
45: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 7: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 7: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 7: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
45: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
45: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 7: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
45: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
45: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
45: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
45: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
45: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
45: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
45: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
45: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
45: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
45: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
45: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
45: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
45: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
45: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
45: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
45: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
45: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
45: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
45: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
45: +--------------------------------------------------------------------------+------------+
45: Total Trainable Params: 336232258
 0: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 2: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
44: +--------------------------------------------------------------------------+------------+
44: |                                 Modules                                  | Parameters |
44: +--------------------------------------------------------------------------+------------+
44: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
44: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
44: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
44: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
44: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
44: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 2: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
36: +--------------------------------------------------------------------------+------------+
36: |                                 Modules                                  | Parameters |
36: +--------------------------------------------------------------------------+------------+
36: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
36: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
36: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
36: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
36: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
36: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 7: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 7: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
44: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
11: +--------------------------------------------------------------------------+------------+
11: |                                 Modules                                  | Parameters |
11: +--------------------------------------------------------------------------+------------+
11: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
11: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
11: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
11: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
11: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
11: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 2: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
44: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 2: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
44: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 2: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 2: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 2: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
23: +--------------------------------------------------------------------------+------------+
23: |                                 Modules                                  | Parameters |
23: +--------------------------------------------------------------------------+------------+
23: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
23: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
23: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
23: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
23: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
23: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 5: +--------------------------------------------------------------------------+------------+
 5: |                                 Modules                                  | Parameters |
 5: +--------------------------------------------------------------------------+------------+
 5: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 5: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 5: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 5: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 5: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
36: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
44: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 5: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
44: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 5: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
44: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 5: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 7: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
44: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
44: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
44: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 0: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
44: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
44: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
44: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 0: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
36: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
23: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
44: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
44: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
44: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
44: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
44: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
44: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
23: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
44: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 2: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
44: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
23: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
44: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 5: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
44: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
44: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
11: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 5: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
11: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
11: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 7: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
11: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 7: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 0: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 0: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
11: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
11: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
11: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
11: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
11: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 0: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 0: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 0: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 0: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 0: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
11: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
11: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
11: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 0: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 0: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 0: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 0: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 0: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 0: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 0: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 0: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 0: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 0: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 0: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
11: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 0: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 0: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 0: +--------------------------------------------------------------------------+------------+
 0: Total Trainable Params: 336232258
11: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
41: +--------------------------------------------------------------------------+------------+
41: |                                 Modules                                  | Parameters |
41: +--------------------------------------------------------------------------+------------+
41: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
41: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
41: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
41: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
41: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
41: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 2: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
11: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
41: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
41: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 2: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
11: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
41: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 2: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
36: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
11: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
23: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
41: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
11: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
23: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
41: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 2: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
11: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
23: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
44: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
11: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
23: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
44: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
11: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
23: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
44: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 5: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 5: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 5: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
11: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
23: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
23: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
23: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
23: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
23: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 5: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 5: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
11: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
23: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
23: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
23: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
44: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
11: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
23: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
41: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 7: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
11: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
23: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
41: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 7: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 7: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 7: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 7: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 7: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
11: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
23: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
41: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
41: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 7: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 7: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 7: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 7: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 7: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 7: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 7: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 7: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 7: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 7: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 7: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
11: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
23: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
41: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
41: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 7: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 7: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 7: +--------------------------------------------------------------------------+------------+
 7: Total Trainable Params: 336232258
11: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
23: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
41: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 2: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
11: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
23: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
44: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
44: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 2: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
11: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
11: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
11: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
11: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
11: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
11: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
11: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
11: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
23: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
44: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
44: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
44: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
44: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
44: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
44: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
44: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
44: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 2: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
11: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
11: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
11: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
11: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
11: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
11: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
11: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
11: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
11: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
11: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
11: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
23: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
44: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
44: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
44: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
44: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
44: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
44: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
44: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
44: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
44: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
44: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
44: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
11: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
11: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
11: +--------------------------------------------------------------------------+------------+
11: Total Trainable Params: 336232258
23: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
44: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
44: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
44: +--------------------------------------------------------------------------+------------+
44: Total Trainable Params: 336232258
 2: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
23: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
41: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
23: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
41: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
41: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
23: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
41: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
41: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
41: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
41: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
41: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 5: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
23: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
41: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
41: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
41: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 5: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
23: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
41: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 5: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
23: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 2: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
41: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 2: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 2: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 2: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 2: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 2: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
23: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
23: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
41: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 2: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 2: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 2: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 2: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 2: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 2: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 2: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 2: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 2: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 2: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 2: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
23: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
23: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
23: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
23: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
23: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
23: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
23: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
23: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
41: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 2: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 2: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 2: +--------------------------------------------------------------------------+------------+
 2: Total Trainable Params: 336232258
23: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
23: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
23: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
23: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
23: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
23: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
23: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
23: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
23: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
23: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
23: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
41: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
36: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
23: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
23: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
23: +--------------------------------------------------------------------------+------------+
23: Total Trainable Params: 336232258
41: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
41: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
41: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
41: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
41: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
41: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
41: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
41: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 5: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 5: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 5: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 5: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 5: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 5: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
36: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
36: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
36: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
36: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
36: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
15: +--------------------------------------------------------------------------+------------+
15: |                                 Modules                                  | Parameters |
15: +--------------------------------------------------------------------------+------------+
15: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
15: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
15: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
15: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
15: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
15: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
41: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
41: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
41: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
41: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
41: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
41: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
41: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
41: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 5: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 5: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 5: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 5: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 5: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 5: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 5: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 5: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 5: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 5: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 5: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
41: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
41: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
41: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
41: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
41: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
41: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
41: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
41: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
41: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
41: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
41: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 5: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 5: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 5: +--------------------------------------------------------------------------+------------+
 5: Total Trainable Params: 336232258
41: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
41: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
41: +--------------------------------------------------------------------------+------------+
41: Total Trainable Params: 336232258
 3: +--------------------------------------------------------------------------+------------+
 3: |                                 Modules                                  | Parameters |
 3: +--------------------------------------------------------------------------+------------+
 3: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 3: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 3: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 3: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 3: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
36: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
36: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
36: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
46: +--------------------------------------------------------------------------+------------+
46: |                                 Modules                                  | Parameters |
46: +--------------------------------------------------------------------------+------------+
46: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
46: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
46: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
46: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
46: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
46: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
36: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
46: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
36: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
46: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
36: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
46: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
36: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
46: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
46: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
36: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
36: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
46: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
36: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
46: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
46: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
36: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
36: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
46: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 3: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 3: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 3: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
36: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
46: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
46: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
46: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
46: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
46: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 3: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 3: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
36: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
46: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
36: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
46: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
36: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
36: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
46: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
39: +--------------------------------------------------------------------------+------------+
39: |                                 Modules                                  | Parameters |
39: +--------------------------------------------------------------------------+------------+
39: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
39: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
39: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
39: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
39: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
39: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
46: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
46: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
39: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
39: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
15: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
39: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
32: +--------------------------------------------------------------------------+------------+
32: |                                 Modules                                  | Parameters |
32: +--------------------------------------------------------------------------+------------+
32: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
32: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
32: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
32: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
32: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
32: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
46: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
32: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
46: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
32: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
46: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
32: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
46: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
32: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
46: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
36: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
46: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
36: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
36: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
36: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
36: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
36: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
36: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
36: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
46: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
36: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
36: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
36: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
36: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
36: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
36: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
36: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
36: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
36: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
36: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
36: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
46: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
36: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
36: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
36: +--------------------------------------------------------------------------+------------+
36: Total Trainable Params: 336232258
46: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
46: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 3: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 3: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 3: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 3: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 3: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 3: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
39: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
46: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
46: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
46: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
46: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
46: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
46: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
46: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
46: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 3: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 3: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 3: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 3: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 3: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 3: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 3: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 3: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 3: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 3: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 3: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
39: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
15: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
46: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
46: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
46: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
46: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
46: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
46: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
46: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
46: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
46: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
46: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
46: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 3: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 3: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 3: +--------------------------------------------------------------------------+------------+
 3: Total Trainable Params: 336232258
39: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
46: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
46: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
46: +--------------------------------------------------------------------------+------------+
46: Total Trainable Params: 336232258
 6: +--------------------------------------------------------------------------+------------+
 6: |                                 Modules                                  | Parameters |
 6: +--------------------------------------------------------------------------+------------+
 6: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 6: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 6: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 6: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 6: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
39: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
47: +--------------------------------------------------------------------------+------------+
47: |                                 Modules                                  | Parameters |
47: +--------------------------------------------------------------------------+------------+
47: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
47: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
47: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
47: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
47: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
47: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
47: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
47: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
47: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
47: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
47: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
32: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
47: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
32: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
47: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
47: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
33: +--------------------------------------------------------------------------+------------+
33: |                                 Modules                                  | Parameters |
33: +--------------------------------------------------------------------------+------------+
33: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
33: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
33: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
33: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
33: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
33: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
47: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
47: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
33: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
33: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
47: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
47: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
33: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
47: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
33: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
47: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
47: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
33: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 8: +--------------------------------------------------------------------------+------------+
 8: |                                 Modules                                  | Parameters |
 8: +--------------------------------------------------------------------------+------------+
 8: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 8: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 8: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 8: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 8: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
47: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
47: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 6: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 6: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 6: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
47: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
47: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
47: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
47: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
47: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 6: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 6: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
39: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
47: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
47: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
47: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
47: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
39: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
39: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
39: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
39: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
47: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
39: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
39: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
39: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
39: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
47: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
39: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
47: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
32: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
47: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
32: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
47: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
32: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
32: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
32: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
32: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
32: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
32: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
47: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
32: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
32: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
32: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
32: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
47: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
32: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
47: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
47: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
33: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
47: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
33: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
33: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
33: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
47: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
47: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
39: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
47: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
39: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
47: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
39: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
47: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
47: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 6: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 6: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 6: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 6: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 6: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 6: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
39: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
47: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
47: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
47: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
47: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
47: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
47: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
47: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 6: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 6: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 6: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 6: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 6: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 6: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 6: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 6: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 6: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 6: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 6: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
32: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
47: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
47: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
47: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
47: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
47: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
47: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
47: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
47: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
47: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
47: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
47: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 6: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 6: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 6: +--------------------------------------------------------------------------+------------+
 6: Total Trainable Params: 336232258
32: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
47: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
47: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
47: +--------------------------------------------------------------------------+------------+
47: Total Trainable Params: 336232258
32: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
33: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
33: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
33: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
33: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
33: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
33: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
33: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
33: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
33: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
33: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
33: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
39: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
32: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
33: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
39: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
30: +--------------------------------------------------------------------------+------------+
30: |                                 Modules                                  | Parameters |
30: +--------------------------------------------------------------------------+------------+
30: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
30: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
30: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
30: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
30: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
30: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
39: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
32: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
33: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
30: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
39: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
39: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
39: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
39: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
39: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
39: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
39: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
39: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
39: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
39: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
39: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
39: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
39: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
39: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
39: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
39: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
39: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
39: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
39: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
39: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
39: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
39: +--------------------------------------------------------------------------+------------+
39: Total Trainable Params: 336232258
32: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
40: +--------------------------------------------------------------------------+------------+
40: |                                 Modules                                  | Parameters |
40: +--------------------------------------------------------------------------+------------+
40: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
40: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
40: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
40: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
40: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
40: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
32: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
32: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
32: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
32: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
32: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
32: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
32: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
32: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
32: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
32: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
32: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
32: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
32: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
32: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
32: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
32: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
32: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
32: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
32: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
32: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
32: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
32: +--------------------------------------------------------------------------+------------+
32: Total Trainable Params: 336232258
 8: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 8: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
33: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
33: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
15: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
33: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
15: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
40: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
33: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
33: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
33: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
33: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
33: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
33: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
33: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
33: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
33: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
15: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
33: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
33: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
33: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
33: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
33: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
33: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
33: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
33: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
33: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
33: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
33: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 8: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
33: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
33: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
33: +--------------------------------------------------------------------------+------------+
33: Total Trainable Params: 336232258
 8: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
30: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
35: +--------------------------------------------------------------------------+------------+
35: |                                 Modules                                  | Parameters |
35: +--------------------------------------------------------------------------+------------+
35: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
35: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
35: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
35: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
35: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
35: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 8: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
35: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 8: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 8: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
35: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
15: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
15: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
35: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
15: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
15: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
15: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
15: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
35: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
15: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
35: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
15: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
40: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
35: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 8: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 8: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
30: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
35: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 8: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 8: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
35: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
35: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
35: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
35: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
35: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 8: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
35: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
35: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
35: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
15: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
15: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
35: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
15: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
35: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
15: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
35: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
15: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 8: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
35: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
35: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 8: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 8: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
35: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
35: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
30: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 1: +--------------------------------------------------------------------------+------------+
 1: |                                 Modules                                  | Parameters |
 1: +--------------------------------------------------------------------------+------------+
 1: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 1: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 1: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 1: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 1: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
35: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
40: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
35: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
40: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
35: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
35: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
35: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
35: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
35: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
35: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
35: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
30: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
35: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
35: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
35: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
35: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
35: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
35: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
35: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
35: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
35: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
35: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
35: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
40: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
35: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
35: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
35: +--------------------------------------------------------------------------+------------+
35: Total Trainable Params: 336232258
40: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
34: +--------------------------------------------------------------------------+------------+
34: |                                 Modules                                  | Parameters |
34: +--------------------------------------------------------------------------+------------+
34: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
34: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
34: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
34: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
34: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
34: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
43: +--------------------------------------------------------------------------+------------+
43: |                                 Modules                                  | Parameters |
43: +--------------------------------------------------------------------------+------------+
43: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
43: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
43: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
43: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
43: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
43: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
43: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 1: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
34: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
43: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
43: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
19: +--------------------------------------------------------------------------+------------+
19: |                                 Modules                                  | Parameters |
19: +--------------------------------------------------------------------------+------------+
19: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
19: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
19: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
19: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
19: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
19: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
40: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 1: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
40: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
40: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
40: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
40: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
40: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
40: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
43: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 1: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
43: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
34: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
19: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
19: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
40: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
13: +--------------------------------------------------------------------------+------------+
13: |                                 Modules                                  | Parameters |
13: +--------------------------------------------------------------------------+------------+
13: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
13: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
13: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
13: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
13: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
13: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
30: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
40: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
13: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
13: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
40: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
34: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
13: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
31: +--------------------------------------------------------------------------+------------+
31: |                                 Modules                                  | Parameters |
31: +--------------------------------------------------------------------------+------------+
31: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
31: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
31: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
31: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
31: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
31: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
40: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 1: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 1: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 1: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
13: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
40: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 1: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
34: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
13: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
31: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
43: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 1: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
34: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
34: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
34: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
34: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
34: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
15: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
31: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
43: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 1: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
34: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
34: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
15: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
31: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
19: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
19: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
43: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
43: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
43: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
43: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
43: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
43: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 1: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
34: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
30: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
43: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
43: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
34: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
15: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 1: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
34: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
15: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
30: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
30: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
30: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
30: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
30: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
40: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
34: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
30: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
30: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
30: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
40: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 1: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
34: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
30: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
40: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
34: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
34: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
31: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
34: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
31: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
34: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
31: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
43: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 1: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
34: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
13: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
43: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
13: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
13: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
43: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 1: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
34: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
13: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
30: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
43: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
34: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
30: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
40: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
15: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
30: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
40: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 1: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
15: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
30: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
19: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
40: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 1: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 1: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 1: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 1: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 1: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 1: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
34: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 1: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 1: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 1: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 1: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 1: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 1: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 1: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 1: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 1: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 1: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 1: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
34: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
34: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
40: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 1: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 1: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 1: +--------------------------------------------------------------------------+------------+
 1: Total Trainable Params: 336232258
34: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
34: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
34: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
34: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
34: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
34: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
34: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
34: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
15: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
31: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
31: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
31: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
31: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
31: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
43: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
34: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
34: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
34: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
34: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
34: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
34: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
34: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
34: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
34: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
34: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
34: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 8: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
31: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
43: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
34: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
34: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
34: +--------------------------------------------------------------------------+------------+
34: Total Trainable Params: 336232258
 8: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
31: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
43: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 8: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
27: +--------------------------------------------------------------------------+------------+
27: |                                 Modules                                  | Parameters |
27: +--------------------------------------------------------------------------+------------+
27: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
27: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
27: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
27: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
27: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
27: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
27: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 8: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
40: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
40: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
13: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
27: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
40: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
40: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
40: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
40: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
40: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
40: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
40: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
40: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
13: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
13: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
27: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
40: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
40: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
40: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
40: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
40: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
40: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
40: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
40: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
40: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
40: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
40: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
13: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
13: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
13: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
13: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
13: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
13: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
30: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
40: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
40: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
40: +--------------------------------------------------------------------------+------------+
40: Total Trainable Params: 336232258
13: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
13: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
13: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
13: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
30: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
43: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
13: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
30: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
43: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
15: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
15: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
30: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
15: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
15: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
15: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
15: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
15: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
15: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
15: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
30: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
43: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
15: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
15: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
15: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
15: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
15: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
15: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
15: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
15: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
15: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
15: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
15: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
31: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
43: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
15: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
15: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
15: +--------------------------------------------------------------------------+------------+
15: Total Trainable Params: 336232258
31: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
43: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
43: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 8: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
31: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
43: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
43: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
43: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
43: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
43: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
43: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
43: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
43: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 8: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 8: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 8: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 8: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 8: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 8: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
31: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
43: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
43: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
43: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
43: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
43: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
43: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
43: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
43: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
43: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
43: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
43: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 8: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 8: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 8: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 8: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 8: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 8: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 8: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 8: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 8: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 8: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 8: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
31: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
43: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
43: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
43: +--------------------------------------------------------------------------+------------+
43: Total Trainable Params: 336232258
 8: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 8: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 8: +--------------------------------------------------------------------------+------------+
 8: Total Trainable Params: 336232258
27: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
13: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
27: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
13: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
27: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
13: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
27: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
13: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
27: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
13: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
30: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
14: +--------------------------------------------------------------------------+------------+
14: |                                 Modules                                  | Parameters |
14: +--------------------------------------------------------------------------+------------+
14: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
14: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
14: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
14: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
14: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
14: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
30: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
14: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
14: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
14: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
30: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
14: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
30: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
14: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
31: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
13: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
13: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
13: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
13: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
13: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
27: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
14: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
27: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
14: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
27: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
27: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
27: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
27: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
14: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
27: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
27: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
27: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
21: +--------------------------------------------------------------------------+------------+
21: |                                 Modules                                  | Parameters |
21: +--------------------------------------------------------------------------+------------+
21: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
21: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
21: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
21: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
21: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
21: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
14: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
14: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
27: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
30: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
13: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
30: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
30: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
30: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
30: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
30: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
30: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
30: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
13: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
30: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
30: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
30: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
30: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
30: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
30: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
30: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
30: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
30: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
30: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
30: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
13: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
30: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
30: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
30: +--------------------------------------------------------------------------+------------+
30: Total Trainable Params: 336232258
13: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
31: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
14: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
14: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
31: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
14: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
31: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
14: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
14: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
14: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
14: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
14: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
14: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
27: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
27: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
13: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
13: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
27: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
13: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
13: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
13: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
13: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
13: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
13: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
13: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
13: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
27: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
13: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
13: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
13: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
13: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
13: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
13: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
13: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
13: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
13: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
13: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
13: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
27: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
13: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
13: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
13: +--------------------------------------------------------------------------+------------+
13: Total Trainable Params: 336232258
31: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
31: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
31: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
31: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
31: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
31: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
31: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
31: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
31: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
31: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
31: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
31: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
31: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
31: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
31: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
31: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
31: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
31: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
31: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
31: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
31: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
14: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
31: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
31: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
31: +--------------------------------------------------------------------------+------------+
31: Total Trainable Params: 336232258
14: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
27: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
27: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
14: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
27: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
27: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
27: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
27: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
27: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
14: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
27: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
14: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
27: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
27: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
27: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
27: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
27: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
27: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
27: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
27: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
14: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
14: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
27: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
27: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
27: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
27: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
27: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
27: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
27: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
27: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
27: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
27: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
27: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
14: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
14: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
14: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
14: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
14: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
14: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
14: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
14: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
27: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
27: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
27: +--------------------------------------------------------------------------+------------+
27: Total Trainable Params: 336232258
14: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
14: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
14: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
14: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
14: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
14: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
14: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
14: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
14: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
14: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
14: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
29: +--------------------------------------------------------------------------+------------+
29: |                                 Modules                                  | Parameters |
29: +--------------------------------------------------------------------------+------------+
29: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
29: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
29: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
29: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
29: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
29: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
14: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
14: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
14: +--------------------------------------------------------------------------+------------+
14: Total Trainable Params: 336232258
29: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
10: +--------------------------------------------------------------------------+------------+
10: |                                 Modules                                  | Parameters |
10: +--------------------------------------------------------------------------+------------+
10: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
10: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
10: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
10: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
10: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
10: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
29: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
29: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
10: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
29: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
10: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
29: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
10: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
29: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
29: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
29: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
29: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
29: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
29: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
29: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
10: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
10: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
10: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
10: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
10: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
10: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
29: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
10: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
29: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
29: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
10: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
29: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
10: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
29: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
10: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
29: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
29: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
10: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
29: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
29: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
29: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
29: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
29: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
10: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
29: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
10: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
29: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
29: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
29: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
29: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
29: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
29: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
29: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
29: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
10: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
10: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
29: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
29: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
29: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
29: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
29: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
29: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
29: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
29: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
29: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
29: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
29: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
10: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
10: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
10: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
10: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
10: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
10: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
10: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
10: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
29: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
29: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
29: +--------------------------------------------------------------------------+------------+
29: Total Trainable Params: 336232258
10: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
10: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
10: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
10: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
10: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
10: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
10: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
10: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
10: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
10: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
10: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
28: +--------------------------------------------------------------------------+------------+
28: |                                 Modules                                  | Parameters |
28: +--------------------------------------------------------------------------+------------+
28: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
28: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
28: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
28: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
28: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
28: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
10: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
10: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
10: +--------------------------------------------------------------------------+------------+
10: Total Trainable Params: 336232258
28: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
28: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
28: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
28: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
28: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
28: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
28: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
28: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
28: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
28: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
28: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
28: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
28: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
28: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
28: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
28: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
28: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
28: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
28: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
28: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
28: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
28: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
28: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
28: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
28: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
28: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
28: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
28: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
28: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
28: +--------------------------------------------------------------------------+------------+
28: Total Trainable Params: 336232258
24: +--------------------------------------------------------------------------+------------+
24: |                                 Modules                                  | Parameters |
24: +--------------------------------------------------------------------------+------------+
24: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
24: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
24: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
24: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
24: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
24: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
21: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
21: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
24: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
24: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
24: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
24: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
24: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
24: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
21: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
24: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
24: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
24: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
21: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
24: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
21: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
24: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
19: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
24: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
19: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
24: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
19: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
19: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
24: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
19: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
19: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
24: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
19: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
24: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
20: +--------------------------------------------------------------------------+------------+
20: |                                 Modules                                  | Parameters |
20: +--------------------------------------------------------------------------+------------+
20: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
20: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
20: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
20: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
20: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
20: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
20: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
20: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
20: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
20: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
20: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
24: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
26: +--------------------------------------------------------------------------+------------+
26: |                                 Modules                                  | Parameters |
26: +--------------------------------------------------------------------------+------------+
26: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
26: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
26: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
26: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
26: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
26: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
21: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
21: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
21: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
21: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
26: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
21: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
21: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
26: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
21: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
24: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
19: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
24: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
19: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
24: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
19: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
19: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
19: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
19: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
19: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
24: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
19: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
26: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
20: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
20: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
20: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
20: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
21: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
24: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
24: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
24: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
24: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
24: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
24: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
24: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
21: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
21: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
24: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
24: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
24: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
24: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
24: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
24: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
24: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
24: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
24: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
24: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
24: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
21: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
21: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
21: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
21: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
21: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
21: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
24: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
24: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
24: +--------------------------------------------------------------------------+------------+
24: Total Trainable Params: 336232258
21: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
21: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
21: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
21: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
26: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
21: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
19: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
26: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
26: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
26: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
26: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
26: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
26: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
19: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
26: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
26: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
26: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
26: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
19: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
26: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
19: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
26: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
19: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
26: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
20: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
20: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
26: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
20: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
20: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
26: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
20: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
20: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
20: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
20: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
20: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
20: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
26: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
20: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
20: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
20: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
26: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
20: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
26: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
21: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
26: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
21: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
26: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
21: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
26: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
21: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
21: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
26: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
19: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
26: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
19: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
26: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
26: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
19: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
26: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
19: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
26: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
26: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
26: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
26: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
26: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
26: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
26: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
20: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
26: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
26: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
26: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
26: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
26: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
26: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
26: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
26: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
26: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
26: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
26: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
20: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
26: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
26: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
26: +--------------------------------------------------------------------------+------------+
26: Total Trainable Params: 336232258
20: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
22: +--------------------------------------------------------------------------+------------+
22: |                                 Modules                                  | Parameters |
22: +--------------------------------------------------------------------------+------------+
22: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
22: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
22: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
22: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
22: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
22: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
20: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
22: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
16: +--------------------------------------------------------------------------+------------+
16: |                                 Modules                                  | Parameters |
16: +--------------------------------------------------------------------------+------------+
16: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
16: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
16: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
16: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
16: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
16: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
19: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
19: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
19: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
19: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
19: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
19: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
19: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
19: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
19: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
19: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
19: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
19: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
19: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
19: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
19: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
19: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
19: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
19: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
19: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
19: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
19: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
19: +--------------------------------------------------------------------------+------------+
19: Total Trainable Params: 336232258
20: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
21: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
21: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
21: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
21: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
21: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
21: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
21: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
21: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
21: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
21: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
21: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
21: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
21: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
21: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
21: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
21: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
21: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
21: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
21: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
21: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
21: +--------------------------------------------------------------------------+------------+
21: Total Trainable Params: 336232258
22: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
22: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
22: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
22: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
22: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
22: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
22: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
22: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
22: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
16: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
17: +--------------------------------------------------------------------------+------------+
17: |                                 Modules                                  | Parameters |
17: +--------------------------------------------------------------------------+------------+
17: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
17: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
17: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
17: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
17: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
17: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
20: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
20: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
20: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
20: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
20: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
20: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
20: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
20: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
20: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
20: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
20: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
20: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
20: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
20: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
20: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
20: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
20: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
20: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
20: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
20: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
20: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
20: +--------------------------------------------------------------------------+------------+
20: Total Trainable Params: 336232258
22: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
16: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
16: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
16: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
16: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
16: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
16: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
16: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
16: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
16: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
17: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
22: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
16: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
17: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
17: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
17: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
17: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
17: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
17: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
17: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
17: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
17: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
22: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
16: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
17: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
22: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
22: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
22: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
22: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
22: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
22: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
22: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
22: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
22: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
22: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
22: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
22: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
22: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
22: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
22: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
22: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
22: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
22: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
22: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
22: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
22: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
22: +--------------------------------------------------------------------------+------------+
22: Total Trainable Params: 336232258
16: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
17: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
16: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
16: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
16: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
16: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
16: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
16: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
16: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
16: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
16: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
16: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
16: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
16: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
16: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
16: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
16: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
16: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
16: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
16: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
16: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
16: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
16: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
16: +--------------------------------------------------------------------------+------------+
16: Total Trainable Params: 336232258
17: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
17: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
17: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
17: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
17: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
17: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
17: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
17: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
17: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
17: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
17: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
17: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
17: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
17: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
17: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
17: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
17: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
17: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
17: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
17: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
17: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
17: +--------------------------------------------------------------------------+------------+
17: Total Trainable Params: 336232258
42: +--------------------------------------------------------------------------+------------+
42: |                                 Modules                                  | Parameters |
42: +--------------------------------------------------------------------------+------------+
42: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
42: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
42: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
42: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
42: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
42: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
42: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
42: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
42: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
42: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
42: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
42: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
42: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
42: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
42: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
42: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
42: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
42: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
42: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
42: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
42: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
42: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
42: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
42: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
42: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
42: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
42: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
42: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
42: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
42: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
42: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
42: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
42: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
42: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
42: +--------------------------------------------------------------------------+------------+
42: Total Trainable Params: 336232258
38: +--------------------------------------------------------------------------+------------+
38: |                                 Modules                                  | Parameters |
38: +--------------------------------------------------------------------------+------------+
38: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
38: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
38: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
38: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
38: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
38: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
38: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
38: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
38: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
38: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
38: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
38: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
38: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
38: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
38: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
38: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
38: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
38: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
38: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
38: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
38: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
38: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
38: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
38: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
38: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
38: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
38: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
38: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
38: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
38: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
38: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
38: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
38: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
38: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
38: +--------------------------------------------------------------------------+------------+
38: Total Trainable Params: 336232258
 9: +--------------------------------------------------------------------------+------------+
 9: |                                 Modules                                  | Parameters |
 9: +--------------------------------------------------------------------------+------------+
 9: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
 9: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
 9: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
 9: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
 9: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
 9: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
 9: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
 9: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
 9: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
 9: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
 9: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
 9: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
 9: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
 9: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
 9: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
 9: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
 9: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
 9: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
 9: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
 9: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
 9: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
 9: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
 9: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
 9: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
 9: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
 9: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
 9: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
 9: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
 9: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
 9: +--------------------------------------------------------------------------+------------+
 9: Total Trainable Params: 336232258
18: +--------------------------------------------------------------------------+------------+
18: |                                 Modules                                  | Parameters |
18: +--------------------------------------------------------------------------+------------+
18: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
18: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
18: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
18: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
18: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
18: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
18: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
18: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
18: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
18: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
18: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
18: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
18: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
18: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
18: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
18: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
18: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
18: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
18: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
18: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
18: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
18: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
18: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
18: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
18: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
18: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
18: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
18: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
18: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
18: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
18: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
18: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
18: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
18: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
18: +--------------------------------------------------------------------------+------------+
18: Total Trainable Params: 336232258
25: +--------------------------------------------------------------------------+------------+
25: |                                 Modules                                  | Parameters |
25: +--------------------------------------------------------------------------+------------+
25: |        bert_model_segment.bert.embeddings.word_embeddings.weight         |  31260672  |
25: |      bert_model_segment.bert.embeddings.position_embeddings.weight       |   524288   |
25: |     bert_model_segment.bert.embeddings.token_type_embeddings.weight      |    2048    |
25: |           bert_model_segment.bert.embeddings.LayerNorm.weight            |    1024    |
25: |            bert_model_segment.bert.embeddings.LayerNorm.bias             |    1024    |
25: |       bert_model_segment.bert.encoder.layer.0.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.0.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.0.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.0.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.0.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.0.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.0.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.0.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.1.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.1.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.1.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.1.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.1.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.1.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.1.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.1.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.2.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.2.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.2.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.2.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.2.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.2.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.2.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.2.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.3.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.3.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.3.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.3.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.3.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.3.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.3.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.3.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.4.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.4.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.4.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.4.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.4.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.4.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.4.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.4.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.5.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.5.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.5.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.5.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.5.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.5.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.5.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.5.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.6.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.6.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.6.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.6.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.6.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.6.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.6.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.6.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.7.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.7.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.7.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.7.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.7.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.7.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.7.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.7.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.8.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.8.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.8.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.8.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.8.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.8.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.8.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.8.output.LayerNorm.bias       |    1024    |
25: |       bert_model_segment.bert.encoder.layer.9.attention.qkv_weight       |  3145728   |
25: |        bert_model_segment.bert.encoder.layer.9.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.9.attention.proj_weight       |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.9.attention.proj_bias        |    1024    |
25: | bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_weight  |    1024    |
25: |  bert_model_segment.bert.encoder.layer.9.layernorm_mlp.layer_norm_bias   |    1024    |
25: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc1_bias      |    4096    |
25: |     bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_weight     |  4194304   |
25: |      bert_model_segment.bert.encoder.layer.9.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.9.output.LayerNorm.weight      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.9.output.LayerNorm.bias       |    1024    |
25: |      bert_model_segment.bert.encoder.layer.10.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.10.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.10.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.10.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.10.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.10.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.10.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.10.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.11.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.11.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.11.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.11.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.11.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.11.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.11.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.11.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.12.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.12.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.12.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.12.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.12.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.12.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.12.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.12.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.13.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.13.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.13.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.13.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.13.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.13.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.13.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.13.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.14.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.14.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.14.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.14.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.14.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.14.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.14.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.14.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.15.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.15.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.15.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.15.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.15.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.15.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.15.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.15.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.16.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.16.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.16.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.16.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.16.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.16.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.16.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.16.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.17.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.17.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.17.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.17.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.17.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.17.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.17.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.17.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.18.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.18.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.18.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.18.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.18.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.18.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.18.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.18.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.19.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.19.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.19.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.19.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.19.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.19.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.19.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.19.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.20.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.20.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.20.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.20.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.20.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.20.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.20.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.20.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.21.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.21.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.21.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.21.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.21.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.21.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.21.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.21.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.22.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.22.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.22.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.22.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.22.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.22.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.22.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.22.output.LayerNorm.bias      |    1024    |
25: |      bert_model_segment.bert.encoder.layer.23.attention.qkv_weight       |  3145728   |
25: |       bert_model_segment.bert.encoder.layer.23.attention.qkv_bias        |    3072    |
25: |      bert_model_segment.bert.encoder.layer.23.attention.proj_weight      |  1048576   |
25: |       bert_model_segment.bert.encoder.layer.23.attention.proj_bias       |    1024    |
25: | bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_weight |    1024    |
25: |  bert_model_segment.bert.encoder.layer.23.layernorm_mlp.layer_norm_bias  |    1024    |
25: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc1_bias      |    4096    |
25: |    bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_weight     |  4194304   |
25: |     bert_model_segment.bert.encoder.layer.23.layernorm_mlp.fc2_bias      |    1024    |
25: |     bert_model_segment.bert.encoder.layer.23.output.LayerNorm.weight     |    1024    |
25: |      bert_model_segment.bert.encoder.layer.23.output.LayerNorm.bias      |    1024    |
25: |               bert_model_segment.bert.pooler.dense.weight                |  1048576   |
25: |                bert_model_segment.bert.pooler.dense.bias                 |    1024    |
25: |                 heads_only_segment.cls.predictions.bias                  |   30528    |
25: |        heads_only_segment.cls.predictions.transform.dense.weight         |  1048576   |
25: |         heads_only_segment.cls.predictions.transform.dense.bias          |    1024    |
25: |      heads_only_segment.cls.predictions.transform.LayerNorm.weight       |    1024    |
25: |       heads_only_segment.cls.predictions.transform.LayerNorm.bias        |    1024    |
25: |              heads_only_segment.cls.seq_relationship.weight              |    2048    |
25: |               heads_only_segment.cls.seq_relationship.bias               |     2      |
25: +--------------------------------------------------------------------------+------------+
25: Total Trainable Params: 336232258
 5: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
 5: RESULT,bert,17321,176,nnisbet,2024-08-16 07:57:23 PM
 2: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
 2: RESULT,bert,9601,176,nnisbet,2024-08-16 07:57:23 PM
18: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
18: RESULT,bert,24334,175,nnisbet,2024-08-16 07:57:24 PM
13: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
13: RESULT,bert,31615,176,nnisbet,2024-08-16 07:57:23 PM
27: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
27: RESULT,bert,17151,175,nnisbet,2024-08-16 07:57:24 PM
 7: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
 7: RESULT,bert,7041,176,nnisbet,2024-08-16 07:57:23 PM
16: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
16: RESULT,bert,18912,175,nnisbet,2024-08-16 07:57:24 PM
 9: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
 9: RESULT,bert,5733,176,nnisbet,2024-08-16 07:57:23 PM
38: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
38: RESULT,bert,8093,175,nnisbet,2024-08-16 07:57:24 PM
31: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
31: RESULT,bert,30743,175,nnisbet,2024-08-16 07:57:24 PM
44: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
44: RESULT,bert,15326,176,nnisbet,2024-08-16 07:57:23 PM
 0: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
 0: RESULT,bert,25011,176,nnisbet,2024-08-16 07:57:23 PM
15: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
15: RESULT,bert,30226,176,nnisbet,2024-08-16 07:57:23 PM
17: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
17: RESULT,bert,21693,175,nnisbet,2024-08-16 07:57:24 PM
34: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
34: RESULT,bert,29840,175,nnisbet,2024-08-16 07:57:24 PM
26: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
26: RESULT,bert,5900,175,nnisbet,2024-08-16 07:57:24 PM
46: ENDING TIMING RUN AT 2024-08-16 08:00:19 PM
46: RESULT,bert,21434,176,nnisbet,2024-08-16 07:57:23 PM
22: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
22: RESULT,bert,15859,176,nnisbet,2024-08-16 07:57:24 PM
 4: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
 4: RESULT,bert,3426,177,nnisbet,2024-08-16 07:57:23 PM
39: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
39: RESULT,bert,20207,176,nnisbet,2024-08-16 07:57:24 PM
10: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
10: RESULT,bert,526,177,nnisbet,2024-08-16 07:57:23 PM
25: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
25: RESULT,bert,12320,176,nnisbet,2024-08-16 07:57:24 PM
32: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
32: RESULT,bert,25906,176,nnisbet,2024-08-16 07:57:24 PM
19: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
19: RESULT,bert,14013,176,nnisbet,2024-08-16 07:57:24 PM
 8: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
 8: RESULT,bert,29514,177,nnisbet,2024-08-16 07:57:23 PM
41: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
41: RESULT,bert,3095,177,nnisbet,2024-08-16 07:57:23 PM
40: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
40: RESULT,bert,15650,177,nnisbet,2024-08-16 07:57:23 PM
 6: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
 6: RESULT,bert,5402,177,nnisbet,2024-08-16 07:57:23 PM
11: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
11: RESULT,bert,12147,177,nnisbet,2024-08-16 07:57:23 PM
33: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
33: RESULT,bert,27650,176,nnisbet,2024-08-16 07:57:24 PM
 1: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
 1: RESULT,bert,27849,177,nnisbet,2024-08-16 07:57:23 PM
20: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
20: RESULT,bert,25419,176,nnisbet,2024-08-16 07:57:24 PM
29: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
29: RESULT,bert,32475,176,nnisbet,2024-08-16 07:57:24 PM
47: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
47: RESULT,bert,1404,177,nnisbet,2024-08-16 07:57:23 PM
36: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
36: RESULT,bert,27869,176,nnisbet,2024-08-16 07:57:24 PM
30: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
30: RESULT,bert,11953,176,nnisbet,2024-08-16 07:57:24 PM
14: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
14: RESULT,bert,17109,177,nnisbet,2024-08-16 07:57:23 PM
24: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
24: RESULT,bert,21416,176,nnisbet,2024-08-16 07:57:24 PM
21: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
21: RESULT,bert,21397,176,nnisbet,2024-08-16 07:57:24 PM
45: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
45: RESULT,bert,13745,177,nnisbet,2024-08-16 07:57:23 PM
37: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
37: RESULT,bert,1929,176,nnisbet,2024-08-16 07:57:24 PM
43: ENDING TIMING RUN AT 2024-08-16 08:00:20 PM
43: RESULT,bert,31423,177,nnisbet,2024-08-16 07:57:23 PM
 3: ENDING TIMING RUN AT 2024-08-16 08:00:22 PM
 3: RESULT,bert,15334,179,nnisbet,2024-08-16 07:57:23 PM
23: ENDING TIMING RUN AT 2024-08-16 08:00:22 PM
23: RESULT,bert,12591,178,nnisbet,2024-08-16 07:57:24 PM
12: ENDING TIMING RUN AT 2024-08-16 08:00:22 PM
12: RESULT,bert,20000,179,nnisbet,2024-08-16 07:57:23 PM
28: ENDING TIMING RUN AT 2024-08-16 08:00:22 PM
28: RESULT,bert,5239,178,nnisbet,2024-08-16 07:57:24 PM
42: ENDING TIMING RUN AT 2024-08-16 08:00:22 PM
42: RESULT,bert,6995,179,nnisbet,2024-08-16 07:57:23 PM
35: ENDING TIMING RUN AT 2024-08-16 08:00:22 PM
35: RESULT,bert,648,178,nnisbet,2024-08-16 07:57:24 PM
