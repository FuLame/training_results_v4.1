+ echo 'Beginning trial 9 of 10'
Beginning trial 9 of 10
+ echo ':::DLPAL dockerd://mlperf-dell:lora 219 1 xe9640node507 '\''unknown'\'' XE9640x4H100-SXM-80GB'
:::DLPAL dockerd://mlperf-dell:lora 219 1 xe9640node507 'unknown' XE9640x4H100-SXM-80GB
++ srun --ntasks=1 --container-name=llama2_70b_lora_219 mlperf-sysjson.sh
+ echo ':::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"1","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8452Y","host_processor_core_count":"36","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"4","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.07"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"UNKNOWN_MLPERF_SUBMITTER","division":"closed","status":"Available on-premise","system_name":"UNKNOWN_MLPERF_SYSTEM_NAME","number_of_nodes":"1","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8452Y","host_processor_core_count":"36","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"4","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.07"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ srun --ntasks=1 --container-name=llama2_70b_lora_219 bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
:::GITCOMMITID  
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on xe9640node507
vm.drop_caches = 3
+ export SEED=6021
+ SEED=6021
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1727830858'
RUNANDTIME_START 1727830858
+ srun -l --mpi=pmix --ntasks=4 --ntasks-per-node=4 --time=UNLIMITED --container-name=llama2_70b_lora_219 --container-mounts=/xe9640_nvme1n1/training_datasets_v4.0/:/data:ro,/xe9640_nvme1n1/training_models_v4.0:/ckpt:ro,/training_results_v4.1/Dell/benchmarks/llama2_70b_lora/log:/results:rw --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
0: STARTING TIMING RUN AT 2024-10-02 01:01:05 AM
3: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=36
1: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=36
2: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=36
0: num_gpus=4 num_sockets = 2 num_nodes=2 cores_per_socket=36
1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
0: 
0: 
0: ************** Experiment configuration ***********
0: 
0: model:
0:   ub_tp_comm_overlap_cfg:
0:     qkv_fprop:
0:       method: ring_exchange
0:       aggregate: 0
0:     fc1_fprop:
0:       method: ring_exchange
0:       aggregate: 0
0:     proj_dgrad:
0:       method: ring_exchange
0:       aggregate: 0
0:     fc2_dgrad:
0:       method: ring_exchange
0:       aggregate: 0
0:     proj_fprop:
0:       method: pipeline
0:       num_sm: 32
0:       cga_size: 2
0:       num_splits: 4
0:       set_sm_margin: 1
0:       atomic_gemm: 1
0:       fp8_buf: 1
0:     fc2_fprop:
0:       method: pipeline
0:       num_sm: 16
0:       cga_size: 2
0:       num_splits: 4
0:       set_sm_margin: 1
0:       atomic_gemm: 1
0:       fp8_buf: 0
0:     qkv_dgrad:
0:       method: bulk
0:       num_sm: 4
0:       cga_size: 2
0:       set_sm_margin: 0
0:     fc1_dgrad:
0:       method: ring_exchange
0:       num_sm: 1
0:       cga_size: 2
0:       set_sm_margin: 1
0:       atomic_gemm: 0
0:       fp8_buf: 0
0:   mcore_gpt: true
0:   seed: 6021
0:   tensor_model_parallel_size: 4
0:   pipeline_model_parallel_size: 1
0:   context_parallel_size: 1
0:   cpu_offloading: false
0:   dist_ckpt_load_strictness: log_all
0:   global_batch_size: 8
0:   micro_batch_size: 1
0:   max_position_embeddings: 8192
0:   encoder_seq_length: 8192
0:   restore_from_path: /ckpt
0:   resume_from_checkpoint: null
0:   save_nemo_on_validation_end: false
0:   sync_batch_comm: false
0:   megatron_amp_O2: true
0:   sequence_parallel: 1
0:   activations_checkpoint_granularity: null
0:   activations_checkpoint_method: null
0:   activations_checkpoint_num_layers: null
0:   activations_checkpoint_layers_per_pipeline: null
0:   answer_only_loss: true
0:   gradient_as_bucket_view: false
0:   hidden_dropout: 0.0
0:   attention_dropout: 0.0
0:   ffn_dropout: 0.0
0:   bias_activation_fusion: true
0:   bias_dropout_add_fusion: false
0:   transformer_engine: true
0:   fp8: true
0:   fp8_params: true
0:   fp8_hybrid: true
0:   fp8_amax_history_len: 32
0:   fp8_amax_compute_algo: max
0:   reduce_amax: false
0:   fp8_e4m3: false
0:   fp8_interval: 1
0:   fp8_margin: 0
0:   fp8_dot_product_attention: 1
0:   activation_func_fp8_input_store: 0
0:   apply_rope_fusion: true
0:   disable_parameter_transpose_cache: true
0:   ub_tp_comm_overlap: 0
0:   tp_comm_overlap_ag: false
0:   tp_comm_overlap_rs: false
0:   tp_comm_overlap_rs_dgrad: false
0:   tp_comm_overlap_disable_qkv: true
0:   batch_p2p_comm: 'False'
0:   virtual_pipeline_model_parallel_size: 1
0:   sharp: false
0:   nccl_communicator_config_path: null
0:   peft:
0:     peft_scheme: lora
0:     restore_from_path: null
0:     lora_tuning:
0:       adapter_dim: 16
0:       alpha: 32
0:       adapter_dropout: 0.1
0:       dropout_position: pre
0:       target_modules:
0:       - attention
0:       column_init_method: kaiming
0:       row_init_method: zero
0:       layer_selection: null
0:       weight_tying: false
0:       position_embedding_strategy: null
0:       a2a_experimental: 1
0:   data:
0:     multiprocessing_context: spawn
0:     pin_memory: true
0:     sample_weight: constant
0:     validation_drop_last: false
0:     train_ds:
0:       file_names:
0:       - /data/train.npy
0:       packed_sequence: true
0:       packed_sequence_return_cu_seqlen: false
0:       index_mapping_dir: /results/data_index/train
0:       global_batch_size: 8
0:       micro_batch_size: 1
0:       shuffle: true
0:       num_workers: 1
0:       memmap_workers: 2
0:       pin_memory: true
0:       max_seq_length: 8192
0:       min_seq_length: 1
0:       drop_last: true
0:       concat_sampling_probabilities:
0:       - 1.0
0:       label_key: output
0:       add_eos: true
0:       add_sep: false
0:       add_bos: false
0:       truncation_field: input
0:       prompt_template: '{input} {output}'
0:       truncation_method: right
0:       seed: 6021
0:     validation_ds:
0:       file_names:
0:       - /data/validation.npy
0:       packed_sequence: true
0:       packed_sequence_return_cu_seqlen: false
0:       index_mapping_dir: /results/data_index/val
0:       names: null
0:       global_batch_size: 8
0:       micro_batch_size: 1
0:       shuffle: false
0:       num_workers: 1
0:       memmap_workers: 2
0:       pin_memory: true
0:       max_seq_length: 8192
0:       min_seq_length: 1
0:       drop_last: false
0:       label_key: output
0:       add_eos: true
0:       add_sep: false
0:       add_bos: false
0:       write_predictions_to_file: false
0:       output_file_path_prefix: null
0:       truncation_field: input
0:       prompt_template: '{input} {output}'
0:       tokens_to_generate: 32
0:       truncation_method: right
0:       metric:
0:         name: loss
0:         average: null
0:         num_classes: null
0:   optim:
0:     name: mcore_distributed_optim
0:     overlap_grad_sync: true
0:     overlap_param_sync: true
0:     delay_grad_reduce: true
0:     delay_param_gather: true
0:     average_in_collective: false
0:     lr: 0.0004
0:     min_lr: 0
0:     weight_decay: 0.0001
0:     betas:
0:     - 0.9
0:     - 0.999
0:     eps: 1.0e-08
0:     amsgrad: false
0:     sched:
0:       name: CosineAnnealing
0:       warmup_ratio: 0.0
0:       min_lr: 0.0
0:       constant_steps: 0
0:       monitor: val_loss
0:       reduce_on_plateau: false
0:   enable_cuda_graph: false
0:   enable_cg_fp8_weight_caching: true
0:   custom:
0:     warmup: true
0:     warmup_train_steps: 5
0:     warmup_validation_steps: 5
0:     reset_fp8_stats_after_warmup: 1
0: name: megatron_gpt_peft_lora_tuning
0: trainer:
0:   devices: 4
0:   num_nodes: 1
0:   accelerator: gpu
0:   precision: bf16-mixed
0:   max_steps: 1024
0:   val_check_interval: 192
0:   check_val_every_n_epoch: null
0:   log_every_n_steps: 0
0:   gradient_clip_val: 0.3
0:   gradient_clip_algorithm: norm
0:   num_sanity_val_steps: 0
0:   max_epochs: 1000
0:   limit_val_batches: 1.0
0:   limit_train_batches: 1.0
0:   limit_test_batches: 0
0:   logger: false
0:   enable_checkpointing: false
0:   use_distributed_sampler: false
0:   enable_progress_bar: false
0: exp_manager:
0:   log_tflops_per_sec_per_gpu: false
0:   explicit_log_dir: null
0:   exp_dir: /results
0:   create_wandb_logger: false
0:   resume_if_exists: false
0:   resume_ignore_no_checkpoint: true
0:   create_checkpoint_callback: false
0:   log_global_rank_0_only: true
0:   create_early_stopping_callback: false
0:   create_tensorboard_logger: false
0: 
0: GPU available: True (cuda), used: True
0: TPU available: False, using: 0 TPU cores
0: HPU available: False, using: 0 HPUs
0: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
0: setting number of microbatches to constant 8
2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
0: ----------------------------------------------------------------------------------------------------
0: distributed_backend=nccl
0: All distributed processes registered. Starting with 4 processes
0: ----------------------------------------------------------------------------------------------------
0: 
0: Some keys found in the checkpoint are missing in the provided sharded state dict. 
0: Missing keys (for all ranks): {'model.decoder.layers.self_attention.linear_proj._extra_state', 'model.decoder.layers.self_attention.linear_qkv._extra_state', 'model.decoder.layers.mlp.linear_fc2._extra_state', 'model.decoder.layers.mlp.linear_fc1._extra_state'}. 
2: Some keys found in the checkpoint are missing in the provided sharded state dict. 
2: Missing keys (for all ranks): {'model.decoder.layers.mlp.linear_fc1._extra_state', 'model.decoder.layers.mlp.linear_fc2._extra_state', 'model.decoder.layers.self_attention.linear_proj._extra_state', 'model.decoder.layers.self_attention.linear_qkv._extra_state'}. 
3: Some keys found in the checkpoint are missing in the provided sharded state dict. 
3: Missing keys (for all ranks): {'model.decoder.layers.mlp.linear_fc2._extra_state', 'model.decoder.layers.self_attention.linear_proj._extra_state', 'model.decoder.layers.self_attention.linear_qkv._extra_state', 'model.decoder.layers.mlp.linear_fc1._extra_state'}. 
1: Some keys found in the checkpoint are missing in the provided sharded state dict. 
1: Missing keys (for all ranks): {'model.decoder.layers.mlp.linear_fc2._extra_state', 'model.decoder.layers.mlp.linear_fc1._extra_state', 'model.decoder.layers.self_attention.linear_proj._extra_state', 'model.decoder.layers.self_attention.linear_qkv._extra_state'}. 
0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
0: Loading distributed checkpoint directly on the GPU
0: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
0: make: Nothing to be done for 'default'.
0: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
0: > building indices for blendable datasets ...
0:  > sample ratios:
0:    dataset 0, input: 1, achieved: 1
0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
0: Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
0: Number of buckets for gradient all-reduce / reduce-scatter: 1
0: Params for bucket 1 (11141120 elements):
0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
0: Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0004, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
0: 
0:   | Name         | Type | Params | Mode
0: ---------------------------------------------
0:   | other params | n/a  | 17.3 B | n/a 
0: ---------------------------------------------
0: 11.1 M    Trainable params
0: 17.2 B    Non-trainable params
0: 17.3 B    Total params
0: 69,029.364Total estimated model params size (MB)
0: 0         Modules in train mode
0: 0         Modules in eval mode
0: :::MLLOG {"namespace": "", "time_ms": 1727831055624, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 332}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055625, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 333}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055625, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055625, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Dell", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055625, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055625, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055625, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xXE9640x4H100-SXM-80GB", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055626, "event_type": "POINT_IN_TIME", "key": "seed", "value": 6021, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 335}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831055626, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 341}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056149, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 346}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056169, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 350}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 354}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 358}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 362}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 8, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 367}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 368}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0004, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 369}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 370}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831056170, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 371}}
0: SLURM auto-requeueing enabled. Setting signal handlers.
2: SLURM auto-requeueing enabled. Setting signal handlers.
1: SLURM auto-requeueing enabled. Setting signal handlers.
3: SLURM auto-requeueing enabled. Setting signal handlers.
0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
0: :::MLLOG {"namespace": "", "time_ms": 1727831138353, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831138354, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831138354, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831216571, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.2029857635498047, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 80, "lr": 0.00039990588350021866}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831294932, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4594895839691162, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.00039962362258002984}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831373369, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4386779069900513, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 240, "lr": 0.000399153482893532}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831451852, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3769323825836182, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.000398495906919742}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831530356, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3081121444702148, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 400, "lr": 0.0003976515135461499}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831608878, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3672229051589966, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.0003966210974862433}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831687433, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3201628923416138, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 560, "lr": 0.0003954056285315509}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831765978, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.301447868347168, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00039400625063890885}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831844540, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2972629070281982, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 720, "lr": 0.00039242428085380836}}
0: :::MLLOG {"namespace": "", "time_ms": 1727831923091, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3652453422546387, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.00039066120807083876}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832001648, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.31977379322052, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 880, "lr": 0.00038871869163239207}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832080213, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3160330057144165, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.0003865985597669478}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832158760, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3080248832702637, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1040, "lr": 0.0003843028078684084}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832237315, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.339341163635254, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.0003818335966181045}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832315871, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4131637811660767, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1200, "lr": 0.00037919324995123706}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832394442, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2725095748901367, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.000376384252869671}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832473039, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.287797451019287, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1360, "lr": 0.00037340924910313856}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832551697, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2703615427017212, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.00037027103862105306}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832630341, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3903121948242188, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1520, "lr": 0.000366972574997276}}
0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
0: :::MLLOG {"namespace": "", "time_ms": 1727832656287, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1.0188079858931052}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 1536}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832656287, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 1536}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832656288, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 1536}}
0: setting number of microbatches to constant 8
0: setting number of microbatches to constant 8
0: :::MLLOG {"namespace": "", "time_ms": 1727832736007, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9415267109870911, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 1536}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832736007, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 1536}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832736007, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 1536}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832798813, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.232714295387268, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.00036351696263031675}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832877361, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2983736991882324, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1680, "lr": 0.00035990745382158103}}
0: :::MLLOG {"namespace": "", "time_ms": 1727832955897, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3793407678604126, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.0003561474457144189}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833034439, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2864656448364258, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1840, "lr": 0.0003522404770968524}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833112990, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2877017259597778, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.00034819022507099186}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833112996, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1.0186505915825983}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 1920}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833112997, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833112997, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
0: setting number of microbatches to constant 8
0: setting number of microbatches to constant 8
0: :::MLLOG {"namespace": "", "time_ms": 1727833192463, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9351202249526978, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833192463, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833192463, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833271010, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3388159275054932, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2000, "lr": 0.00034400050159227635}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833349584, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.308237910270691, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.0003396752498817946}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833428124, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.297312617301941, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2160, "lr": 0.0003352185407150632}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833506762, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3147542476654053, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0003306345685907553}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833569625, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1.0181827194967972}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2304}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833569626, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833569626, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
0: setting number of microbatches to constant 8
0: setting number of microbatches to constant 8
0: :::MLLOG {"namespace": "", "time_ms": 1727833649186, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9337118268013, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833649186, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833649186, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833664935, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2795662879943848, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2320, "lr": 0.00032592764778298545}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833743643, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3080002069473267, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.00032110220828086513}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833822342, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2105634212493896, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2480, "lr": 0.0003161627916191529}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833900991, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2550556659698486, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.00031111404660392046}}
0: :::MLLOG {"namespace": "", "time_ms": 1727833979699, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3254700899124146, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2640, "lr": 0.000305960724937259}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834026907, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1.0166772072185577}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2688}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834026908, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834026908, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
0: setting number of microbatches to constant 8
0: setting number of microbatches to constant 8
0: :::MLLOG {"namespace": "", "time_ms": 1727834106358, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9297387599945068, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834106358, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834106358, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834137845, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2720345258712769, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.00030070767674514353}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834216577, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3649446964263916, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2800, "lr": 0.00029535984601266447}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834295316, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.380002498626709, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.00028992226593092135}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834374056, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3126469850540161, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2960, "lr": 0.00028440005415995997}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834452788, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2888615131378174, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.00027879840801220965}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834484285, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1.0161255566845835}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3072}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834484285, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834484285, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
0: setting number of microbatches to constant 8
0: setting number of microbatches to constant 8
0: :::MLLOG {"namespace": "", "time_ms": 1727834563762, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9263691306114197, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834563762, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834563762, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3072}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834610990, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3809235095977783, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3120, "lr": 0.0002731225995609548}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834689707, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2882025241851807, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3200, "lr": 0.000267377970678444}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834768453, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.1966822147369385, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3280, "lr": 0.000261569928008307}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834847174, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2493250370025635, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3360, "lr": 0.0002557039378770106}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834925901, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2484076023101807, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3440, "lr": 0.0002497855211491441}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834941647, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 1.0162365984676915}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3456}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834941647, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3456}}
0: :::MLLOG {"namespace": "", "time_ms": 1727834941648, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3456}}
0: setting number of microbatches to constant 8
0: setting number of microbatches to constant 8
0: :::MLLOG {"namespace": "", "time_ms": 1727835021370, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9223161935806274, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3456}}
0: :::MLLOG {"namespace": "", "time_ms": 1727835021370, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3456}}
0: :::MLLOG {"namespace": "", "time_ms": 1727835021370, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9223161935806274, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 321, "samples_count": 3456, "status": "success"}}
2: [rank2]:[W1002 02:10:25.336591570 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
3: [rank3]:[W1002 02:10:25.336784521 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
1: [rank1]:[W1002 02:10:25.372296558 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
0: [rank0]:[W1002 02:10:25.375319011 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
0: ENDING TIMING RUN AT 2024-10-02 02:11:03 AM
0: RESULT,LLM_FINETUNING,,4198,dell,2024-10-02 01:01:05 AM
++ date +%s
+ echo 'RUNANDTIME_STOP 1727835121'
RUNANDTIME_STOP 1727835121
+ set -e
