+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 246 16 GPU-[94,180,146,457,993,130,881,351,338,8,342,234,245,186,792,332] BM.GPU.H100.8 Cluster DGXH100_16x8x1xtp4pp1cp2'
:::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 246 16 GPU-[94,180,146,457,993,130,881,351,338,8,342,234,245,186,792,332] BM.GPU.H100.8 Cluster DGXH100_16x8x1xtp4pp1cp2
++ srun --ntasks=1 --container-name=llama2_70b_lora_246 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ srun --ntasks=1 --container-name=llama2_70b_lora_246 bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
:::GITCOMMITID  
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on GPU-792
Clearing cache on GPU-332
Clearing cache on GPU-881
Clearing cache on GPU-338
Clearing cache on GPU-351
Clearing cache on GPU-342
Clearing cache on GPU-180
Clearing cache on GPU-245
Clearing cache on GPU-234
Clearing cache on GPU-186
Clearing cache on GPU-993
Clearing cache on GPU-8
Clearing cache on GPU-457
Clearing cache on GPU-130
Clearing cache on GPU-94
Clearing cache on GPU-146
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ export SEED=11227
+ SEED=11227
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1728616179'
RUNANDTIME_START 1728616179
+ srun -l --mpi=pmix --ntasks=128 --ntasks-per-node=8 --time=20 --container-name=llama2_70b_lora_246 --container-mounts=/mnt/orangefs/mlperf/llama/data_model/gov_report:/data:ro,/mnt/orangefs/mlperf/llama/data_model/model:/ckpt:ro,/mnt/orangefs/mlperf/llama/logs:/results:rw --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
  0: STARTING TIMING RUN AT 2024-10-11 03:09:48 AM
112: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
106: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
107: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
113: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
117: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
126: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
110: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
119: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
104: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 91: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 74: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 73: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 66: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 88: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
124: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 95: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
108: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
111: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
105: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
109: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 69: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 80: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 81: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 67: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
114: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
118: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
115: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
116: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 89: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
122: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 85: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 94: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 92: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 90: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 93: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 75: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 78: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 76: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 72: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 77: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 79: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 87: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 84: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
101: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
127: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
123: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
125: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
121: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
120: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 86: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 83: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 82: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 99: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 96: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 68: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 98: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 64: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 97: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 65: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 70: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 71: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
102: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
103: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
100: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: 
  0: 
  0: ************** Experiment configuration ***********
  0: 
  0: model:
  0:   ub_tp_comm_overlap_cfg:
  0:     qkv_fprop:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     fc1_fprop:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     proj_dgrad:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     fc2_dgrad:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     proj_fprop:
  0:       method: pipeline
  0:       num_sm: 32
  0:       cga_size: 2
  0:       num_splits: 4
  0:       set_sm_margin: 1
  0:       atomic_gemm: 1
  0:       fp8_buf: 1
  0:     fc2_fprop:
  0:       method: pipeline
  0:       num_sm: 16
  0:       cga_size: 2
  0:       num_splits: 4
  0:       set_sm_margin: 1
  0:       atomic_gemm: 1
  0:       fp8_buf: 0
  0:     qkv_dgrad:
  0:       method: bulk
  0:       num_sm: 4
  0:       cga_size: 2
  0:       set_sm_margin: 0
  0:     fc1_dgrad:
  0:       method: ring_exchange
  0:       num_sm: 1
  0:       cga_size: 2
  0:       set_sm_margin: 1
  0:       atomic_gemm: 0
  0:       fp8_buf: 0
  0:   mcore_gpt: true
  0:   seed: 11227
  0:   tensor_model_parallel_size: 4
  0:   pipeline_model_parallel_size: 1
  0:   context_parallel_size: 2
  0:   cpu_offloading: false
  0:   dist_ckpt_load_strictness: log_all
  0:   global_batch_size: 16
  0:   micro_batch_size: 1
  0:   max_position_embeddings: 8192
  0:   encoder_seq_length: 8192
  0:   restore_from_path: /ckpt
  0:   resume_from_checkpoint: null
  0:   save_nemo_on_validation_end: false
  0:   sync_batch_comm: false
  0:   megatron_amp_O2: true
  0:   sequence_parallel: 1
  0:   activations_checkpoint_granularity: null
  0:   activations_checkpoint_method: null
  0:   activations_checkpoint_num_layers: null
  0:   activations_checkpoint_layers_per_pipeline: null
  0:   answer_only_loss: true
  0:   gradient_as_bucket_view: false
  0:   hidden_dropout: 0.0
  0:   attention_dropout: 0.0
  0:   ffn_dropout: 0.0
  0:   bias_activation_fusion: true
  0:   bias_dropout_add_fusion: false
  0:   transformer_engine: true
  0:   fp8: true
  0:   fp8_params: true
  0:   fp8_hybrid: true
  0:   fp8_amax_history_len: 32
  0:   fp8_amax_compute_algo: max
  0:   reduce_amax: false
  0:   fp8_e4m3: false
  0:   fp8_interval: 1
  0:   fp8_margin: 0
  0:   fp8_dot_product_attention: 0
  0:   activation_func_fp8_input_store: 0
  0:   apply_rope_fusion: true
  0:   disable_parameter_transpose_cache: true
  0:   ub_tp_comm_overlap: 1
  0:   tp_comm_overlap_ag: true
  0:   tp_comm_overlap_rs: true
  0:   tp_comm_overlap_rs_dgrad: false
  0:   tp_comm_overlap_disable_qkv: true
  0:   batch_p2p_comm: 'False'
  0:   virtual_pipeline_model_parallel_size: 1
  0:   sharp: true
  0:   nccl_communicator_config_path: null
  0:   peft:
  0:     peft_scheme: lora
  0:     restore_from_path: null
  0:     lora_tuning:
  0:       adapter_dim: 16
  0:       alpha: 32
  0:       adapter_dropout: 0.1
  0:       dropout_position: pre
  0:       target_modules:
  0:       - attention
  0:       column_init_method: kaiming
  0:       row_init_method: zero
  0:       layer_selection: null
  0:       weight_tying: false
  0:       position_embedding_strategy: null
  0:       a2a_experimental: 1
  0:   data:
  0:     multiprocessing_context: spawn
  0:     pin_memory: true
  0:     sample_weight: constant
  0:     validation_drop_last: false
  0:     train_ds:
  0:       file_names:
  0:       - /data/train.npy
  0:       packed_sequence: true
  0:       packed_sequence_return_cu_seqlen: false
  0:       index_mapping_dir: /results/data_index/train
  0:       global_batch_size: 16
  0:       micro_batch_size: 1
  0:       shuffle: true
  0:       num_workers: 1
  0:       memmap_workers: 2
  0:       pin_memory: true
  0:       max_seq_length: 8192
  0:       min_seq_length: 1
  0:       drop_last: true
  0:       concat_sampling_probabilities:
  0:       - 1.0
  0:       label_key: output
  0:       add_eos: true
  0:       add_sep: false
  0:       add_bos: false
  0:       truncation_field: input
  0:       prompt_template: '{input} {output}'
  0:       truncation_method: right
  0:       seed: 11227
  0:     validation_ds:
  0:       file_names:
  0:       - /data/validation.npy
  0:       packed_sequence: true
  0:       packed_sequence_return_cu_seqlen: false
  0:       index_mapping_dir: /results/data_index/val
  0:       names: null
  0:       global_batch_size: 16
  0:       micro_batch_size: 1
  0:       shuffle: false
  0:       num_workers: 1
  0:       memmap_workers: 2
  0:       pin_memory: true
  0:       max_seq_length: 8192
  0:       min_seq_length: 1
  0:       drop_last: false
  0:       label_key: output
  0:       add_eos: true
  0:       add_sep: false
  0:       add_bos: false
  0:       write_predictions_to_file: false
  0:       output_file_path_prefix: null
  0:       truncation_field: input
  0:       prompt_template: '{input} {output}'
  0:       tokens_to_generate: 32
  0:       truncation_method: right
  0:       metric:
  0:         name: loss
  0:         average: null
  0:         num_classes: null
  0:   optim:
  0:     name: mcore_distributed_optim
  0:     overlap_grad_sync: true
  0:     overlap_param_sync: true
  0:     delay_grad_reduce: true
  0:     delay_param_gather: true
  0:     average_in_collective: false
  0:     lr: 0.00035
  0:     min_lr: 0
  0:     weight_decay: 0.0001
  0:     betas:
  0:     - 0.9
  0:     - 0.999
  0:     eps: 1.0e-08
  0:     amsgrad: false
  0:     sched:
  0:       name: CosineAnnealing
  0:       warmup_ratio: 0.0
  0:       min_lr: 0.0
  0:       constant_steps: 0
  0:       monitor: val_loss
  0:       reduce_on_plateau: false
  0:   enable_cuda_graph: 1
  0:   enable_cg_fp8_weight_caching: true
  0:   custom:
  0:     warmup: true
  0:     warmup_train_steps: 5
  0:     warmup_validation_steps: 5
  0:     reset_fp8_stats_after_warmup: 1
  0: name: megatron_gpt_peft_lora_tuning
  0: trainer:
  0:   devices: 8
  0:   num_nodes: 16
  0:   accelerator: gpu
  0:   precision: bf16-mixed
  0:   max_steps: 1024
  0:   val_check_interval: 120
  0:   check_val_every_n_epoch: null
  0:   log_every_n_steps: 0
  0:   gradient_clip_val: 0.3
  0:   gradient_clip_algorithm: norm
  0:   num_sanity_val_steps: 0
  0:   max_epochs: 1000
  0:   limit_val_batches: 1.0
  0:   limit_train_batches: 1.0
  0:   limit_test_batches: 0
  0:   logger: false
  0:   enable_checkpointing: false
  0:   use_distributed_sampler: false
  0:   enable_progress_bar: false
  0: exp_manager:
  0:   log_tflops_per_sec_per_gpu: false
  0:   explicit_log_dir: null
  0:   exp_dir: /results
  0:   create_wandb_logger: false
  0:   resume_if_exists: false
  0:   resume_ignore_no_checkpoint: true
  0:   create_checkpoint_callback: false
  0:   log_global_rank_0_only: true
  0:   create_early_stopping_callback: false
  0:   create_tensorboard_logger: false
  0: 
  0: GPU available: True (cuda), used: True
  0: TPU available: False, using: 0 TPU cores
  0: HPU available: False, using: 0 HPUs
  0: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
  0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
113: Initializing distributed: GLOBAL_RANK: 113, MEMBER: 114/128
115: Initializing distributed: GLOBAL_RANK: 115, MEMBER: 116/128
112: Initializing distributed: GLOBAL_RANK: 112, MEMBER: 113/128
 65: Initializing distributed: GLOBAL_RANK: 65, MEMBER: 66/128
 66: Initializing distributed: GLOBAL_RANK: 66, MEMBER: 67/128
 64: Initializing distributed: GLOBAL_RANK: 64, MEMBER: 65/128
118: Initializing distributed: GLOBAL_RANK: 118, MEMBER: 119/128
117: Initializing distributed: GLOBAL_RANK: 117, MEMBER: 118/128
 60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/128
114: Initializing distributed: GLOBAL_RANK: 114, MEMBER: 115/128
 59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/128
119: Initializing distributed: GLOBAL_RANK: 119, MEMBER: 120/128
 57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/128
116: Initializing distributed: GLOBAL_RANK: 116, MEMBER: 117/128
 67: Initializing distributed: GLOBAL_RANK: 67, MEMBER: 68/128
 69: Initializing distributed: GLOBAL_RANK: 69, MEMBER: 70/128
 71: Initializing distributed: GLOBAL_RANK: 71, MEMBER: 72/128
 68: Initializing distributed: GLOBAL_RANK: 68, MEMBER: 69/128
 70: Initializing distributed: GLOBAL_RANK: 70, MEMBER: 71/128
 78: Initializing distributed: GLOBAL_RANK: 78, MEMBER: 79/128
 74: Initializing distributed: GLOBAL_RANK: 74, MEMBER: 75/128
 56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/128
 63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/128
 61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/128
 58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/128
 79: Initializing distributed: GLOBAL_RANK: 79, MEMBER: 80/128
121: Initializing distributed: GLOBAL_RANK: 121, MEMBER: 122/128
125: Initializing distributed: GLOBAL_RANK: 125, MEMBER: 126/128
124: Initializing distributed: GLOBAL_RANK: 124, MEMBER: 125/128
 62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/128
 75: Initializing distributed: GLOBAL_RANK: 75, MEMBER: 76/128
 10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/128
  9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/128
 76: Initializing distributed: GLOBAL_RANK: 76, MEMBER: 77/128
 73: Initializing distributed: GLOBAL_RANK: 73, MEMBER: 74/128
 13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/128
 77: Initializing distributed: GLOBAL_RANK: 77, MEMBER: 78/128
 11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/128
126: Initializing distributed: GLOBAL_RANK: 126, MEMBER: 127/128
 72: Initializing distributed: GLOBAL_RANK: 72, MEMBER: 73/128
123: Initializing distributed: GLOBAL_RANK: 123, MEMBER: 124/128
120: Initializing distributed: GLOBAL_RANK: 120, MEMBER: 121/128
111: Initializing distributed: GLOBAL_RANK: 111, MEMBER: 112/128
105: Initializing distributed: GLOBAL_RANK: 105, MEMBER: 106/128
106: Initializing distributed: GLOBAL_RANK: 106, MEMBER: 107/128
110: Initializing distributed: GLOBAL_RANK: 110, MEMBER: 111/128
127: Initializing distributed: GLOBAL_RANK: 127, MEMBER: 128/128
 12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/128
 14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/128
 46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/128
 47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/128
 44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/128
  8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/128
108: Initializing distributed: GLOBAL_RANK: 108, MEMBER: 109/128
 15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/128
122: Initializing distributed: GLOBAL_RANK: 122, MEMBER: 123/128
104: Initializing distributed: GLOBAL_RANK: 104, MEMBER: 105/128
 40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/128
  0: setting number of microbatches to constant 1
107: Initializing distributed: GLOBAL_RANK: 107, MEMBER: 108/128
 45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/128
 41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/128
 43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/128
109: Initializing distributed: GLOBAL_RANK: 109, MEMBER: 110/128
 42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/128
 31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/128
 27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/128
 25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/128
 29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/128
 26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/128
 36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/128
 39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/128
 38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/128
 83: Initializing distributed: GLOBAL_RANK: 83, MEMBER: 84/128
 85: Initializing distributed: GLOBAL_RANK: 85, MEMBER: 86/128
 84: Initializing distributed: GLOBAL_RANK: 84, MEMBER: 85/128
 80: Initializing distributed: GLOBAL_RANK: 80, MEMBER: 81/128
 24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/128
 33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/128
 30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/128
 28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/128
 87: Initializing distributed: GLOBAL_RANK: 87, MEMBER: 88/128
 86: Initializing distributed: GLOBAL_RANK: 86, MEMBER: 87/128
 34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/128
 32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/128
 81: Initializing distributed: GLOBAL_RANK: 81, MEMBER: 82/128
 49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/128
 48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/128
 35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/128
 82: Initializing distributed: GLOBAL_RANK: 82, MEMBER: 83/128
 37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/128
 53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/128
 50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/128
 54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/128
 55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/128
 51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/128
 52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/128
  3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/128
 19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/128
 22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/128
 17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/128
  7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/128
  1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/128
  2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/128
  6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/128
 21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/128
 20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/128
  5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/128
 16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/128
 23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/128
  0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/128
 18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/128
  4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/128
101: Initializing distributed: GLOBAL_RANK: 101, MEMBER: 102/128
100: Initializing distributed: GLOBAL_RANK: 100, MEMBER: 101/128
 99: Initializing distributed: GLOBAL_RANK: 99, MEMBER: 100/128
103: Initializing distributed: GLOBAL_RANK: 103, MEMBER: 104/128
 98: Initializing distributed: GLOBAL_RANK: 98, MEMBER: 99/128
 97: Initializing distributed: GLOBAL_RANK: 97, MEMBER: 98/128
 96: Initializing distributed: GLOBAL_RANK: 96, MEMBER: 97/128
 89: Initializing distributed: GLOBAL_RANK: 89, MEMBER: 90/128
102: Initializing distributed: GLOBAL_RANK: 102, MEMBER: 103/128
 91: Initializing distributed: GLOBAL_RANK: 91, MEMBER: 92/128
 94: Initializing distributed: GLOBAL_RANK: 94, MEMBER: 95/128
 93: Initializing distributed: GLOBAL_RANK: 93, MEMBER: 94/128
 88: Initializing distributed: GLOBAL_RANK: 88, MEMBER: 89/128
 92: Initializing distributed: GLOBAL_RANK: 92, MEMBER: 93/128
 90: Initializing distributed: GLOBAL_RANK: 90, MEMBER: 91/128
 95: Initializing distributed: GLOBAL_RANK: 95, MEMBER: 96/128
  1: NCCL version 2.22.3+cuda12.6
  0: ----------------------------------------------------------------------------------------------------
  0: distributed_backend=nccl
  0: All distributed processes registered. Starting with 128 processes
  0: ----------------------------------------------------------------------------------------------------
  0: 
  2: NCCL version 2.22.3+cuda12.6
  0: The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
  3: NCCL version 2.22.3+cuda12.6
  0: NCCL version 2.22.3+cuda12.6
  0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
  0: Loading distributed checkpoint directly on the GPU
 48: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
120: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 72: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 96: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 88: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 80: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  8: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 16: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 64: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 24: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 40: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 32: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 56: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
112: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
104: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
120: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 48: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 96: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 88: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
104: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 80: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 16: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
112: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
  8: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 40: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 64: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 32: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 72: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 24: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
  0: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 56: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 48: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 88: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
120: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
104: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 80: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 96: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 16: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  8: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 32: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 64: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
112: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 72: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 40: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 24: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 56: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: > building indices for blendable datasets ...
  0:  > sample ratios:
  0:    dataset 0, input: 1, achieved: 1
  0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 96: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
105: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 97: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
106: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 98: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
107: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 72: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 99: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
120: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 88: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
108: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 81: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
100: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
109: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
112: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 73: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 64: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
101: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 89: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
110: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
113: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 74: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 65: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
103: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 90: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
104: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
114: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 75: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 66: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
102: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
121: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 91: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
111: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
115: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 76: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 67: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
122: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 92: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
116: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 77: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 68: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
123: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 93: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
117: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 78: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 69: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 82: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
124: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 94: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
118: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 79: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 70: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 83: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
125: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 95: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
119: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 71: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 84: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
126: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 85: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
127: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 86: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 87: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 80: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  0: Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
  0: Number of buckets for gradient all-reduce / reduce-scatter: 1
  0: Params for bucket 1 (11141120 elements):
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00035, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
  0: 
  0:   | Name         | Type | Params | Mode
  0: ---------------------------------------------
  0:   | other params | n/a  | 17.3 B | n/a 
  0: ---------------------------------------------
  0: 11.1 M    Trainable params
  0: 17.2 B    Non-trainable params
  0: 17.3 B    Total params
  0: 69,029.364Total estimated model params size (MB)
  0: 0         Modules in train mode
  0: 0         Modules in eval mode
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482171, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 332}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482171, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 333}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482171, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482171, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482171, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482171, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482172, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482172, "event_type": "POINT_IN_TIME", "key": "seed", "value": 11227, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 335}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482172, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 341}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482863, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 346}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482898, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 350}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482898, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 354}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482899, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 358}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482899, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 362}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482899, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 367}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482899, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 368}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482899, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 369}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482899, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 370}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616482899, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 371}}
 96: SLURM auto-requeueing enabled. Setting signal handlers.
121: SLURM auto-requeueing enabled. Setting signal handlers.
 16: SLURM auto-requeueing enabled. Setting signal handlers.
 72: SLURM auto-requeueing enabled. Setting signal handlers.
  0: SLURM auto-requeueing enabled. Setting signal handlers.
 64: SLURM auto-requeueing enabled. Setting signal handlers.
 80: SLURM auto-requeueing enabled. Setting signal handlers.
 32: SLURM auto-requeueing enabled. Setting signal handlers.
 97: SLURM auto-requeueing enabled. Setting signal handlers.
122: SLURM auto-requeueing enabled. Setting signal handlers.
 24: SLURM auto-requeueing enabled. Setting signal handlers.
 48: SLURM auto-requeueing enabled. Setting signal handlers.
  8: SLURM auto-requeueing enabled. Setting signal handlers.
 17: SLURM auto-requeueing enabled. Setting signal handlers.
 88: SLURM auto-requeueing enabled. Setting signal handlers.
104: SLURM auto-requeueing enabled. Setting signal handlers.
112: SLURM auto-requeueing enabled. Setting signal handlers.
 44: SLURM auto-requeueing enabled. Setting signal handlers.
 73: SLURM auto-requeueing enabled. Setting signal handlers.
 56: SLURM auto-requeueing enabled. Setting signal handlers.
  5: SLURM auto-requeueing enabled. Setting signal handlers.
 65: SLURM auto-requeueing enabled. Setting signal handlers.
 98: SLURM auto-requeueing enabled. Setting signal handlers.
123: SLURM auto-requeueing enabled. Setting signal handlers.
 25: SLURM auto-requeueing enabled. Setting signal handlers.
 49: SLURM auto-requeueing enabled. Setting signal handlers.
 10: SLURM auto-requeueing enabled. Setting signal handlers.
 18: SLURM auto-requeueing enabled. Setting signal handlers.
 89: SLURM auto-requeueing enabled. Setting signal handlers.
105: SLURM auto-requeueing enabled. Setting signal handlers.
113: SLURM auto-requeueing enabled. Setting signal handlers.
 45: SLURM auto-requeueing enabled. Setting signal handlers.
 74: SLURM auto-requeueing enabled. Setting signal handlers.
 59: SLURM auto-requeueing enabled. Setting signal handlers.
  6: SLURM auto-requeueing enabled. Setting signal handlers.
 66: SLURM auto-requeueing enabled. Setting signal handlers.
 33: SLURM auto-requeueing enabled. Setting signal handlers.
 99: SLURM auto-requeueing enabled. Setting signal handlers.
124: SLURM auto-requeueing enabled. Setting signal handlers.
 26: SLURM auto-requeueing enabled. Setting signal handlers.
 50: SLURM auto-requeueing enabled. Setting signal handlers.
 11: SLURM auto-requeueing enabled. Setting signal handlers.
 19: SLURM auto-requeueing enabled. Setting signal handlers.
 90: SLURM auto-requeueing enabled. Setting signal handlers.
106: SLURM auto-requeueing enabled. Setting signal handlers.
114: SLURM auto-requeueing enabled. Setting signal handlers.
 46: SLURM auto-requeueing enabled. Setting signal handlers.
 75: SLURM auto-requeueing enabled. Setting signal handlers.
 60: SLURM auto-requeueing enabled. Setting signal handlers.
  7: SLURM auto-requeueing enabled. Setting signal handlers.
 67: SLURM auto-requeueing enabled. Setting signal handlers.
 83: SLURM auto-requeueing enabled. Setting signal handlers.
 34: SLURM auto-requeueing enabled. Setting signal handlers.
100: SLURM auto-requeueing enabled. Setting signal handlers.
125: SLURM auto-requeueing enabled. Setting signal handlers.
 27: SLURM auto-requeueing enabled. Setting signal handlers.
 51: SLURM auto-requeueing enabled. Setting signal handlers.
 12: SLURM auto-requeueing enabled. Setting signal handlers.
 20: SLURM auto-requeueing enabled. Setting signal handlers.
 91: SLURM auto-requeueing enabled. Setting signal handlers.
108: SLURM auto-requeueing enabled. Setting signal handlers.
115: SLURM auto-requeueing enabled. Setting signal handlers.
 47: SLURM auto-requeueing enabled. Setting signal handlers.
 76: SLURM auto-requeueing enabled. Setting signal handlers.
 63: SLURM auto-requeueing enabled. Setting signal handlers.
  1: SLURM auto-requeueing enabled. Setting signal handlers.
 68: SLURM auto-requeueing enabled. Setting signal handlers.
 84: SLURM auto-requeueing enabled. Setting signal handlers.
 35: SLURM auto-requeueing enabled. Setting signal handlers.
101: SLURM auto-requeueing enabled. Setting signal handlers.
126: SLURM auto-requeueing enabled. Setting signal handlers.
 29: SLURM auto-requeueing enabled. Setting signal handlers.
 52: SLURM auto-requeueing enabled. Setting signal handlers.
 13: SLURM auto-requeueing enabled. Setting signal handlers.
 21: SLURM auto-requeueing enabled. Setting signal handlers.
 94: SLURM auto-requeueing enabled. Setting signal handlers.
109: SLURM auto-requeueing enabled. Setting signal handlers.
116: SLURM auto-requeueing enabled. Setting signal handlers.
 40: SLURM auto-requeueing enabled. Setting signal handlers.
 77: SLURM auto-requeueing enabled. Setting signal handlers.
 62: SLURM auto-requeueing enabled. Setting signal handlers.
  3: SLURM auto-requeueing enabled. Setting signal handlers.
 69: SLURM auto-requeueing enabled. Setting signal handlers.
 81: SLURM auto-requeueing enabled. Setting signal handlers.
 36: SLURM auto-requeueing enabled. Setting signal handlers.
102: SLURM auto-requeueing enabled. Setting signal handlers.
127: SLURM auto-requeueing enabled. Setting signal handlers.
 30: SLURM auto-requeueing enabled. Setting signal handlers.
 53: SLURM auto-requeueing enabled. Setting signal handlers.
 14: SLURM auto-requeueing enabled. Setting signal handlers.
 23: SLURM auto-requeueing enabled. Setting signal handlers.
 92: SLURM auto-requeueing enabled. Setting signal handlers.
110: SLURM auto-requeueing enabled. Setting signal handlers.
117: SLURM auto-requeueing enabled. Setting signal handlers.
 41: SLURM auto-requeueing enabled. Setting signal handlers.
 78: SLURM auto-requeueing enabled. Setting signal handlers.
 57: SLURM auto-requeueing enabled. Setting signal handlers.
  4: SLURM auto-requeueing enabled. Setting signal handlers.
 70: SLURM auto-requeueing enabled. Setting signal handlers.
 82: SLURM auto-requeueing enabled. Setting signal handlers.
 37: SLURM auto-requeueing enabled. Setting signal handlers.
103: SLURM auto-requeueing enabled. Setting signal handlers.
120: SLURM auto-requeueing enabled. Setting signal handlers.
 31: SLURM auto-requeueing enabled. Setting signal handlers.
 54: SLURM auto-requeueing enabled. Setting signal handlers.
 15: SLURM auto-requeueing enabled. Setting signal handlers.
 22: SLURM auto-requeueing enabled. Setting signal handlers.
 93: SLURM auto-requeueing enabled. Setting signal handlers.
111: SLURM auto-requeueing enabled. Setting signal handlers.
118: SLURM auto-requeueing enabled. Setting signal handlers.
 42: SLURM auto-requeueing enabled. Setting signal handlers.
 79: SLURM auto-requeueing enabled. Setting signal handlers.
 58: SLURM auto-requeueing enabled. Setting signal handlers.
  2: SLURM auto-requeueing enabled. Setting signal handlers.
 71: SLURM auto-requeueing enabled. Setting signal handlers.
 85: SLURM auto-requeueing enabled. Setting signal handlers.
 39: SLURM auto-requeueing enabled. Setting signal handlers.
 28: SLURM auto-requeueing enabled. Setting signal handlers.
 55: SLURM auto-requeueing enabled. Setting signal handlers.
  9: SLURM auto-requeueing enabled. Setting signal handlers.
 95: SLURM auto-requeueing enabled. Setting signal handlers.
107: SLURM auto-requeueing enabled. Setting signal handlers.
119: SLURM auto-requeueing enabled. Setting signal handlers.
 43: SLURM auto-requeueing enabled. Setting signal handlers.
 61: SLURM auto-requeueing enabled. Setting signal handlers.
 86: SLURM auto-requeueing enabled. Setting signal handlers.
 38: SLURM auto-requeueing enabled. Setting signal handlers.
 87: SLURM auto-requeueing enabled. Setting signal handlers.
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: !!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
 48: !!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
 40: !!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
 64: !!! [UB] Global ranks on node 8: [64, 65, 66, 67, 68, 69, 70, 71]
 72: !!! [UB] Global ranks on node 9: [72, 73, 74, 75, 76, 77, 78, 79]
 24: !!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
 16: !!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
  8: !!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
 32: !!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
 88: !!! [UB] Global ranks on node 11: [88, 89, 90, 91, 92, 93, 94, 95]
 80: !!! [UB] Global ranks on node 10: [80, 81, 82, 83, 84, 85, 86, 87]
112: !!! [UB] Global ranks on node 14: [112, 113, 114, 115, 116, 117, 118, 119]
120: !!! [UB] Global ranks on node 15: [120, 121, 122, 123, 124, 125, 126, 127]
 96: !!! [UB] Global ranks on node 12: [96, 97, 98, 99, 100, 101, 102, 103]
104: !!! [UB] Global ranks on node 13: [104, 105, 106, 107, 108, 109, 110, 111]
  0: !!! [UB] Number of physical nodes: 16
  0: !!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
  0: !!! [UB] Create Userbuffers Communicator
  0: UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
  0: MC initialized succesfully, window size = 549755813888
  0: !!! [UBP2P] Register UBuf 1
  0: !!! [UBP2P] Register UBuf 2
  0: !!! [UBP2P] Register UBuf 3
  0: !!! [UBP2P] Register UBuf 4
  0: !!! [UBP2P] Register UBuf 5
  0: !!! [UB] Register UBuf 6
  0: !!! [UB] Register UBuf 7
  0: !!! [UB] Register UBuf 8
  0: !!! [UB] Register UBuf 9
  0: !!! [UB] Register UBuf 10
 40: NCCL version 2.22.3+cuda12.6
 76: NCCL version 2.22.3+cuda12.6
 32: NCCL version 2.22.3+cuda12.6
116: NCCL version 2.22.3+cuda12.6
112: NCCL version 2.22.3+cuda12.6
 96: NCCL version 2.22.3+cuda12.6
 88: NCCL version 2.22.3+cuda12.6
 92: NCCL version 2.22.3+cuda12.6
 84: NCCL version 2.22.3+cuda12.6
100: NCCL version 2.22.3+cuda12.6
108: NCCL version 2.22.3+cuda12.6
 16: NCCL version 2.22.3+cuda12.6
 12: NCCL version 2.22.3+cuda12.6
 60: NCCL version 2.22.3+cuda12.6
 68: NCCL version 2.22.3+cuda12.6
 28: NCCL version 2.22.3+cuda12.6
124: NCCL version 2.22.3+cuda12.6
 24: NCCL version 2.22.3+cuda12.6
 64: NCCL version 2.22.3+cuda12.6
 44: NCCL version 2.22.3+cuda12.6
120: NCCL version 2.22.3+cuda12.6
  4: NCCL version 2.22.3+cuda12.6
  8: NCCL version 2.22.3+cuda12.6
 80: NCCL version 2.22.3+cuda12.6
 56: NCCL version 2.22.3+cuda12.6
 36: NCCL version 2.22.3+cuda12.6
 72: NCCL version 2.22.3+cuda12.6
104: NCCL version 2.22.3+cuda12.6
 20: NCCL version 2.22.3+cuda12.6
 52: NCCL version 2.22.3+cuda12.6
 48: NCCL version 2.22.3+cuda12.6
 33: NCCL version 2.22.3+cuda12.6
 34: NCCL version 2.22.3+cuda12.6
 35: NCCL version 2.22.3+cuda12.6
 89: NCCL version 2.22.3+cuda12.6
 90: NCCL version 2.22.3+cuda12.6
 91: NCCL version 2.22.3+cuda12.6
 18: NCCL version 2.22.3+cuda12.6
 17: NCCL version 2.22.3+cuda12.6
 41: NCCL version 2.22.3+cuda12.6
 42: NCCL version 2.22.3+cuda12.6
 43: NCCL version 2.22.3+cuda12.6
 19: NCCL version 2.22.3+cuda12.6
105: NCCL version 2.22.3+cuda12.6
106: NCCL version 2.22.3+cuda12.6
107: NCCL version 2.22.3+cuda12.6
 25: NCCL version 2.22.3+cuda12.6
 26: NCCL version 2.22.3+cuda12.6
 27: NCCL version 2.22.3+cuda12.6
 81: NCCL version 2.22.3+cuda12.6
 82: NCCL version 2.22.3+cuda12.6
 83: NCCL version 2.22.3+cuda12.6
114: NCCL version 2.22.3+cuda12.6
113: NCCL version 2.22.3+cuda12.6
115: NCCL version 2.22.3+cuda12.6
121: NCCL version 2.22.3+cuda12.6
122: NCCL version 2.22.3+cuda12.6
123: NCCL version 2.22.3+cuda12.6
 57: NCCL version 2.22.3+cuda12.6
 58: NCCL version 2.22.3+cuda12.6
 74: NCCL version 2.22.3+cuda12.6
 59: NCCL version 2.22.3+cuda12.6
 75: NCCL version 2.22.3+cuda12.6
 73: NCCL version 2.22.3+cuda12.6
 98: NCCL version 2.22.3+cuda12.6
 97: NCCL version 2.22.3+cuda12.6
 99: NCCL version 2.22.3+cuda12.6
 65: NCCL version 2.22.3+cuda12.6
 10: NCCL version 2.22.3+cuda12.6
 11: NCCL version 2.22.3+cuda12.6
 66: NCCL version 2.22.3+cuda12.6
 67: NCCL version 2.22.3+cuda12.6
  9: NCCL version 2.22.3+cuda12.6
 49: NCCL version 2.22.3+cuda12.6
 51: NCCL version 2.22.3+cuda12.6
 50: NCCL version 2.22.3+cuda12.6
  5: NCCL version 2.22.3+cuda12.6
  6: NCCL version 2.22.3+cuda12.6
  7: NCCL version 2.22.3+cuda12.6
  0: :::MLLOG {"namespace": "", "time_ms": 1728616598633, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616598633, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616598634, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616604147, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.2534894943237305, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.0003499176480626913}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616609663, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.430848479270935, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.0003496706697575261}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616615184, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3928241729736328, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00034925929753184046}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616620716, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3543956279754639, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00034868391855477426}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616626267, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3486911058425903, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.00034794507435288117}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616631828, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3697267770767212, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.00034704346030046284}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616637386, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3338193893432617, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.000345979924965107}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616642935, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2806984186172485, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003447554693090452}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616648494, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3146053552627563, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.0003433712457470823}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616654036, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3634521961212158, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003418285570619839}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616659603, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.294734239578247, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.00034012885517834305}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616665163, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.261666178703308, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003382737397960793}}
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: :::MLLOG {"namespace": "", "time_ms": 1728616674913, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.873331880463716}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616674913, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616674913, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616680486, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9384735822677612, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616680486, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616680486, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616686006, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2688376903533936, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00033626495688485734}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616691548, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3046075105667114, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0003341043970408414}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616693771, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.932764100461323}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616693771, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616693771, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616698980, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9353367686271667, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616698980, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616698980, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616702313, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3138060569763184, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.00033179409370733237}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616707870, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2848929166793823, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0003293362212609621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616712312, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.829367703720358}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616712312, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616712312, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616717487, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9316672682762146, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616717487, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616717488, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616718592, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3067920207977295, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.00032673309296524624}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616724132, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2997949123382568, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.0003239871587934214}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616729674, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3108335733413696, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.0003211010031226165}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616730788, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.89695970361061}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616730789, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616730789, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616735913, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9274531602859497, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616735914, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616735914, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616740352, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.332336187362671, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3200, "lr": 0.0003180773423015271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616745905, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3004454374313354, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3360, "lr": 0.00031491902209388335}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616749241, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.84009593258125}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616749241, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616749241, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3456}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616754408, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.928163468837738, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616754409, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616754409, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616756628, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3019912242889404, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3520, "lr": 0.0003116290150001165}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616762178, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.296109676361084, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3680, "lr": 0.0003082104174597458}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616767734, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3176442384719849, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3840, "lr": 0.00030466644693711784}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616767740, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.831290062037084}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616767740, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616767740, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3840}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616772934, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9288249015808105, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616772934, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616772934, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616778483, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2420361042022705, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 4000, "lr": 0.0003010004388932418}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616784041, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.261012315750122, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 4160, "lr": 0.0002972158436465703}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616786265, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.831741143779386}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 4224}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616786265, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 4224}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616786265, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 4224}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616791331, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9257702231407166, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 4224}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616791331, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 4224}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616791331, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 4224}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616794649, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2470186948776245, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 4320, "lr": 0.00029331622312568027}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616800196, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2877720594406128, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 4480, "lr": 0.0002893052475169109}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616804638, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.883430152254967}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 4608}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616804638, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 4608}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616804639, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 4608}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728616809779, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9248954653739929, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 4608}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616809779, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 4608}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728616809779, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9248954653739929, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 321, "samples_count": 4608, "status": "success"}}
 63: [rank63]:[W1011 03:20:13.504779491 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 54: [rank54]:[W1011 03:20:13.788870033 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 30: [rank30]:[W1011 03:20:14.360511725 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
125: [rank125]:[W1011 03:20:14.057322174 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 62: [rank62]:[W1011 03:20:14.564639586 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 78: [rank78]:[W1011 03:20:14.373109955 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
100: [rank100]:[W1011 03:20:14.020914796 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  9: [rank9]:[W1011 03:20:14.042245199 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
110: [rank110]:[W1011 03:20:14.842935306 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 86: [rank86]:[W1011 03:20:14.017878872 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 92: [rank92]:[W1011 03:20:14.433164025 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 41: [rank41]:[W1011 03:20:14.494750120 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 60: [rank60]:[W1011 03:20:14.592324425 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  6: [rank6]:[W1011 03:20:14.572839469 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  7: [rank7]:[W1011 03:20:14.573231155 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  5: [rank5]:[W1011 03:20:14.575221572 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
126: [rank126]:[W1011 03:20:14.114852975 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 29: [rank29]:[W1011 03:20:14.438971864 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 21: [rank21]:[W1011 03:20:14.086943815 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  0: [rank0]:[W1011 03:20:14.585527824 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
124: [rank124]:[W1011 03:20:14.118242462 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 52: [rank52]:[W1011 03:20:14.872784892 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 17: [rank17]:[W1011 03:20:14.088867535 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  1: [rank1]:[W1011 03:20:14.587333060 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  4: [rank4]:[W1011 03:20:14.590358271 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 22: [rank22]:[W1011 03:20:14.094181313 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
127: [rank127]:[W1011 03:20:14.125710184 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 23: [rank23]:[W1011 03:20:14.094952581 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 31: [rank31]:[W1011 03:20:14.447079918 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  3: [rank3]:[W1011 03:20:14.594445157 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 61: [rank61]:[W1011 03:20:14.627602937 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 84: [rank84]:[W1011 03:20:14.061481199 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 76: [rank76]:[W1011 03:20:14.426642514 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 20: [rank20]:[W1011 03:20:14.097770635 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 25: [rank25]:[W1011 03:20:14.450925173 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 33: [rank33]:[W1011 03:20:14.614488227 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 94: [rank94]:[W1011 03:20:14.480097198 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 39: [rank39]:[W1011 03:20:14.620461351 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 53: [rank53]:[W1011 03:20:14.892388308 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 13: [rank13]:[W1011 03:20:14.100739412 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  8: [rank8]:[W1011 03:20:14.100848684 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 45: [rank45]:[W1011 03:20:14.543577677 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 65: [rank65]:[W1011 03:20:14.076681811 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 49: [rank49]:[W1011 03:20:14.894380487 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 11: [rank11]:[W1011 03:20:14.103674500 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  2: [rank2]:[W1011 03:20:14.613359177 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 15: [rank15]:[W1011 03:20:14.108840489 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 47: [rank47]:[W1011 03:20:14.551446588 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 69: [rank69]:[W1011 03:20:14.084985958 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 28: [rank28]:[W1011 03:20:14.469822674 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
108: [rank108]:[W1011 03:20:14.906931315 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 24: [rank24]:[W1011 03:20:14.470960983 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 32: [rank32]:[W1011 03:20:14.634234687 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 57: [rank57]:[W1011 03:20:14.649671041 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 81: [rank81]:[W1011 03:20:14.083024314 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 73: [rank73]:[W1011 03:20:14.448119710 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 27: [rank27]:[W1011 03:20:14.472209159 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 19: [rank19]:[W1011 03:20:14.120235125 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
118: [rank118]:[W1011 03:20:14.790881610 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 46: [rank46]:[W1011 03:20:14.560617217 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 14: [rank14]:[W1011 03:20:14.118117348 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 55: [rank55]:[W1011 03:20:14.910427699 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 10: [rank10]:[W1011 03:20:14.119224037 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 77: [rank77]:[W1011 03:20:14.456334248 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 12: [rank12]:[W1011 03:20:14.121523368 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 36: [rank36]:[W1011 03:20:14.643349170 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 89: [rank89]:[W1011 03:20:14.505028485 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 43: [rank43]:[W1011 03:20:14.566358152 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 35: [rank35]:[W1011 03:20:14.645599862 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 64: [rank64]:[W1011 03:20:14.098971745 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
102: [rank102]:[W1011 03:20:14.108328897 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 79: [rank79]:[W1011 03:20:14.464222363 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 71: [rank71]:[W1011 03:20:14.102968816 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 26: [rank26]:[W1011 03:20:14.487521651 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 93: [rank93]:[W1011 03:20:14.513239104 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
116: [rank116]:[W1011 03:20:14.804498893 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 44: [rank44]:[W1011 03:20:14.574143134 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 68: [rank68]:[W1011 03:20:14.105847350 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 48: [rank48]:[W1011 03:20:14.924262084 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 40: [rank40]:[W1011 03:20:14.575168194 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 97: [rank97]:[W1011 03:20:14.113059215 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 56: [rank56]:[W1011 03:20:14.672128022 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 18: [rank18]:[W1011 03:20:14.141563590 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 70: [rank70]:[W1011 03:20:14.112480641 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 38: [rank38]:[W1011 03:20:14.660051235 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
101: [rank101]:[W1011 03:20:14.121305793 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 16: [rank16]:[W1011 03:20:14.147481763 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 59: [rank59]:[W1011 03:20:14.679275803 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 67: [rank67]:[W1011 03:20:14.116473512 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 51: [rank51]:[W1011 03:20:14.934018640 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
105: [rank105]:[W1011 03:20:14.938385515 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 72: [rank72]:[W1011 03:20:14.480550676 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 80: [rank80]:[W1011 03:20:14.115474210 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 34: [rank34]:[W1011 03:20:14.669166638 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 95: [rank95]:[W1011 03:20:14.531188552 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
103: [rank103]:[W1011 03:20:14.129251189 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 66: [rank66]:[W1011 03:20:14.123143638 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 74: [rank74]:[W1011 03:20:14.484436210 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
111: [rank111]:[W1011 03:20:14.944575056 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 50: [rank50]:[W1011 03:20:14.940731649 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
119: [rank119]:[W1011 03:20:14.822044357 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 42: [rank42]:[W1011 03:20:14.591726871 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 58: [rank58]:[W1011 03:20:14.686363834 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
109: [rank109]:[W1011 03:20:14.946563109 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
117: [rank117]:[W1011 03:20:14.824015201 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 85: [rank85]:[W1011 03:20:14.121573077 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 37: [rank37]:[W1011 03:20:14.672933239 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 75: [rank75]:[W1011 03:20:14.487662711 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 83: [rank83]:[W1011 03:20:14.122643496 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
121: [rank121]:[W1011 03:20:14.190074449 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
113: [rank113]:[W1011 03:20:14.825717246 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
122: [rank122]:[W1011 03:20:14.196210945 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 82: [rank82]:[W1011 03:20:14.129393419 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 87: [rank87]:[W1011 03:20:14.129497577 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
123: [rank123]:[W1011 03:20:14.199490005 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 91: [rank91]:[W1011 03:20:14.544619476 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 99: [rank99]:[W1011 03:20:14.142595700 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 88: [rank88]:[W1011 03:20:14.547320069 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 90: [rank90]:[W1011 03:20:14.551348762 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
107: [rank107]:[W1011 03:20:14.967861454 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
120: [rank120]:[W1011 03:20:14.212249203 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 96: [rank96]:[W1011 03:20:14.155333126 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
104: [rank104]:[W1011 03:20:14.970572905 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
112: [rank112]:[W1011 03:20:14.847959283 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 98: [rank98]:[W1011 03:20:14.159364896 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
115: [rank115]:[W1011 03:20:14.855354252 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
106: [rank106]:[W1011 03:20:14.984729852 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
114: [rank114]:[W1011 03:20:14.872099151 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 3, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 3, retcode 3
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 2, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 2, retcode 3
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 1, retcode 3
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 1, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 1, retcode 3
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 4, retcode 3
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 4, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 4, retcode 3
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 4, retcode 3
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 4, retcode 3
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 6, retcode 3
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 6, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 6, retcode 3
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 6, retcode 3
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 6, retcode 3
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 24, retcode 3
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 16, retcode 3
  5: 
  5: GPU-94:446555:449012 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  7: 
  7: GPU-94:446556:449010 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:446521:449015 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:446521:449015 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 5, retcode 3
  0: 
  0: GPU-94:446521:449015 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 5, retcode 3
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 5, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 5, retcode 3
  0: 
  0: GPU-94:446521:449015 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  0: 
  0: GPU-94:446521:449015 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 0, retcode 3
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 7, retcode 3
  0: 
  0: GPU-94:446521:449015 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  0: 
  0: GPU-94:446521:449015 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 7, retcode 3
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 5, retcode 3
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  1: 
  1: GPU-94:446612:449008 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 0, retcode 3
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 7, retcode 3
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  2: 
  2: GPU-94:446547:449007 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 0, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 0, retcode 3
  5: 
  5: GPU-94:446555:449012 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  5: 
  5: GPU-94:446555:449012 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 5, retcode 3
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  3: 
  3: GPU-94:446601:449013 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 7, retcode 3
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 0, retcode 3
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 5, retcode 3
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  4: 
  4: GPU-94:446503:449009 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 7, retcode 3
  5: 
  5: GPU-94:446555:449012 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  5: 
  5: GPU-94:446555:449012 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 7, retcode 3
  7: 
  7: GPU-94:446556:449010 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  7: 
  7: GPU-94:446556:449010 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 5, retcode 3
  5: 
  5: GPU-94:446555:449012 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  5: 
  5: GPU-94:446555:449012 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 0, retcode 3
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 7, retcode 3
  7: 
  7: GPU-94:446556:449010 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  7: 
  7: GPU-94:446556:449010 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 7, retcode 3
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  6: 
  6: GPU-94:446591:449011 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 0, retcode 3
  7: 
  7: GPU-94:446556:449010 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  7: 
  7: GPU-94:446556:449010 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 0, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 11, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 11, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 10, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 10, retcode 3
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 9, retcode 3
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 8, retcode 3
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 9, retcode 3
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 8, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 9, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 8, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 9, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 8, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 20, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 20, retcode 3
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 12, retcode 3
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 12, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 12, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 12, retcode 3
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 12, retcode 3
 14: 
 14: GPU-180:1690400:1692837 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 14, retcode 3
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 14, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 14, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 14, retcode 3
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 14, retcode 3
 14: 
 14: GPU-180:1690400:1692837 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 14: 
 14: GPU-180:1690400:1692837 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 14, retcode 3
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 19, retcode 3
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 22, retcode 3
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 18, retcode 3
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 19, retcode 3
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 22, retcode 3
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 18, retcode 3
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 19, retcode 3
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 18, retcode 3
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 22, retcode 3
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 18, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 19, retcode 3
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 22, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 18, retcode 3
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 22, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 22, retcode 3
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 19, retcode 3
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 17, retcode 3
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 18, retcode 3
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 17, retcode 3
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 17, retcode 3
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 17, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 17, retcode 3
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 17, retcode 3
 13: 
 13: GPU-180:1690438:1692835 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 13, retcode 3
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 13, retcode 3
 15: 
 15: GPU-180:1690487:1692834 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 13, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 13, retcode 3
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  8: 
  8: GPU-180:1690443:1692838 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 15, retcode 3
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 13, retcode 3
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  9: 
  9: GPU-180:1690416:1692841 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 15, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 13: 
 13: GPU-180:1690438:1692835 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 13: 
 13: GPU-180:1690438:1692835 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 13, retcode 3
 10: 
 10: GPU-180:1690512:1692842 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 15, retcode 3
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 11: 
 11: GPU-180:1690488:1692836 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 15, retcode 3
 14: 
 14: GPU-180:1690400:1692837 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 14: 
 14: GPU-180:1690400:1692837 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 13, retcode 3
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 12: 
 12: GPU-180:1690464:1692839 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 15, retcode 3
 15: 
 15: GPU-180:1690487:1692834 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 15: 
 15: GPU-180:1690487:1692834 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 13, retcode 3
 13: 
 13: GPU-180:1690438:1692835 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 13: 
 13: GPU-180:1690438:1692835 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 15, retcode 3
 14: 
 14: GPU-180:1690400:1692837 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 14: 
 14: GPU-180:1690400:1692837 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 15, retcode 3
 15: 
 15: GPU-180:1690487:1692834 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 15: 
 15: GPU-180:1690487:1692834 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 15, retcode 3
 21: 
 21: GPU-146:1187098:1188927 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 23: 
 23: GPU-146:1187155:1188923 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 23, retcode 3
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 16: 
 16: GPU-146:1187123:1188937 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 21, retcode 3
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 23, retcode 3
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 17: 
 17: GPU-146:1187203:1188934 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 21, retcode 3
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 23, retcode 3
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 18: 
 18: GPU-146:1187178:1188933 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 21, retcode 3
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 23, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 21, retcode 3
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 19: 
 19: GPU-146:1187179:1188931 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 21, retcode 3
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 20: 
 20: GPU-146:1187099:1188929 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 23, retcode 3
 21: 
 21: GPU-146:1187098:1188927 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 21: 
 21: GPU-146:1187098:1188927 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 21, retcode 3
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 21, retcode 3
 21: 
 21: GPU-146:1187098:1188927 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 21: 
 21: GPU-146:1187098:1188927 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 23, retcode 3
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 22: 
 22: GPU-146:1187139:1188925 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 23, retcode 3
 23: 
 23: GPU-146:1187155:1188923 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 23: 
 23: GPU-146:1187155:1188923 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 23, retcode 3
 23: 
 23: GPU-146:1187155:1188923 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 23: 
 23: GPU-146:1187155:1188923 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 21, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 27, retcode 3
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 26, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 27, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 26, retcode 3
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 27, retcode 3
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 26, retcode 3
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 25, retcode 3
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 25, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 25, retcode 3
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 25, retcode 3
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 34, retcode 3
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 32, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 34, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 32, retcode 3
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 40, retcode 3
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 35, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 35, retcode 3
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 35, retcode 3
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 28, retcode 3
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 29, retcode 3
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 28, retcode 3
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 29, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 28, retcode 3
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 28, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 29, retcode 3
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 29, retcode 3
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 29, retcode 3
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 28, retcode 3
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 29, retcode 3
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 28, retcode 3
 30: 
 30: GPU-457:1174548:1176322 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 30, retcode 3
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 30, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 30, retcode 3
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 30, retcode 3
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 30, retcode 3
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 30, retcode 3
 30: 
 30: GPU-457:1174548:1176322 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 30: 
 30: GPU-457:1174548:1176322 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 30, retcode 3
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 33, retcode 3
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 33, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 33, retcode 3
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 33, retcode 3
 31: 
 31: GPU-457:1174532:1176323 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 24: 
 24: GPU-457:1174612:1176331 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 31, retcode 3
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 25: 
 25: GPU-457:1174634:1176333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 31, retcode 3
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 26: 
 26: GPU-457:1174564:1176328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 31, retcode 3
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 27: 
 27: GPU-457:1174592:1176332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 31, retcode 3
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 28: 
 28: GPU-457:1174583:1176325 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 31, retcode 3
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 29: 
 29: GPU-457:1174639:1176324 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 31, retcode 3
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 30: 
 30: GPU-457:1174548:1176322 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 30: 
 30: GPU-457:1174548:1176322 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 31, retcode 3
 31: 
 31: GPU-457:1174532:1176323 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 31: 
 31: GPU-457:1174532:1176323 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 31, retcode 3
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 48, retcode 3
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 36, retcode 3
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 36, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 36, retcode 3
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 36, retcode 3
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 36, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 42, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 42, retcode 3
 38: 
 38: GPU-993:1169983:1171995 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 39: 
 39: GPU-993:1170026:1171994 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 38, retcode 3
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 38, retcode 3
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 39, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 38, retcode 3
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 39, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 39, retcode 3
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 38, retcode 3
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 38, retcode 3
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 39, retcode 3
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 39, retcode 3
 38: 
 38: GPU-993:1169983:1171995 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 38: 
 38: GPU-993:1169983:1171995 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 38, retcode 3
 38: 
 38: GPU-993:1169983:1171995 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 38: 
 38: GPU-993:1169983:1171995 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 39, retcode 3
 39: 
 39: GPU-993:1170026:1171994 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 39: 
 39: GPU-993:1170026:1171994 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 39, retcode 3
 39: 
 39: GPU-993:1170026:1171994 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 39: 
 39: GPU-993:1170026:1171994 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 38, retcode 3
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 56, retcode 3
 37: 
 37: GPU-993:1170072:1171996 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 32: 
 32: GPU-993:1170074:1172003 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 37, retcode 3
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 33: 
 33: GPU-993:1170042:1172004 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 37, retcode 3
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 34: 
 34: GPU-993:1170073:1172008 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 37, retcode 3
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 35: 
 35: GPU-993:1169989:1172002 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 37, retcode 3
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 36: 
 36: GPU-993:1170010:1171999 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 37, retcode 3
 37: 
 37: GPU-993:1170072:1171996 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 37: 
 37: GPU-993:1170072:1171996 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 37, retcode 3
 38: 
 38: GPU-993:1169983:1171995 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 38: 
 38: GPU-993:1169983:1171995 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 37, retcode 3
 39: 
 39: GPU-993:1170026:1171994 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 39: 
 39: GPU-993:1170026:1171994 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 37, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 50, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 50, retcode 3
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 44, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 44, retcode 3
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 44, retcode 3
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 44, retcode 3
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 43, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 43, retcode 3
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 43, retcode 3
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 43, retcode 3
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 41, retcode 3
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 41, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 41, retcode 3
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 41, retcode 3
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 41, retcode 3
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 51, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 51, retcode 3
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 51, retcode 3
 45: 
 45: GPU-130:3985657:3987411 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 45, retcode 3
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 45, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 45, retcode 3
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 45, retcode 3
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 45, retcode 3
 45: 
 45: GPU-130:3985657:3987411 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 45: 
 45: GPU-130:3985657:3987411 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 45, retcode 3
 46: 
 46: GPU-130:3985584:3987413 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 46, retcode 3
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 46, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 46, retcode 3
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 46, retcode 3
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 46, retcode 3
 45: 
 45: GPU-130:3985657:3987411 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 45: 
 45: GPU-130:3985657:3987411 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 46, retcode 3
 46: 
 46: GPU-130:3985584:3987413 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 46: 
 46: GPU-130:3985584:3987413 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 46, retcode 3
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 49, retcode 3
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 49, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 49, retcode 3
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 49, retcode 3
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 52, retcode 3
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 52, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 52, retcode 3
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 52, retcode 3
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 52, retcode 3
 47: 
 47: GPU-130:3985609:3987412 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 40: 
 40: GPU-130:3985606:3987420 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 47, retcode 3
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 41: 
 41: GPU-130:3985668:3987419 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 47, retcode 3
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 42: 
 42: GPU-130:3985666:3987421 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 47, retcode 3
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 43: 
 43: GPU-130:3985632:3987422 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 47, retcode 3
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 44: 
 44: GPU-130:3985568:3987414 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 47, retcode 3
 45: 
 45: GPU-130:3985657:3987411 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 45: 
 45: GPU-130:3985657:3987411 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 47, retcode 3
 46: 
 46: GPU-130:3985584:3987413 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 46: 
 46: GPU-130:3985584:3987413 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 47, retcode 3
 47: 
 47: GPU-130:3985609:3987412 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 47: 
 47: GPU-130:3985609:3987412 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 47, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 58, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 58, retcode 3
 53: 
 53: GPU-881:1167044:1169434 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 53, retcode 3
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 53, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 53, retcode 3
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 53, retcode 3
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 53, retcode 3
 53: 
 53: GPU-881:1167044:1169434 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 53: 
 53: GPU-881:1167044:1169434 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 53, retcode 3
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 64, retcode 3
 54: 
 54: GPU-881:1166989:1169436 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 54, retcode 3
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 54, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 54, retcode 3
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 54, retcode 3
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 54, retcode 3
 53: 
 53: GPU-881:1167044:1169434 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 53: 
 53: GPU-881:1167044:1169434 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 54, retcode 3
 54: 
 54: GPU-881:1166989:1169436 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 54: 
 54: GPU-881:1166989:1169436 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 54, retcode 3
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 59, retcode 3
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 60, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 59, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 60, retcode 3
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 59, retcode 3
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 60, retcode 3
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 60, retcode 3
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 59, retcode 3
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 57, retcode 3
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 57, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 57, retcode 3
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 57, retcode 3
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 57, retcode 3
 55: 
 55: GPU-881:1167085:1169435 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 48: 
 48: GPU-881:1167101:1169426 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 55, retcode 3
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 49: 
 49: GPU-881:1167005:1169428 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 55, retcode 3
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 50: 
 50: GPU-881:1167069:1169427 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 55, retcode 3
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 51: 
 51: GPU-881:1167021:1169432 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 55, retcode 3
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 52: 
 52: GPU-881:1167045:1169433 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 55, retcode 3
 53: 
 53: GPU-881:1167044:1169434 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 53: 
 53: GPU-881:1167044:1169434 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 55, retcode 3
 54: 
 54: GPU-881:1166989:1169436 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 54: 
 54: GPU-881:1166989:1169436 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 55, retcode 3
 55: 
 55: GPU-881:1167085:1169435 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 55: 
 55: GPU-881:1167085:1169435 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 55, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 67, retcode 3
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 66, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 66, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 67, retcode 3
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 67, retcode 3
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 66, retcode 3
 62: 
 62: GPU-351:1169047:1170843 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 63: 
 63: GPU-351:1169111:1170842 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 62, retcode 3
 61: 
 61: GPU-351:1169087:1170839 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 63, retcode 3
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 63, retcode 3
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 56: 
 56: GPU-351:1169145:1170841 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 61, retcode 3
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 62, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 63, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 62, retcode 3
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 57: 
 57: GPU-351:1169139:1170840 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 61, retcode 3
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 62, retcode 3
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 63, retcode 3
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 63, retcode 3
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 58: 
 58: GPU-351:1169063:1170844 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 61, retcode 3
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 62, retcode 3
 61: 
 61: GPU-351:1169087:1170839 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 61: 
 61: GPU-351:1169087:1170839 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 63, retcode 3
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 59: 
 59: GPU-351:1169086:1170845 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 61, retcode 3
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 60: 
 60: GPU-351:1169143:1170838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 61, retcode 3
 61: 
 61: GPU-351:1169087:1170839 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 61: 
 61: GPU-351:1169087:1170839 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 62, retcode 3
 62: 
 62: GPU-351:1169047:1170843 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 62: 
 62: GPU-351:1169047:1170843 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 62, retcode 3
 61: 
 61: GPU-351:1169087:1170839 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 61: 
 61: GPU-351:1169087:1170839 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 61, retcode 3
 62: 
 62: GPU-351:1169047:1170843 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 62: 
 62: GPU-351:1169047:1170843 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 63, retcode 3
 63: 
 63: GPU-351:1169111:1170842 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 63: 
 63: GPU-351:1169111:1170842 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 63, retcode 3
 62: 
 62: GPU-351:1169047:1170843 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 62: 
 62: GPU-351:1169047:1170843 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 61, retcode 3
 63: 
 63: GPU-351:1169111:1170842 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 63: 
 63: GPU-351:1169111:1170842 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 62, retcode 3
 63: 
 63: GPU-351:1169111:1170842 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 63: 
 63: GPU-351:1169111:1170842 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 61, retcode 3
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 68, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 68, retcode 3
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 68, retcode 3
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 68, retcode 3
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 72, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 73, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 73, retcode 3
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 70, retcode 3
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 71, retcode 3
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 69, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 70, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 71, retcode 3
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 70, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 69, retcode 3
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 71, retcode 3
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 70, retcode 3
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 69, retcode 3
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 71, retcode 3
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 70, retcode 3
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 69, retcode 3
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 71, retcode 3
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 70, retcode 3
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 71, retcode 3
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 71, retcode 3
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 70, retcode 3
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 69, retcode 3
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 69, retcode 3
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 69, retcode 3
 65: 
 65: GPU-338:1166135:1167835 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 64: 
 64: GPU-338:1166136:1167834 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 65, retcode 3
 65: 
 65: GPU-338:1166135:1167835 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 65: 
 65: GPU-338:1166135:1167835 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 65, retcode 3
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 66: 
 66: GPU-338:1166058:1167836 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 65, retcode 3
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 67: 
 67: GPU-338:1166087:1167839 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 65, retcode 3
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 68: 
 68: GPU-338:1166134:1167838 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 65, retcode 3
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 69: 
 69: GPU-338:1166057:1167840 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 65, retcode 3
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 70: 
 70: GPU-338:1166146:1167844 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 65, retcode 3
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 71: 
 71: GPU-338:1166093:1167843 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 65, retcode 3
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 74, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 74, retcode 3
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 74, retcode 3
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 80, retcode 3
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 82, retcode 3
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 82, retcode 3
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 81, retcode 3
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 81, retcode 3
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 81, retcode 3
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 88, retcode 3
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 76, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 76, retcode 3
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 76, retcode 3
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 76, retcode 3
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 75, retcode 3
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 76, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 75, retcode 3
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 76, retcode 3
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 75, retcode 3
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 76, retcode 3
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 77, retcode 3
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 75, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 77, retcode 3
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 75, retcode 3
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 77, retcode 3
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 75, retcode 3
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 77, retcode 3
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 77, retcode 3
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 75, retcode 3
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 78, retcode 3
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 77, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 78, retcode 3
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 77, retcode 3
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 78, retcode 3
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 78, retcode 3
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 78, retcode 3
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 78, retcode 3
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 78, retcode 3
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 83, retcode 3
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 83, retcode 3
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 83, retcode 3
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 83, retcode 3
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 84, retcode 3
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 83, retcode 3
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 84, retcode 3
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 84, retcode 3
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 84, retcode 3
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 84, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 89, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 89, retcode 3
 79: 
 79: GPU-8:4524:6704 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 72: 
 72: GPU-8:4457:6698 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 79, retcode 3
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 73: 
 73: GPU-8:4473:6699 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 79, retcode 3
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 74: 
 74: GPU-8:4432:6697 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 79, retcode 3
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 75: 
 75: GPU-8:4489:6700 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 79, retcode 3
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 76: 
 76: GPU-8:4507:6701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 79, retcode 3
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 77: 
 77: GPU-8:4558:6703 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 79, retcode 3
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 78: 
 78: GPU-8:4557:6702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 79, retcode 3
 79: 
 79: GPU-8:4524:6704 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 79: 
 79: GPU-8:4524:6704 [7] proxy.cc:1521 NCCL WARN [Proxy Service 79] Failed to execute operation Close from rank 79, retcode 3
 86: 
 86: GPU-342:1167259:1169387 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 87: 
 87: GPU-342:1167187:1169386 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 86, retcode 3
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 87, retcode 3
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 86, retcode 3
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 87, retcode 3
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 86, retcode 3
 85: 
 85: GPU-342:1167295:1169385 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 87, retcode 3
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 86, retcode 3
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 80: 
 80: GPU-342:1167226:1169379 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 85, retcode 3
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 86, retcode 3
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 87, retcode 3
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 81: 
 81: GPU-342:1167227:1169377 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 85, retcode 3
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 87, retcode 3
 85: 
 85: GPU-342:1167295:1169385 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 85: 
 85: GPU-342:1167295:1169385 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 86, retcode 3
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 82: 
 82: GPU-342:1167258:1169378 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 85, retcode 3
 85: 
 85: GPU-342:1167295:1169385 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 85: 
 85: GPU-342:1167295:1169385 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 87, retcode 3
 86: 
 86: GPU-342:1167259:1169387 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 86: 
 86: GPU-342:1167259:1169387 [6] proxy.cc:1521 NCCL WARN [Proxy Service 86] Failed to execute operation Close from rank 86, retcode 3
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 83: 
 83: GPU-342:1167288:1169383 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 85, retcode 3
 86: 
 86: GPU-342:1167259:1169387 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 86: 
 86: GPU-342:1167259:1169387 [6] proxy.cc:1521 NCCL WARN [Proxy Service 86] Failed to execute operation Close from rank 87, retcode 3
 87: 
 87: GPU-342:1167187:1169386 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 87: 
 87: GPU-342:1167187:1169386 [7] proxy.cc:1521 NCCL WARN [Proxy Service 87] Failed to execute operation Close from rank 87, retcode 3
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 84: 
 84: GPU-342:1167203:1169384 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 85, retcode 3
 87: 
 87: GPU-342:1167187:1169386 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 87: 
 87: GPU-342:1167187:1169386 [7] proxy.cc:1521 NCCL WARN [Proxy Service 87] Failed to execute operation Close from rank 86, retcode 3
 85: 
 85: GPU-342:1167295:1169385 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 85: 
 85: GPU-342:1167295:1169385 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 85, retcode 3
 86: 
 86: GPU-342:1167259:1169387 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 86: 
 86: GPU-342:1167259:1169387 [6] proxy.cc:1521 NCCL WARN [Proxy Service 86] Failed to execute operation Close from rank 85, retcode 3
 87: 
 87: GPU-342:1167187:1169386 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 87: 
 87: GPU-342:1167187:1169386 [7] proxy.cc:1521 NCCL WARN [Proxy Service 87] Failed to execute operation Close from rank 85, retcode 3
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 96, retcode 3
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 97, retcode 3
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 97, retcode 3
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 96, retcode 3
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 104, retcode 3
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 90, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 90, retcode 3
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 90, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 105, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 105, retcode 3
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 98, retcode 3
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 98, retcode 3
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 98, retcode 3
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 112, retcode 3
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 92, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 92, retcode 3
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 92, retcode 3
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 92, retcode 3
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 91, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 91, retcode 3
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 91, retcode 3
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 91, retcode 3
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 91, retcode 3
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 121, retcode 3
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 106, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 106, retcode 3
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 106, retcode 3
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 99, retcode 3
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 102, retcode 3
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 99, retcode 3
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 102, retcode 3
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 99, retcode 3
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 102, retcode 3
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 99, retcode 3
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 100, retcode 3
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 102, retcode 3
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 100, retcode 3
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 102, retcode 3
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 99, retcode 3
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 100, retcode 3
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 100, retcode 3
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 102, retcode 3
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 99, retcode 3
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 100, retcode 3
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 100, retcode 3
 93: 
 93: GPU-234:1175916:1178012 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 93, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 93, retcode 3
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 93, retcode 3
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 93, retcode 3
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 93, retcode 3
 93: 
 93: GPU-234:1175916:1178012 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 93: 
 93: GPU-234:1175916:1178012 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 93, retcode 3
 94: 
 94: GPU-234:1175917:1178014 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 94, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 94, retcode 3
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 94, retcode 3
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 94, retcode 3
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 94, retcode 3
 93: 
 93: GPU-234:1175916:1178012 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 93: 
 93: GPU-234:1175916:1178012 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 94, retcode 3
 94: 
 94: GPU-234:1175917:1178014 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 94: 
 94: GPU-234:1175917:1178014 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 94, retcode 3
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 120, retcode 3
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 120, retcode 3
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 107, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 107, retcode 3
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 107, retcode 3
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 107, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 113, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 113, retcode 3
101: 
101: GPU-245:1173950:1176046 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 101, retcode 3
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 101, retcode 3
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 101, retcode 3
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 101, retcode 3
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 101, retcode 3
101: 
101: GPU-245:1173950:1176046 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
101: 
101: GPU-245:1173950:1176046 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 101, retcode 3
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 101, retcode 3
 95: 
 95: GPU-234:1175893:1178015 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 88: 
 88: GPU-234:1175864:1178008 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 95, retcode 3
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 89: 
 89: GPU-234:1175829:1178010 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 95, retcode 3
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 90: 
 90: GPU-234:1175813:1178011 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 95, retcode 3
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 91: 
 91: GPU-234:1175863:1178009 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 95, retcode 3
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 92: 
 92: GPU-234:1175855:1178013 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 95, retcode 3
 93: 
 93: GPU-234:1175916:1178012 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 93: 
 93: GPU-234:1175916:1178012 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 95, retcode 3
 94: 
 94: GPU-234:1175917:1178014 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 94: 
 94: GPU-234:1175917:1178014 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 95, retcode 3
 95: 
 95: GPU-234:1175893:1178015 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 95: 
 95: GPU-234:1175893:1178015 [7] proxy.cc:1521 NCCL WARN [Proxy Service 95] Failed to execute operation Close from rank 95, retcode 3
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 122, retcode 3
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 122, retcode 3
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 122, retcode 3
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 123, retcode 3
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 123, retcode 3
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 123, retcode 3
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 123, retcode 3
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 109, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 109, retcode 3
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 109, retcode 3
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 109, retcode 3
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 109, retcode 3
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 109, retcode 3
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 108, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 108, retcode 3
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 108, retcode 3
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 108, retcode 3
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 108, retcode 3
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 108, retcode 3
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 117, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 117, retcode 3
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 114, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 114, retcode 3
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 117, retcode 3
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 117, retcode 3
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 115, retcode 3
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 114, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 115, retcode 3
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 115, retcode 3
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 114, retcode 3
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 117, retcode 3
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 114, retcode 3
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 115, retcode 3
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 115, retcode 3
103: 
103: GPU-245:1174071:1176044 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 96: 
 96: GPU-245:1173974:1176048 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 103, retcode 3
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 97: 
 97: GPU-245:1174047:1176047 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 103, retcode 3
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 98: 
 98: GPU-245:1174013:1176049 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 103, retcode 3
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 99: 
 99: GPU-245:1173973:1176050 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 103, retcode 3
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
100: 
100: GPU-245:1174046:1176045 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 103, retcode 3
101: 
101: GPU-245:1173950:1176046 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
101: 
101: GPU-245:1173950:1176046 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 103, retcode 3
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
102: 
102: GPU-245:1174017:1176043 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 103, retcode 3
103: 
103: GPU-245:1174071:1176044 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
103: 
103: GPU-245:1174071:1176044 [7] proxy.cc:1521 NCCL WARN [Proxy Service 103] Failed to execute operation Close from rank 103, retcode 3
110: 
110: GPU-186:1168521:1170313 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
111: 
111: GPU-186:1168560:1170312 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 124, retcode 3
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 110, retcode 3
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 124, retcode 3
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
104: 
104: GPU-186:1168602:1170320 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 111, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 110, retcode 3
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
105: 
105: GPU-186:1168601:1170319 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 111, retcode 3
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 124, retcode 3
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 124, retcode 3
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 110, retcode 3
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 110, retcode 3
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 124, retcode 3
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
106: 
106: GPU-186:1168538:1170317 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 111, retcode 3
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
107: 
107: GPU-186:1168600:1170318 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 111, retcode 3
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 110, retcode 3
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
108: 
108: GPU-186:1168565:1170315 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 111, retcode 3
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 110, retcode 3
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
109: 
109: GPU-186:1168634:1170314 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 111, retcode 3
110: 
110: GPU-186:1168521:1170313 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
110: 
110: GPU-186:1168521:1170313 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 110, retcode 3
110: 
110: GPU-186:1168521:1170313 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
110: 
110: GPU-186:1168521:1170313 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 111, retcode 3
111: 
111: GPU-186:1168560:1170312 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
111: 
111: GPU-186:1168560:1170312 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 110, retcode 3
111: 
111: GPU-186:1168560:1170312 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
111: 
111: GPU-186:1168560:1170312 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 111, retcode 3
116: 
116: GPU-792:1155151:1157133 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
118: 
118: GPU-792:1155224:1157131 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
119: 
119: GPU-792:1155199:1157134 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 116, retcode 3
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 118, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 116, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 118, retcode 3
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 116, retcode 3
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
112: 
112: GPU-792:1155132:1157135 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 119, retcode 3
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
113: 
113: GPU-792:1155160:1157128 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 119, retcode 3
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 118, retcode 3
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 116, retcode 3
116: 
116: GPU-792:1155151:1157133 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
114: 
114: GPU-792:1155180:1157127 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 119, retcode 3
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 118, retcode 3
116: 
116: GPU-792:1155151:1157133 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 118, retcode 3
116: 
116: GPU-792:1155151:1157133 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
116: 
116: GPU-792:1155151:1157133 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 116, retcode 3
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 118, retcode 3
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
115: 
115: GPU-792:1155244:1157129 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 119, retcode 3
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 116, retcode 3
116: 
116: GPU-792:1155151:1157133 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
116: 
116: GPU-792:1155151:1157133 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 119, retcode 3
118: 
118: GPU-792:1155224:1157131 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
118: 
118: GPU-792:1155224:1157131 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 118, retcode 3
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
117: 
117: GPU-792:1155215:1157132 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 119, retcode 3
119: 
119: GPU-792:1155199:1157134 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
119: 
119: GPU-792:1155199:1157134 [7] proxy.cc:1521 NCCL WARN [Proxy Service 119] Failed to execute operation Close from rank 116, retcode 3
118: 
118: GPU-792:1155224:1157131 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
118: 
118: GPU-792:1155224:1157131 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 116, retcode 3
118: 
118: GPU-792:1155224:1157131 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
118: 
118: GPU-792:1155224:1157131 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 119, retcode 3
119: 
119: GPU-792:1155199:1157134 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
119: 
119: GPU-792:1155199:1157134 [7] proxy.cc:1521 NCCL WARN [Proxy Service 119] Failed to execute operation Close from rank 118, retcode 3
119: 
119: GPU-792:1155199:1157134 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
119: 
119: GPU-792:1155199:1157134 [7] proxy.cc:1521 NCCL WARN [Proxy Service 119] Failed to execute operation Close from rank 119, retcode 3
125: 
125: GPU-332:1169216:1170963 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 125, retcode 3
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 125, retcode 3
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 125, retcode 3
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 125, retcode 3
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 125, retcode 3
125: 
125: GPU-332:1169216:1170963 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
125: 
125: GPU-332:1169216:1170963 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 125, retcode 3
126: 
126: GPU-332:1169163:1170962 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
126: 
126: GPU-332:1169163:1170962 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
126: 
126: GPU-332:1169163:1170962 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 125, retcode 3
127: 
127: GPU-332:1169179:1170964 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
127: 
127: GPU-332:1169179:1170964 [7] proxy.cc:1521 NCCL WARN [Proxy Service 127] Failed to execute operation Close from rank 125, retcode 3
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 126, retcode 3
127: 
127: GPU-332:1169179:1170964 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 126, retcode 3
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
120: 
120: GPU-332:1169214:1170954 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 127, retcode 3
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 126, retcode 3
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
121: 
121: GPU-332:1169217:1170955 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 127, retcode 3
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 126, retcode 3
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
122: 
122: GPU-332:1169228:1170956 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 127, retcode 3
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 126, retcode 3
125: 
125: GPU-332:1169216:1170963 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
125: 
125: GPU-332:1169216:1170963 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 126, retcode 3
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
123: 
123: GPU-332:1169266:1170960 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 127, retcode 3
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
124: 
124: GPU-332:1169267:1170961 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 127, retcode 3
126: 
126: GPU-332:1169163:1170962 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
126: 
126: GPU-332:1169163:1170962 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 126, retcode 3
125: 
125: GPU-332:1169216:1170963 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
125: 
125: GPU-332:1169216:1170963 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 127, retcode 3
127: 
127: GPU-332:1169179:1170964 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
127: 
127: GPU-332:1169179:1170964 [7] proxy.cc:1521 NCCL WARN [Proxy Service 127] Failed to execute operation Close from rank 126, retcode 3
126: 
126: GPU-332:1169163:1170962 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
126: 
126: GPU-332:1169163:1170962 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 127, retcode 3
127: 
127: GPU-332:1169179:1170964 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
127: 
127: GPU-332:1169179:1170964 [7] proxy.cc:1521 NCCL WARN [Proxy Service 127] Failed to execute operation Close from rank 127, retcode 3
  0: ENDING TIMING RUN AT 2024-10-11 03:20:23 AM
  0: RESULT,LLM_FINETUNING,,635,nvidia,2024-10-11 03:09:48 AM
++ date +%s
+ echo 'RUNANDTIME_STOP 1728616836'
RUNANDTIME_STOP 1728616836
+ set -e
