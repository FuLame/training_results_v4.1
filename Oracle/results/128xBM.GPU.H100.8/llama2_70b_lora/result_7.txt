+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 243 16 GPU-[94,180,146,457,993,130,881,351,338,8,342,234,245,186,792,332] BM.GPU.H100.8 Cluster DGXH100_16x8x1xtp4pp1cp2'
:::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 243 16 GPU-[94,180,146,457,993,130,881,351,338,8,342,234,245,186,792,332] BM.GPU.H100.8 Cluster DGXH100_16x8x1xtp4pp1cp2
++ srun --ntasks=1 --container-name=llama2_70b_lora_243 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ srun --ntasks=1 --container-name=llama2_70b_lora_243 bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
:::GITCOMMITID  
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on GPU-245
Clearing cache on GPU-332
Clearing cache on GPU-186
Clearing cache on GPU-881
Clearing cache on GPU-234
Clearing cache on GPU-342
Clearing cache on GPU-180
Clearing cache on GPU-457
Clearing cache on GPU-351
Clearing cache on GPU-94
Clearing cache on GPU-338
Clearing cache on GPU-792
Clearing cache on GPU-130
Clearing cache on GPU-993
Clearing cache on GPU-146
Clearing cache on GPU-8
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ export SEED=6678
+ SEED=6678
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1728614675'
RUNANDTIME_START 1728614675
+ srun -l --mpi=pmix --ntasks=128 --ntasks-per-node=8 --time=20 --container-name=llama2_70b_lora_243 --container-mounts=/mnt/orangefs/mlperf/llama/data_model/gov_report:/data:ro,/mnt/orangefs/mlperf/llama/data_model/model:/ckpt:ro,/mnt/orangefs/mlperf/llama/logs:/results:rw --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
  0: STARTING TIMING RUN AT 2024-10-11 02:44:44 AM
110: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
126: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 96: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
103: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 71: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 89: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 97: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 99: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 90: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 91: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 67: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 92: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
114: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 93: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 74: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 75: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
113: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
117: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 86: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 83: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
121: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 77: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
106: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
109: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
100: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
127: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
102: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
101: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 98: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
104: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
123: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
120: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
125: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
122: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
124: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 72: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
108: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 94: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 88: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 95: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
107: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
105: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
111: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
115: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
118: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
116: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
119: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
112: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 76: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 73: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 78: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 79: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 68: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 64: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 69: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 65: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 66: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 70: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 84: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 81: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 82: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 80: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 85: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 87: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: 
  0: 
  0: ************** Experiment configuration ***********
  0: 
  0: model:
  0:   ub_tp_comm_overlap_cfg:
  0:     qkv_fprop:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     fc1_fprop:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     proj_dgrad:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     fc2_dgrad:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     proj_fprop:
  0:       method: pipeline
  0:       num_sm: 32
  0:       cga_size: 2
  0:       num_splits: 4
  0:       set_sm_margin: 1
  0:       atomic_gemm: 1
  0:       fp8_buf: 1
  0:     fc2_fprop:
  0:       method: pipeline
  0:       num_sm: 16
  0:       cga_size: 2
  0:       num_splits: 4
  0:       set_sm_margin: 1
  0:       atomic_gemm: 1
  0:       fp8_buf: 0
  0:     qkv_dgrad:
  0:       method: bulk
  0:       num_sm: 4
  0:       cga_size: 2
  0:       set_sm_margin: 0
  0:     fc1_dgrad:
  0:       method: ring_exchange
  0:       num_sm: 1
  0:       cga_size: 2
  0:       set_sm_margin: 1
  0:       atomic_gemm: 0
  0:       fp8_buf: 0
  0:   mcore_gpt: true
  0:   seed: 6678
  0:   tensor_model_parallel_size: 4
  0:   pipeline_model_parallel_size: 1
  0:   context_parallel_size: 2
  0:   cpu_offloading: false
  0:   dist_ckpt_load_strictness: log_all
  0:   global_batch_size: 16
  0:   micro_batch_size: 1
  0:   max_position_embeddings: 8192
  0:   encoder_seq_length: 8192
  0:   restore_from_path: /ckpt
  0:   resume_from_checkpoint: null
  0:   save_nemo_on_validation_end: false
  0:   sync_batch_comm: false
  0:   megatron_amp_O2: true
  0:   sequence_parallel: 1
  0:   activations_checkpoint_granularity: null
  0:   activations_checkpoint_method: null
  0:   activations_checkpoint_num_layers: null
  0:   activations_checkpoint_layers_per_pipeline: null
  0:   answer_only_loss: true
  0:   gradient_as_bucket_view: false
  0:   hidden_dropout: 0.0
  0:   attention_dropout: 0.0
  0:   ffn_dropout: 0.0
  0:   bias_activation_fusion: true
  0:   bias_dropout_add_fusion: false
  0:   transformer_engine: true
  0:   fp8: true
  0:   fp8_params: true
  0:   fp8_hybrid: true
  0:   fp8_amax_history_len: 32
  0:   fp8_amax_compute_algo: max
  0:   reduce_amax: false
  0:   fp8_e4m3: false
  0:   fp8_interval: 1
  0:   fp8_margin: 0
  0:   fp8_dot_product_attention: 0
  0:   activation_func_fp8_input_store: 0
  0:   apply_rope_fusion: true
  0:   disable_parameter_transpose_cache: true
  0:   ub_tp_comm_overlap: 1
  0:   tp_comm_overlap_ag: true
  0:   tp_comm_overlap_rs: true
  0:   tp_comm_overlap_rs_dgrad: false
  0:   tp_comm_overlap_disable_qkv: true
  0:   batch_p2p_comm: 'False'
  0:   virtual_pipeline_model_parallel_size: 1
  0:   sharp: true
  0:   nccl_communicator_config_path: null
  0:   peft:
  0:     peft_scheme: lora
  0:     restore_from_path: null
  0:     lora_tuning:
  0:       adapter_dim: 16
  0:       alpha: 32
  0:       adapter_dropout: 0.1
  0:       dropout_position: pre
  0:       target_modules:
  0:       - attention
  0:       column_init_method: kaiming
  0:       row_init_method: zero
  0:       layer_selection: null
  0:       weight_tying: false
  0:       position_embedding_strategy: null
  0:       a2a_experimental: 1
  0:   data:
  0:     multiprocessing_context: spawn
  0:     pin_memory: true
  0:     sample_weight: constant
  0:     validation_drop_last: false
  0:     train_ds:
  0:       file_names:
  0:       - /data/train.npy
  0:       packed_sequence: true
  0:       packed_sequence_return_cu_seqlen: false
  0:       index_mapping_dir: /results/data_index/train
  0:       global_batch_size: 16
  0:       micro_batch_size: 1
  0:       shuffle: true
  0:       num_workers: 1
  0:       memmap_workers: 2
  0:       pin_memory: true
  0:       max_seq_length: 8192
  0:       min_seq_length: 1
  0:       drop_last: true
  0:       concat_sampling_probabilities:
  0:       - 1.0
  0:       label_key: output
  0:       add_eos: true
  0:       add_sep: false
  0:       add_bos: false
  0:       truncation_field: input
  0:       prompt_template: '{input} {output}'
  0:       truncation_method: right
  0:       seed: 6678
  0:     validation_ds:
  0:       file_names:
  0:       - /data/validation.npy
  0:       packed_sequence: true
  0:       packed_sequence_return_cu_seqlen: false
  0:       index_mapping_dir: /results/data_index/val
  0:       names: null
  0:       global_batch_size: 16
  0:       micro_batch_size: 1
  0:       shuffle: false
  0:       num_workers: 1
  0:       memmap_workers: 2
  0:       pin_memory: true
  0:       max_seq_length: 8192
  0:       min_seq_length: 1
  0:       drop_last: false
  0:       label_key: output
  0:       add_eos: true
  0:       add_sep: false
  0:       add_bos: false
  0:       write_predictions_to_file: false
  0:       output_file_path_prefix: null
  0:       truncation_field: input
  0:       prompt_template: '{input} {output}'
  0:       tokens_to_generate: 32
  0:       truncation_method: right
  0:       metric:
  0:         name: loss
  0:         average: null
  0:         num_classes: null
  0:   optim:
  0:     name: mcore_distributed_optim
  0:     overlap_grad_sync: true
  0:     overlap_param_sync: true
  0:     delay_grad_reduce: true
  0:     delay_param_gather: true
  0:     average_in_collective: false
  0:     lr: 0.00035
  0:     min_lr: 0
  0:     weight_decay: 0.0001
  0:     betas:
  0:     - 0.9
  0:     - 0.999
  0:     eps: 1.0e-08
  0:     amsgrad: false
  0:     sched:
  0:       name: CosineAnnealing
  0:       warmup_ratio: 0.0
  0:       min_lr: 0.0
  0:       constant_steps: 0
  0:       monitor: val_loss
  0:       reduce_on_plateau: false
  0:   enable_cuda_graph: 1
  0:   enable_cg_fp8_weight_caching: true
  0:   custom:
  0:     warmup: true
  0:     warmup_train_steps: 5
  0:     warmup_validation_steps: 5
  0:     reset_fp8_stats_after_warmup: 1
  0: name: megatron_gpt_peft_lora_tuning
  0: trainer:
  0:   devices: 8
  0:   num_nodes: 16
  0:   accelerator: gpu
  0:   precision: bf16-mixed
  0:   max_steps: 1024
  0:   val_check_interval: 120
  0:   check_val_every_n_epoch: null
  0:   log_every_n_steps: 0
  0:   gradient_clip_val: 0.3
  0:   gradient_clip_algorithm: norm
  0:   num_sanity_val_steps: 0
  0:   max_epochs: 1000
  0:   limit_val_batches: 1.0
  0:   limit_train_batches: 1.0
  0:   limit_test_batches: 0
  0:   logger: false
  0:   enable_checkpointing: false
  0:   use_distributed_sampler: false
  0:   enable_progress_bar: false
  0: exp_manager:
  0:   log_tflops_per_sec_per_gpu: false
  0:   explicit_log_dir: null
  0:   exp_dir: /results
  0:   create_wandb_logger: false
  0:   resume_if_exists: false
  0:   resume_ignore_no_checkpoint: true
  0:   create_checkpoint_callback: false
  0:   log_global_rank_0_only: true
  0:   create_early_stopping_callback: false
  0:   create_tensorboard_logger: false
  0: 
  0: GPU available: True (cuda), used: True
  0: TPU available: False, using: 0 TPU cores
  0: HPU available: False, using: 0 HPUs
  0: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
  0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
108: Initializing distributed: GLOBAL_RANK: 108, MEMBER: 109/128
110: Initializing distributed: GLOBAL_RANK: 110, MEMBER: 111/128
111: Initializing distributed: GLOBAL_RANK: 111, MEMBER: 112/128
106: Initializing distributed: GLOBAL_RANK: 106, MEMBER: 107/128
104: Initializing distributed: GLOBAL_RANK: 104, MEMBER: 105/128
105: Initializing distributed: GLOBAL_RANK: 105, MEMBER: 106/128
107: Initializing distributed: GLOBAL_RANK: 107, MEMBER: 108/128
109: Initializing distributed: GLOBAL_RANK: 109, MEMBER: 110/128
 17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/128
 20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/128
 19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/128
 21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/128
 18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/128
 22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/128
 16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/128
 23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/128
  0: setting number of microbatches to constant 1
 69: Initializing distributed: GLOBAL_RANK: 69, MEMBER: 70/128
 65: Initializing distributed: GLOBAL_RANK: 65, MEMBER: 66/128
 38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/128
 39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/128
 67: Initializing distributed: GLOBAL_RANK: 67, MEMBER: 68/128
 71: Initializing distributed: GLOBAL_RANK: 71, MEMBER: 72/128
 66: Initializing distributed: GLOBAL_RANK: 66, MEMBER: 67/128
 35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/128
 36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/128
 34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/128
 33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/128
125: Initializing distributed: GLOBAL_RANK: 125, MEMBER: 126/128
 32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/128
 64: Initializing distributed: GLOBAL_RANK: 64, MEMBER: 65/128
 70: Initializing distributed: GLOBAL_RANK: 70, MEMBER: 71/128
 68: Initializing distributed: GLOBAL_RANK: 68, MEMBER: 69/128
 37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/128
124: Initializing distributed: GLOBAL_RANK: 124, MEMBER: 125/128
122: Initializing distributed: GLOBAL_RANK: 122, MEMBER: 123/128
127: Initializing distributed: GLOBAL_RANK: 127, MEMBER: 128/128
123: Initializing distributed: GLOBAL_RANK: 123, MEMBER: 124/128
121: Initializing distributed: GLOBAL_RANK: 121, MEMBER: 122/128
 52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/128
120: Initializing distributed: GLOBAL_RANK: 120, MEMBER: 121/128
126: Initializing distributed: GLOBAL_RANK: 126, MEMBER: 127/128
 54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/128
 43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/128
 24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/128
 26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/128
 46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/128
 25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/128
 42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/128
 29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/128
 30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/128
 55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/128
 40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/128
 50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/128
 97: Initializing distributed: GLOBAL_RANK: 97, MEMBER: 98/128
103: Initializing distributed: GLOBAL_RANK: 103, MEMBER: 104/128
 49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/128
 53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/128
 47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/128
 44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/128
 45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/128
 48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/128
 27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/128
 28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/128
 51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/128
 41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/128
 31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/128
 94: Initializing distributed: GLOBAL_RANK: 94, MEMBER: 95/128
102: Initializing distributed: GLOBAL_RANK: 102, MEMBER: 103/128
 89: Initializing distributed: GLOBAL_RANK: 89, MEMBER: 90/128
113: Initializing distributed: GLOBAL_RANK: 113, MEMBER: 114/128
115: Initializing distributed: GLOBAL_RANK: 115, MEMBER: 116/128
 98: Initializing distributed: GLOBAL_RANK: 98, MEMBER: 99/128
100: Initializing distributed: GLOBAL_RANK: 100, MEMBER: 101/128
101: Initializing distributed: GLOBAL_RANK: 101, MEMBER: 102/128
 93: Initializing distributed: GLOBAL_RANK: 93, MEMBER: 94/128
 96: Initializing distributed: GLOBAL_RANK: 96, MEMBER: 97/128
 99: Initializing distributed: GLOBAL_RANK: 99, MEMBER: 100/128
 91: Initializing distributed: GLOBAL_RANK: 91, MEMBER: 92/128
116: Initializing distributed: GLOBAL_RANK: 116, MEMBER: 117/128
119: Initializing distributed: GLOBAL_RANK: 119, MEMBER: 120/128
117: Initializing distributed: GLOBAL_RANK: 117, MEMBER: 118/128
 88: Initializing distributed: GLOBAL_RANK: 88, MEMBER: 89/128
 92: Initializing distributed: GLOBAL_RANK: 92, MEMBER: 93/128
114: Initializing distributed: GLOBAL_RANK: 114, MEMBER: 115/128
112: Initializing distributed: GLOBAL_RANK: 112, MEMBER: 113/128
118: Initializing distributed: GLOBAL_RANK: 118, MEMBER: 119/128
 90: Initializing distributed: GLOBAL_RANK: 90, MEMBER: 91/128
 95: Initializing distributed: GLOBAL_RANK: 95, MEMBER: 96/128
 58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/128
 61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/128
 62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/128
 72: Initializing distributed: GLOBAL_RANK: 72, MEMBER: 73/128
 79: Initializing distributed: GLOBAL_RANK: 79, MEMBER: 80/128
 78: Initializing distributed: GLOBAL_RANK: 78, MEMBER: 79/128
 77: Initializing distributed: GLOBAL_RANK: 77, MEMBER: 78/128
 74: Initializing distributed: GLOBAL_RANK: 74, MEMBER: 75/128
 56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/128
 60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/128
 75: Initializing distributed: GLOBAL_RANK: 75, MEMBER: 76/128
 76: Initializing distributed: GLOBAL_RANK: 76, MEMBER: 77/128
 63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/128
 59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/128
 73: Initializing distributed: GLOBAL_RANK: 73, MEMBER: 74/128
 57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/128
  1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/128
 83: Initializing distributed: GLOBAL_RANK: 83, MEMBER: 84/128
 86: Initializing distributed: GLOBAL_RANK: 86, MEMBER: 87/128
 80: Initializing distributed: GLOBAL_RANK: 80, MEMBER: 81/128
 85: Initializing distributed: GLOBAL_RANK: 85, MEMBER: 86/128
  7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/128
  4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/128
  2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/128
 10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/128
 81: Initializing distributed: GLOBAL_RANK: 81, MEMBER: 82/128
  0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/128
 87: Initializing distributed: GLOBAL_RANK: 87, MEMBER: 88/128
 82: Initializing distributed: GLOBAL_RANK: 82, MEMBER: 83/128
 84: Initializing distributed: GLOBAL_RANK: 84, MEMBER: 85/128
  3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/128
 14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/128
  6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/128
  5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/128
  9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/128
  8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/128
 13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/128
 11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/128
 15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/128
 12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/128
  2: NCCL version 2.22.3+cuda12.6
  3: NCCL version 2.22.3+cuda12.6
  0: ----------------------------------------------------------------------------------------------------
  0: distributed_backend=nccl
  0: All distributed processes registered. Starting with 128 processes
  0: ----------------------------------------------------------------------------------------------------
  0: 
  1: NCCL version 2.22.3+cuda12.6
  0: The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
  0: NCCL version 2.22.3+cuda12.6
  0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
  0: Loading distributed checkpoint directly on the GPU
 24: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 80: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 72: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 88: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 56: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
120: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  8: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 64: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 40: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 16: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 32: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
104: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
112: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 96: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 48: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
120: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 40: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 24: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 88: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 80: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
  8: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 16: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 72: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
  0: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 32: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
104: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
112: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 64: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 96: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 48: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 56: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
120: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 80: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 24: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 40: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 16: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 72: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 88: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
104: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  8: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 32: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 64: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 96: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 48: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
112: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 56: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: > building indices for blendable datasets ...
  0:  > sample ratios:
  0:    dataset 0, input: 1, achieved: 1
  0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 64: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 65: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 73: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 72: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 96: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
104: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
112: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
120: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 89: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
105: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 66: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
113: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 74: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
121: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 97: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 91: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
106: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 67: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
114: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 75: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 80: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
122: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 98: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 92: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
107: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 68: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
115: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 77: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 81: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
123: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 99: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 93: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
108: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 69: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
116: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 79: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 82: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
124: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
100: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 94: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
109: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 70: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
117: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 76: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 83: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
125: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
101: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 95: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
110: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 71: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
118: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 78: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 84: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
126: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
102: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 88: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
111: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
119: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 85: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
127: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
103: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 90: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 86: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 87: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  0: Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
  0: Number of buckets for gradient all-reduce / reduce-scatter: 1
  0: Params for bucket 1 (11141120 elements):
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00035, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
  0: 
  0:   | Name         | Type | Params | Mode
  0: ---------------------------------------------
  0:   | other params | n/a  | 17.3 B | n/a 
  0: ---------------------------------------------
  0: 11.1 M    Trainable params
  0: 17.2 B    Non-trainable params
  0: 17.3 B    Total params
  0: 69,029.364Total estimated model params size (MB)
  0: 0         Modules in train mode
  0: 0         Modules in eval mode
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982217, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 332}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 333}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982218, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982218, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982218, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982218, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982218, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982218, "event_type": "POINT_IN_TIME", "key": "seed", "value": 6678, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 335}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982218, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 341}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982873, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 346}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982905, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 350}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 354}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 358}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 362}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 367}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 368}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 369}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 370}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728614982906, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 371}}
  0: SLURM auto-requeueing enabled. Setting signal handlers.
 48: SLURM auto-requeueing enabled. Setting signal handlers.
 32: SLURM auto-requeueing enabled. Setting signal handlers.
 64: SLURM auto-requeueing enabled. Setting signal handlers.
 49: SLURM auto-requeueing enabled. Setting signal handlers.
  8: SLURM auto-requeueing enabled. Setting signal handlers.
 16: SLURM auto-requeueing enabled. Setting signal handlers.
 33: SLURM auto-requeueing enabled. Setting signal handlers.
  1: SLURM auto-requeueing enabled. Setting signal handlers.
 65: SLURM auto-requeueing enabled. Setting signal handlers.
 40: SLURM auto-requeueing enabled. Setting signal handlers.
112: SLURM auto-requeueing enabled. Setting signal handlers.
 72: SLURM auto-requeueing enabled. Setting signal handlers.
 56: SLURM auto-requeueing enabled. Setting signal handlers.
 50: SLURM auto-requeueing enabled. Setting signal handlers.
120: SLURM auto-requeueing enabled. Setting signal handlers.
 96: SLURM auto-requeueing enabled. Setting signal handlers.
 24: SLURM auto-requeueing enabled. Setting signal handlers.
  9: SLURM auto-requeueing enabled. Setting signal handlers.
 88: SLURM auto-requeueing enabled. Setting signal handlers.
 17: SLURM auto-requeueing enabled. Setting signal handlers.
104: SLURM auto-requeueing enabled. Setting signal handlers.
 34: SLURM auto-requeueing enabled. Setting signal handlers.
  4: SLURM auto-requeueing enabled. Setting signal handlers.
 67: SLURM auto-requeueing enabled. Setting signal handlers.
 41: SLURM auto-requeueing enabled. Setting signal handlers.
113: SLURM auto-requeueing enabled. Setting signal handlers.
 73: SLURM auto-requeueing enabled. Setting signal handlers.
 80: SLURM auto-requeueing enabled. Setting signal handlers.
 57: SLURM auto-requeueing enabled. Setting signal handlers.
 52: SLURM auto-requeueing enabled. Setting signal handlers.
121: SLURM auto-requeueing enabled. Setting signal handlers.
 97: SLURM auto-requeueing enabled. Setting signal handlers.
 26: SLURM auto-requeueing enabled. Setting signal handlers.
 12: SLURM auto-requeueing enabled. Setting signal handlers.
 89: SLURM auto-requeueing enabled. Setting signal handlers.
 18: SLURM auto-requeueing enabled. Setting signal handlers.
105: SLURM auto-requeueing enabled. Setting signal handlers.
 35: SLURM auto-requeueing enabled. Setting signal handlers.
  7: SLURM auto-requeueing enabled. Setting signal handlers.
 66: SLURM auto-requeueing enabled. Setting signal handlers.
 42: SLURM auto-requeueing enabled. Setting signal handlers.
115: SLURM auto-requeueing enabled. Setting signal handlers.
 74: SLURM auto-requeueing enabled. Setting signal handlers.
 81: SLURM auto-requeueing enabled. Setting signal handlers.
 58: SLURM auto-requeueing enabled. Setting signal handlers.
 51: SLURM auto-requeueing enabled. Setting signal handlers.
122: SLURM auto-requeueing enabled. Setting signal handlers.
 98: SLURM auto-requeueing enabled. Setting signal handlers.
 27: SLURM auto-requeueing enabled. Setting signal handlers.
 13: SLURM auto-requeueing enabled. Setting signal handlers.
 90: SLURM auto-requeueing enabled. Setting signal handlers.
 19: SLURM auto-requeueing enabled. Setting signal handlers.
106: SLURM auto-requeueing enabled. Setting signal handlers.
 36: SLURM auto-requeueing enabled. Setting signal handlers.
  2: SLURM auto-requeueing enabled. Setting signal handlers.
 68: SLURM auto-requeueing enabled. Setting signal handlers.
 43: SLURM auto-requeueing enabled. Setting signal handlers.
116: SLURM auto-requeueing enabled. Setting signal handlers.
 75: SLURM auto-requeueing enabled. Setting signal handlers.
 83: SLURM auto-requeueing enabled. Setting signal handlers.
 59: SLURM auto-requeueing enabled. Setting signal handlers.
 53: SLURM auto-requeueing enabled. Setting signal handlers.
123: SLURM auto-requeueing enabled. Setting signal handlers.
 99: SLURM auto-requeueing enabled. Setting signal handlers.
 28: SLURM auto-requeueing enabled. Setting signal handlers.
 14: SLURM auto-requeueing enabled. Setting signal handlers.
 91: SLURM auto-requeueing enabled. Setting signal handlers.
 20: SLURM auto-requeueing enabled. Setting signal handlers.
107: SLURM auto-requeueing enabled. Setting signal handlers.
 37: SLURM auto-requeueing enabled. Setting signal handlers.
  5: SLURM auto-requeueing enabled. Setting signal handlers.
 69: SLURM auto-requeueing enabled. Setting signal handlers.
 44: SLURM auto-requeueing enabled. Setting signal handlers.
117: SLURM auto-requeueing enabled. Setting signal handlers.
 76: SLURM auto-requeueing enabled. Setting signal handlers.
 82: SLURM auto-requeueing enabled. Setting signal handlers.
 60: SLURM auto-requeueing enabled. Setting signal handlers.
 55: SLURM auto-requeueing enabled. Setting signal handlers.
124: SLURM auto-requeueing enabled. Setting signal handlers.
100: SLURM auto-requeueing enabled. Setting signal handlers.
 30: SLURM auto-requeueing enabled. Setting signal handlers.
 15: SLURM auto-requeueing enabled. Setting signal handlers.
 92: SLURM auto-requeueing enabled. Setting signal handlers.
 21: SLURM auto-requeueing enabled. Setting signal handlers.
108: SLURM auto-requeueing enabled. Setting signal handlers.
 39: SLURM auto-requeueing enabled. Setting signal handlers.
  6: SLURM auto-requeueing enabled. Setting signal handlers.
 70: SLURM auto-requeueing enabled. Setting signal handlers.
 45: SLURM auto-requeueing enabled. Setting signal handlers.
118: SLURM auto-requeueing enabled. Setting signal handlers.
 77: SLURM auto-requeueing enabled. Setting signal handlers.
 84: SLURM auto-requeueing enabled. Setting signal handlers.
 61: SLURM auto-requeueing enabled. Setting signal handlers.
 54: SLURM auto-requeueing enabled. Setting signal handlers.
125: SLURM auto-requeueing enabled. Setting signal handlers.
103: SLURM auto-requeueing enabled. Setting signal handlers.
 31: SLURM auto-requeueing enabled. Setting signal handlers.
 11: SLURM auto-requeueing enabled. Setting signal handlers.
 93: SLURM auto-requeueing enabled. Setting signal handlers.
 22: SLURM auto-requeueing enabled. Setting signal handlers.
109: SLURM auto-requeueing enabled. Setting signal handlers.
 38: SLURM auto-requeueing enabled. Setting signal handlers.
  3: SLURM auto-requeueing enabled. Setting signal handlers.
 71: SLURM auto-requeueing enabled. Setting signal handlers.
 47: SLURM auto-requeueing enabled. Setting signal handlers.
119: SLURM auto-requeueing enabled. Setting signal handlers.
 79: SLURM auto-requeueing enabled. Setting signal handlers.
 85: SLURM auto-requeueing enabled. Setting signal handlers.
 63: SLURM auto-requeueing enabled. Setting signal handlers.
126: SLURM auto-requeueing enabled. Setting signal handlers.
101: SLURM auto-requeueing enabled. Setting signal handlers.
 25: SLURM auto-requeueing enabled. Setting signal handlers.
 10: SLURM auto-requeueing enabled. Setting signal handlers.
 94: SLURM auto-requeueing enabled. Setting signal handlers.
 23: SLURM auto-requeueing enabled. Setting signal handlers.
110: SLURM auto-requeueing enabled. Setting signal handlers.
 46: SLURM auto-requeueing enabled. Setting signal handlers.
114: SLURM auto-requeueing enabled. Setting signal handlers.
 78: SLURM auto-requeueing enabled. Setting signal handlers.
 86: SLURM auto-requeueing enabled. Setting signal handlers.
 62: SLURM auto-requeueing enabled. Setting signal handlers.
127: SLURM auto-requeueing enabled. Setting signal handlers.
102: SLURM auto-requeueing enabled. Setting signal handlers.
 29: SLURM auto-requeueing enabled. Setting signal handlers.
 95: SLURM auto-requeueing enabled. Setting signal handlers.
111: SLURM auto-requeueing enabled. Setting signal handlers.
 87: SLURM auto-requeueing enabled. Setting signal handlers.
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: !!! [UB] Global ranks on node 11: [88, 89, 90, 91, 92, 93, 94, 95]
 80: !!! [UB] Global ranks on node 10: [80, 81, 82, 83, 84, 85, 86, 87]
104: !!! [UB] Global ranks on node 13: [104, 105, 106, 107, 108, 109, 110, 111]
 96: !!! [UB] Global ranks on node 12: [96, 97, 98, 99, 100, 101, 102, 103]
112: !!! [UB] Global ranks on node 14: [112, 113, 114, 115, 116, 117, 118, 119]
 72: !!! [UB] Global ranks on node 9: [72, 73, 74, 75, 76, 77, 78, 79]
 64: !!! [UB] Global ranks on node 8: [64, 65, 66, 67, 68, 69, 70, 71]
  0: !!! [UB] Number of physical nodes: 16
  0: !!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
120: !!! [UB] Global ranks on node 15: [120, 121, 122, 123, 124, 125, 126, 127]
  0: !!! [UB] Create Userbuffers Communicator
 56: !!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
  8: !!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
 48: !!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
 16: !!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
 40: !!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
 32: !!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
 24: !!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
  0: UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
  0: MC initialized succesfully, window size = 549755813888
  0: !!! [UBP2P] Register UBuf 1
  0: !!! [UBP2P] Register UBuf 2
  0: !!! [UBP2P] Register UBuf 3
  0: !!! [UBP2P] Register UBuf 4
  0: !!! [UBP2P] Register UBuf 5
  0: !!! [UB] Register UBuf 6
  0: !!! [UB] Register UBuf 7
  0: !!! [UB] Register UBuf 8
  0: !!! [UB] Register UBuf 9
  0: !!! [UB] Register UBuf 10
 92: NCCL version 2.22.3+cuda12.6
 84: NCCL version 2.22.3+cuda12.6
 44: NCCL version 2.22.3+cuda12.6
124: NCCL version 2.22.3+cuda12.6
 24: NCCL version 2.22.3+cuda12.6
108: NCCL version 2.22.3+cuda12.6
 12: NCCL version 2.22.3+cuda12.6
 28: NCCL version 2.22.3+cuda12.6
 80: NCCL version 2.22.3+cuda12.6
120: NCCL version 2.22.3+cuda12.6
104: NCCL version 2.22.3+cuda12.6
 56: NCCL version 2.22.3+cuda12.6
 40: NCCL version 2.22.3+cuda12.6
100: NCCL version 2.22.3+cuda12.6
  8: NCCL version 2.22.3+cuda12.6
  4: NCCL version 2.22.3+cuda12.6
 32: NCCL version 2.22.3+cuda12.6
 60: NCCL version 2.22.3+cuda12.6
 96: NCCL version 2.22.3+cuda12.6
112: NCCL version 2.22.3+cuda12.6
 68: NCCL version 2.22.3+cuda12.6
 88: NCCL version 2.22.3+cuda12.6
 72: NCCL version 2.22.3+cuda12.6
 76: NCCL version 2.22.3+cuda12.6
 36: NCCL version 2.22.3+cuda12.6
 64: NCCL version 2.22.3+cuda12.6
 16: NCCL version 2.22.3+cuda12.6
 20: NCCL version 2.22.3+cuda12.6
116: NCCL version 2.22.3+cuda12.6
 48: NCCL version 2.22.3+cuda12.6
 52: NCCL version 2.22.3+cuda12.6
 57: NCCL version 2.22.3+cuda12.6
 58: NCCL version 2.22.3+cuda12.6
 59: NCCL version 2.22.3+cuda12.6
 89: NCCL version 2.22.3+cuda12.6
 90: NCCL version 2.22.3+cuda12.6
105: NCCL version 2.22.3+cuda12.6
106: NCCL version 2.22.3+cuda12.6
107: NCCL version 2.22.3+cuda12.6
 91: NCCL version 2.22.3+cuda12.6
 41: NCCL version 2.22.3+cuda12.6
 42: NCCL version 2.22.3+cuda12.6
 43: NCCL version 2.22.3+cuda12.6
 25: NCCL version 2.22.3+cuda12.6
 26: NCCL version 2.22.3+cuda12.6
 27: NCCL version 2.22.3+cuda12.6
 83: NCCL version 2.22.3+cuda12.6
121: NCCL version 2.22.3+cuda12.6
122: NCCL version 2.22.3+cuda12.6
 82: NCCL version 2.22.3+cuda12.6
 81: NCCL version 2.22.3+cuda12.6
123: NCCL version 2.22.3+cuda12.6
113: NCCL version 2.22.3+cuda12.6
115: NCCL version 2.22.3+cuda12.6
114: NCCL version 2.22.3+cuda12.6
 99: NCCL version 2.22.3+cuda12.6
 97: NCCL version 2.22.3+cuda12.6
 98: NCCL version 2.22.3+cuda12.6
 33: NCCL version 2.22.3+cuda12.6
 34: NCCL version 2.22.3+cuda12.6
 35: NCCL version 2.22.3+cuda12.6
 17: NCCL version 2.22.3+cuda12.6
 18: NCCL version 2.22.3+cuda12.6
 19: NCCL version 2.22.3+cuda12.6
 67: NCCL version 2.22.3+cuda12.6
 65: NCCL version 2.22.3+cuda12.6
 66: NCCL version 2.22.3+cuda12.6
 75: NCCL version 2.22.3+cuda12.6
 73: NCCL version 2.22.3+cuda12.6
 74: NCCL version 2.22.3+cuda12.6
 10: NCCL version 2.22.3+cuda12.6
 11: NCCL version 2.22.3+cuda12.6
  9: NCCL version 2.22.3+cuda12.6
 49: NCCL version 2.22.3+cuda12.6
 50: NCCL version 2.22.3+cuda12.6
 51: NCCL version 2.22.3+cuda12.6
  5: NCCL version 2.22.3+cuda12.6
  6: NCCL version 2.22.3+cuda12.6
  7: NCCL version 2.22.3+cuda12.6
  0: :::MLLOG {"namespace": "", "time_ms": 1728615098930, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615098930, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615098930, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615104440, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.311368703842163, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.0003499176480626913}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615109953, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4569123983383179, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.0003496706697575261}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615115491, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3930304050445557, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00034925929753184046}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615121011, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4008142948150635, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00034868391855477426}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615126544, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3338792324066162, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.00034794507435288117}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615132089, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.40485417842865, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.00034704346030046284}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615137634, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3275930881500244, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.000345979924965107}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615143179, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.359362244606018, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003447554693090452}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615148731, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3008512258529663, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.0003433712457470823}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615154282, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.371472954750061, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003418285570619839}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615159833, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3555927276611328, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.00034012885517834305}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615165385, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2846187353134155, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003382737397960793}}
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: :::MLLOG {"namespace": "", "time_ms": 1728615175060, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.90585699569026}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615175060, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615175061, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728615180448, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9415575861930847, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615180448, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615180448, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615186015, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.270055890083313, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00033626495688485734}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615191585, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3375024795532227, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0003341043970408414}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615193822, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.737999553427937}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615193822, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615193823, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728615198982, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9366611242294312, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615198983, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615198983, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615202318, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3442957401275635, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.00033179409370733237}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615207870, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2862820625305176, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0003293362212609621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615212318, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.822946047579638}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615212318, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615212318, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728615217434, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9314011335372925, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615217434, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615217434, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615218539, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3051791191101074, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.00032673309296524624}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615224088, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2813307046890259, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.0003239871587934214}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615229638, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3330302238464355, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.0003211010031226165}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615230755, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.853128490281037}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615230755, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615230755, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728615235959, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9321504235267639, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615235959, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615235960, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615240398, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2411435842514038, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3200, "lr": 0.0003180773423015271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615245950, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2810304164886475, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3360, "lr": 0.00031491902209388335}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615249285, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.843446332273015}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615249285, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615249285, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3456}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728615254488, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9331176280975342, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615254488, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615254488, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615256713, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3105086088180542, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3520, "lr": 0.0003116290150001165}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615262266, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2628806829452515, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3680, "lr": 0.0003082104174597458}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615267824, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3035060167312622, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3840, "lr": 0.00030466644693711784}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615267830, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.808375218518275}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615267830, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615267830, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3840}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728615272916, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9246416687965393, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615272916, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728615272916, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9246416687965393, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 321, "samples_count": 3840, "status": "success"}}
 89: [rank89]:[W1011 02:54:37.486889509 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 31: [rank31]:[W1011 02:54:37.466490288 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 44: [rank44]:[W1011 02:54:37.558742252 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  7: [rank7]:[W1011 02:54:37.623219895 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 14: [rank14]:[W1011 02:54:37.121461452 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 57: [rank57]:[W1011 02:54:37.662000974 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 18: [rank18]:[W1011 02:54:37.131747968 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 52: [rank52]:[W1011 02:54:37.918200715 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 65: [rank65]:[W1011 02:54:37.109199962 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 74: [rank74]:[W1011 02:54:37.470686199 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
107: [rank107]:[W1011 02:54:37.933533846 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 86: [rank86]:[W1011 02:54:37.121816119 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  5: [rank5]:[W1011 02:54:37.662844788 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  6: [rank6]:[W1011 02:54:37.666574533 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 36: [rank36]:[W1011 02:54:37.688675047 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 13: [rank13]:[W1011 02:54:37.168068588 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 15: [rank15]:[W1011 02:54:37.168872574 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 22: [rank22]:[W1011 02:54:37.178444746 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
117: [rank117]:[W1011 02:54:37.845982416 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  4: [rank4]:[W1011 02:54:37.681977804 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 19: [rank19]:[W1011 02:54:37.185300342 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 96: [rank96]:[W1011 02:54:37.159278697 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  3: [rank3]:[W1011 02:54:37.683846417 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 16: [rank16]:[W1011 02:54:37.185580831 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  8: [rank8]:[W1011 02:54:37.180223194 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 30: [rank30]:[W1011 02:54:37.540183318 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 21: [rank21]:[W1011 02:54:37.189750090 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 10: [rank10]:[W1011 02:54:37.185627876 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  2: [rank2]:[W1011 02:54:37.690501512 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 12: [rank12]:[W1011 02:54:37.187204441 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 47: [rank47]:[W1011 02:54:37.631146237 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 23: [rank23]:[W1011 02:54:37.195339915 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
123: [rank123]:[W1011 02:54:37.226086023 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 39: [rank39]:[W1011 02:54:37.710735696 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 11: [rank11]:[W1011 02:54:37.189031160 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 27: [rank27]:[W1011 02:54:37.547508616 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 24: [rank24]:[W1011 02:54:37.547662974 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  0: [rank0]:[W1011 02:54:37.694206781 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 54: [rank54]:[W1011 02:54:37.983475582 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 38: [rank38]:[W1011 02:54:37.713629684 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 37: [rank37]:[W1011 02:54:37.715078771 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 53: [rank53]:[W1011 02:54:37.985004272 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 45: [rank45]:[W1011 02:54:37.635761179 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 29: [rank29]:[W1011 02:54:37.551908316 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  9: [rank9]:[W1011 02:54:37.195492236 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  1: [rank1]:[W1011 02:54:37.700353679 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 25: [rank25]:[W1011 02:54:37.554005043 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 26: [rank26]:[W1011 02:54:37.554105834 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 34: [rank34]:[W1011 02:54:37.717548622 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 20: [rank20]:[W1011 02:54:37.203710752 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 63: [rank63]:[W1011 02:54:37.735940223 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 55: [rank55]:[W1011 02:54:37.990594464 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 35: [rank35]:[W1011 02:54:37.720849975 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 46: [rank46]:[W1011 02:54:37.644268418 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 62: [rank62]:[W1011 02:54:37.739000747 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 70: [rank70]:[W1011 02:54:37.176069696 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 61: [rank61]:[W1011 02:54:37.740543916 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 42: [rank42]:[W1011 02:54:37.648225941 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 60: [rank60]:[W1011 02:54:37.744292337 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 28: [rank28]:[W1011 02:54:37.565730642 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 87: [rank87]:[W1011 02:54:37.179213432 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 43: [rank43]:[W1011 02:54:37.651532135 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 32: [rank32]:[W1011 02:54:37.731097692 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 40: [rank40]:[W1011 02:54:37.651722590 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 78: [rank78]:[W1011 02:54:37.547385658 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 69: [rank69]:[W1011 02:54:37.187650577 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 33: [rank33]:[W1011 02:54:37.737365258 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 41: [rank41]:[W1011 02:54:37.658002340 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 17: [rank17]:[W1011 02:54:37.222181929 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 58: [rank58]:[W1011 02:54:37.752967017 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 50: [rank50]:[W1011 02:54:37.007468076 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 76: [rank76]:[W1011 02:54:37.552762840 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 68: [rank68]:[W1011 02:54:37.191522407 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 79: [rank79]:[W1011 02:54:37.554279261 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 71: [rank71]:[W1011 02:54:37.193070170 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 95: [rank95]:[W1011 02:54:37.601419280 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
103: [rank103]:[W1011 02:54:37.199540204 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
111: [rank111]:[W1011 02:54:37.014378451 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
119: [rank119]:[W1011 02:54:37.892263122 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 48: [rank48]:[W1011 02:54:37.011005672 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 94: [rank94]:[W1011 02:54:37.604248795 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 77: [rank77]:[W1011 02:54:37.558957354 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 49: [rank49]:[W1011 02:54:37.017312965 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 66: [rank66]:[W1011 02:54:37.200111390 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 92: [rank92]:[W1011 02:54:37.609605687 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 84: [rank84]:[W1011 02:54:37.197760175 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 51: [rank51]:[W1011 02:54:37.020743873 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 67: [rank67]:[W1011 02:54:37.203350887 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 59: [rank59]:[W1011 02:54:37.766345588 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 56: [rank56]:[W1011 02:54:37.766574131 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
127: [rank127]:[W1011 02:54:37.266691920 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
110: [rank110]:[W1011 02:54:37.027052135 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
118: [rank118]:[W1011 02:54:37.904906276 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
102: [rank102]:[W1011 02:54:37.212483258 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 93: [rank93]:[W1011 02:54:37.615830492 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
101: [rank101]:[W1011 02:54:37.213960488 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 73: [rank73]:[W1011 02:54:37.571191668 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 82: [rank82]:[W1011 02:54:37.206366382 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
100: [rank100]:[W1011 02:54:37.217796569 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 99: [rank99]:[W1011 02:54:37.219518513 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 75: [rank75]:[W1011 02:54:37.574698304 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 91: [rank91]:[W1011 02:54:37.621577002 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 83: [rank83]:[W1011 02:54:37.209720359 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 64: [rank64]:[W1011 02:54:37.213657141 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
126: [rank126]:[W1011 02:54:37.279327833 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 85: [rank85]:[W1011 02:54:37.213942731 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
109: [rank109]:[W1011 02:54:37.038748152 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 97: [rank97]:[W1011 02:54:37.226020738 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 81: [rank81]:[W1011 02:54:37.216129066 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
106: [rank106]:[W1011 02:54:37.041029846 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 90: [rank90]:[W1011 02:54:37.628263566 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
108: [rank108]:[W1011 02:54:37.042526906 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
115: [rank115]:[W1011 02:54:37.922210624 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 72: [rank72]:[W1011 02:54:37.584980502 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
125: [rank125]:[W1011 02:54:37.290960208 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
105: [rank105]:[W1011 02:54:37.050920992 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
113: [rank113]:[W1011 02:54:37.928749264 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
121: [rank121]:[W1011 02:54:37.293128611 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
122: [rank122]:[W1011 02:54:37.293261420 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 98: [rank98]:[W1011 02:54:37.236375382 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
114: [rank114]:[W1011 02:54:37.928945972 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
116: [rank116]:[W1011 02:54:37.930376733 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
124: [rank124]:[W1011 02:54:37.294812755 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 80: [rank80]:[W1011 02:54:37.229979755 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 88: [rank88]:[W1011 02:54:37.651859750 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
104: [rank104]:[W1011 02:54:37.074738214 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
120: [rank120]:[W1011 02:54:37.326941003 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
112: [rank112]:[W1011 02:54:37.962595873 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 0, retcode 3
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 8, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 1, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 1, retcode 3
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 2, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 2, retcode 3
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 2, retcode 3
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 3, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 3, retcode 3
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 3, retcode 3
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 3, retcode 3
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 9, retcode 3
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 9, retcode 3
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 4, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 4, retcode 3
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 4, retcode 3
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 4, retcode 3
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 4, retcode 3
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 10, retcode 3
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 10, retcode 3
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 10, retcode 3
  5: 
  5: GPU-94:2299767:2302242 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 5, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 5, retcode 3
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 5, retcode 3
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 5, retcode 3
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 5, retcode 3
  5: 
  5: GPU-94:2299767:2302242 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  5: 
  5: GPU-94:2299767:2302242 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 5, retcode 3
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 16, retcode 3
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 11, retcode 3
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 11, retcode 3
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 11, retcode 3
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 11, retcode 3
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 24, retcode 3
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 12, retcode 3
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 12, retcode 3
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 12, retcode 3
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 12, retcode 3
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 12, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 17, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 17, retcode 3
  6: 
  6: GPU-94:2299783:2302240 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 6, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 6, retcode 3
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 6, retcode 3
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 6, retcode 3
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 6, retcode 3
  5: 
  5: GPU-94:2299767:2302242 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  5: 
  5: GPU-94:2299767:2302242 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 6, retcode 3
  6: 
  6: GPU-94:2299783:2302240 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  6: 
  6: GPU-94:2299783:2302240 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 6, retcode 3
  7: 
  7: GPU-94:2299855:2302241 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  0: 
  0: GPU-94:2299751:2302254 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 7, retcode 3
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  1: 
  1: GPU-94:2299823:2302248 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 7, retcode 3
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  2: 
  2: GPU-94:2299854:2302249 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 7, retcode 3
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  3: 
  3: GPU-94:2299822:2302246 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 7, retcode 3
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  4: 
  4: GPU-94:2299799:2302244 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 7, retcode 3
  5: 
  5: GPU-94:2299767:2302242 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  5: 
  5: GPU-94:2299767:2302242 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 7, retcode 3
  6: 
  6: GPU-94:2299783:2302240 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  6: 
  6: GPU-94:2299783:2302240 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 7, retcode 3
  7: 
  7: GPU-94:2299855:2302241 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  7: 
  7: GPU-94:2299855:2302241 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 7, retcode 3
 13: 
 13: GPU-180:1658312:1660742 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 13, retcode 3
 14: 
 14: GPU-180:1658352:1660741 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 13, retcode 3
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 13, retcode 3
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 14, retcode 3
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 13, retcode 3
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 14, retcode 3
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 13, retcode 3
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 14, retcode 3
 13: 
 13: GPU-180:1658312:1660742 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 13: 
 13: GPU-180:1658312:1660742 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 13, retcode 3
 15: 
 15: GPU-180:1658296:1660743 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 14, retcode 3
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 14, retcode 3
 14: 
 14: GPU-180:1658352:1660741 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 14: 
 14: GPU-180:1658352:1660741 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 13, retcode 3
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  8: 
  8: GPU-180:1658392:1660733 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 15, retcode 3
 13: 
 13: GPU-180:1658312:1660742 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 13: 
 13: GPU-180:1658312:1660742 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 14, retcode 3
 15: 
 15: GPU-180:1658296:1660743 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 15: 
 15: GPU-180:1658296:1660743 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 13, retcode 3
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  9: 
  9: GPU-180:1658376:1660734 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 15, retcode 3
 14: 
 14: GPU-180:1658352:1660741 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 14: 
 14: GPU-180:1658352:1660741 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 14, retcode 3
 15: 
 15: GPU-180:1658296:1660743 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 15: 
 15: GPU-180:1658296:1660743 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 14, retcode 3
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 10: 
 10: GPU-180:1658351:1660735 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 15, retcode 3
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 11: 
 11: GPU-180:1658408:1660737 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 15, retcode 3
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 12: 
 12: GPU-180:1658328:1660740 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 15, retcode 3
 13: 
 13: GPU-180:1658312:1660742 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 13: 
 13: GPU-180:1658312:1660742 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 15, retcode 3
 14: 
 14: GPU-180:1658352:1660741 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 14: 
 14: GPU-180:1658352:1660741 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 15, retcode 3
 15: 
 15: GPU-180:1658296:1660743 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 15: 
 15: GPU-180:1658296:1660743 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 15, retcode 3
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 19, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 19, retcode 3
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 18, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 19, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 18, retcode 3
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 18, retcode 3
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 19, retcode 3
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 18, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 25, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 25, retcode 3
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 21, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 21, retcode 3
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 21, retcode 3
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 21, retcode 3
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 21, retcode 3
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 20, retcode 3
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 21, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 20, retcode 3
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 20, retcode 3
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 20, retcode 3
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 20, retcode 3
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 20, retcode 3
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 26, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 26, retcode 3
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 26, retcode 3
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 32, retcode 3
 23: 
 23: GPU-146:1156670:1158459 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 22: 
 22: GPU-146:1156744:1158464 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 23, retcode 3
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 16: 
 16: GPU-146:1156671:1158454 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 22, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 23, retcode 3
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 23, retcode 3
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 17: 
 17: GPU-146:1156717:1158456 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 22, retcode 3
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 18: 
 18: GPU-146:1156722:1158455 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 22, retcode 3
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 22, retcode 3
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 22, retcode 3
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 22, retcode 3
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 19: 
 19: GPU-146:1156770:1158461 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 23, retcode 3
 22: 
 22: GPU-146:1156744:1158464 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 22: 
 22: GPU-146:1156744:1158464 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 22, retcode 3
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 20: 
 20: GPU-146:1156764:1158460 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 23, retcode 3
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 21: 
 21: GPU-146:1156695:1158462 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 23, retcode 3
 23: 
 23: GPU-146:1156670:1158459 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 23: 
 23: GPU-146:1156670:1158459 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 22, retcode 3
 22: 
 22: GPU-146:1156744:1158464 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 22: 
 22: GPU-146:1156744:1158464 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 23, retcode 3
 23: 
 23: GPU-146:1156670:1158459 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 23: 
 23: GPU-146:1156670:1158459 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 23, retcode 3
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 33, retcode 3
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 33, retcode 3
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 28, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 28, retcode 3
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 28, retcode 3
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 27, retcode 3
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 28, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 27, retcode 3
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 28, retcode 3
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 27, retcode 3
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 27, retcode 3
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 27, retcode 3
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 40, retcode 3
 29: 
 29: GPU-457:1144210:1145994 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 29, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 29, retcode 3
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 29, retcode 3
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 29, retcode 3
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 29, retcode 3
 29: 
 29: GPU-457:1144210:1145994 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 29: 
 29: GPU-457:1144210:1145994 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 29, retcode 3
 30: 
 30: GPU-457:1144235:1145993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 30: 
 30: GPU-457:1144235:1145993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 29, retcode 3
 31: 
 31: GPU-457:1144288:1145988 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 31: 
 31: GPU-457:1144288:1145988 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 29, retcode 3
 30: 
 30: GPU-457:1144235:1145993 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 30, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 30, retcode 3
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 30, retcode 3
 31: 
 31: GPU-457:1144288:1145988 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 30, retcode 3
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 24: 
 24: GPU-457:1144290:1145987 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 31, retcode 3
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 30, retcode 3
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 25: 
 25: GPU-457:1144294:1145991 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 31, retcode 3
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 26: 
 26: GPU-457:1144259:1145992 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 31, retcode 3
 29: 
 29: GPU-457:1144210:1145994 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 29: 
 29: GPU-457:1144210:1145994 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 30, retcode 3
 30: 
 30: GPU-457:1144235:1145993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 30: 
 30: GPU-457:1144235:1145993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 30, retcode 3
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 27: 
 27: GPU-457:1144234:1145990 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 31, retcode 3
 31: 
 31: GPU-457:1144288:1145988 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 31: 
 31: GPU-457:1144288:1145988 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 30, retcode 3
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 28: 
 28: GPU-457:1144194:1145989 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 31, retcode 3
 29: 
 29: GPU-457:1144210:1145994 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 29: 
 29: GPU-457:1144210:1145994 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 31, retcode 3
 30: 
 30: GPU-457:1144235:1145993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 30: 
 30: GPU-457:1144235:1145993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 31, retcode 3
 31: 
 31: GPU-457:1144288:1145988 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 31: 
 31: GPU-457:1144288:1145988 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 31, retcode 3
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 34, retcode 3
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 34, retcode 3
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 34, retcode 3
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 48, retcode 3
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 36, retcode 3
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 36, retcode 3
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 36, retcode 3
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 36, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 41, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 41, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 41, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 42, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 42, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 42, retcode 3
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 35, retcode 3
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 35, retcode 3
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 35, retcode 3
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 35, retcode 3
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 35, retcode 3
 37: 
 37: GPU-993:1139548:1141345 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 37, retcode 3
 38: 
 38: GPU-993:1139617:1141342 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 37, retcode 3
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 38, retcode 3
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 37, retcode 3
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 38, retcode 3
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 37, retcode 3
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 37, retcode 3
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 38, retcode 3
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 38, retcode 3
 37: 
 37: GPU-993:1139548:1141345 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 37: 
 37: GPU-993:1139548:1141345 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 37, retcode 3
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 38, retcode 3
 39: 
 39: GPU-993:1139574:1141343 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 38: 
 38: GPU-993:1139617:1141342 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 38: 
 38: GPU-993:1139617:1141342 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 37, retcode 3
 37: 
 37: GPU-993:1139548:1141345 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 37: 
 37: GPU-993:1139548:1141345 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 38, retcode 3
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 32: 
 32: GPU-993:1139551:1141353 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 39, retcode 3
 39: 
 39: GPU-993:1139574:1141343 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 39: 
 39: GPU-993:1139574:1141343 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 37, retcode 3
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 33: 
 33: GPU-993:1139618:1141339 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 39, retcode 3
 38: 
 38: GPU-993:1139617:1141342 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 38: 
 38: GPU-993:1139617:1141342 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 38, retcode 3
 39: 
 39: GPU-993:1139574:1141343 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 39: 
 39: GPU-993:1139574:1141343 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 38, retcode 3
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 34: 
 34: GPU-993:1139615:1141340 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 39, retcode 3
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 35: 
 35: GPU-993:1139526:1141341 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 39, retcode 3
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 36: 
 36: GPU-993:1139603:1141344 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 39, retcode 3
 37: 
 37: GPU-993:1139548:1141345 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 37: 
 37: GPU-993:1139548:1141345 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 39, retcode 3
 38: 
 38: GPU-993:1139617:1141342 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 38: 
 38: GPU-993:1139617:1141342 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 39, retcode 3
 39: 
 39: GPU-993:1139574:1141343 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 39: 
 39: GPU-993:1139574:1141343 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 39, retcode 3
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 43, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 43, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 50, retcode 3
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 49, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 43, retcode 3
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 50, retcode 3
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 49, retcode 3
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 43, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 50, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 49, retcode 3
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 56, retcode 3
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 44, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 44, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 44, retcode 3
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 44, retcode 3
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 44, retcode 3
 45: 
 45: GPU-130:1651889:1653679 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 45, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 45, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 45, retcode 3
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 45, retcode 3
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 45, retcode 3
 45: 
 45: GPU-130:1651889:1653679 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 45: 
 45: GPU-130:1651889:1653679 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 45, retcode 3
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 51, retcode 3
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 51, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 51, retcode 3
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 51, retcode 3
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 51, retcode 3
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 52, retcode 3
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 52, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 52, retcode 3
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 52, retcode 3
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 52, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 57, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 57, retcode 3
 46: 
 46: GPU-130:1651913:1653678 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 46, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 46, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 46, retcode 3
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 46, retcode 3
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 46, retcode 3
 45: 
 45: GPU-130:1651889:1653679 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 45: 
 45: GPU-130:1651889:1653679 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 46, retcode 3
 46: 
 46: GPU-130:1651913:1653678 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 46: 
 46: GPU-130:1651913:1653678 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 46, retcode 3
 53: 
 53: GPU-881:1137241:1139673 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 54: 
 54: GPU-881:1137278:1139672 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 53, retcode 3
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 54, retcode 3
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 53, retcode 3
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 54, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 53, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 54, retcode 3
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 53, retcode 3
 55: 
 55: GPU-881:1137225:1139674 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 54, retcode 3
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 53, retcode 3
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 48: 
 48: GPU-881:1137304:1139668 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 55, retcode 3
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 54, retcode 3
 53: 
 53: GPU-881:1137241:1139673 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 53: 
 53: GPU-881:1137241:1139673 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 53, retcode 3
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 49: 
 49: GPU-881:1137307:1139676 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 55, retcode 3
 53: 
 53: GPU-881:1137241:1139673 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 53: 
 53: GPU-881:1137241:1139673 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 54, retcode 3
 54: 
 54: GPU-881:1137278:1139672 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 54: 
 54: GPU-881:1137278:1139672 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 54, retcode 3
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 50: 
 50: GPU-881:1137337:1139669 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 55, retcode 3
 54: 
 54: GPU-881:1137278:1139672 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 54: 
 54: GPU-881:1137278:1139672 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 53, retcode 3
 55: 
 55: GPU-881:1137225:1139674 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 55: 
 55: GPU-881:1137225:1139674 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 53, retcode 3
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 51: 
 51: GPU-881:1137298:1139670 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 55, retcode 3
 55: 
 55: GPU-881:1137225:1139674 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 55: 
 55: GPU-881:1137225:1139674 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 54, retcode 3
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 52: 
 52: GPU-881:1137257:1139675 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 55, retcode 3
 53: 
 53: GPU-881:1137241:1139673 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 53: 
 53: GPU-881:1137241:1139673 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 55, retcode 3
 54: 
 54: GPU-881:1137278:1139672 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 54: 
 54: GPU-881:1137278:1139672 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 55, retcode 3
 55: 
 55: GPU-881:1137225:1139674 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 55: 
 55: GPU-881:1137225:1139674 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 55, retcode 3
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 72, retcode 3
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 64, retcode 3
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 58, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 58, retcode 3
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 58, retcode 3
 47: 
 47: GPU-130:1651888:1653680 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 40: 
 40: GPU-130:1651865:1653689 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 47, retcode 3
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 41: 
 41: GPU-130:1651936:1653686 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 47, retcode 3
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 42: 
 42: GPU-130:1651833:1653687 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 47, retcode 3
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 43: 
 43: GPU-130:1651937:1653682 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 47, retcode 3
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 44: 
 44: GPU-130:1651849:1653681 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 47, retcode 3
 45: 
 45: GPU-130:1651889:1653679 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 45: 
 45: GPU-130:1651889:1653679 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 47, retcode 3
 46: 
 46: GPU-130:1651913:1653678 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 46: 
 46: GPU-130:1651913:1653678 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 47, retcode 3
 47: 
 47: GPU-130:1651888:1653680 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 47: 
 47: GPU-130:1651888:1653680 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 47, retcode 3
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 73, retcode 3
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 65, retcode 3
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 73, retcode 3
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 65, retcode 3
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 60, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 61, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 60, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 61, retcode 3
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 60, retcode 3
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 61, retcode 3
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 60, retcode 3
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 60, retcode 3
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 59, retcode 3
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 61, retcode 3
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 61, retcode 3
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 60, retcode 3
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 61, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 59, retcode 3
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 59, retcode 3
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 59, retcode 3
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 59, retcode 3
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 59, retcode 3
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 74, retcode 3
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 74, retcode 3
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 74, retcode 3
 62: 
 62: GPU-351:1139365:1141082 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 62, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 62, retcode 3
 63: 
 63: GPU-351:1139368:1141083 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 62, retcode 3
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 62, retcode 3
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 56: 
 56: GPU-351:1139263:1141091 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 63, retcode 3
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 62, retcode 3
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 57: 
 57: GPU-351:1139361:1141089 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 63, retcode 3
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 62, retcode 3
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 58: 
 58: GPU-351:1139333:1141087 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 63, retcode 3
 62: 
 62: GPU-351:1139365:1141082 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 62: 
 62: GPU-351:1139365:1141082 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 62, retcode 3
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 59: 
 59: GPU-351:1139317:1141090 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 63, retcode 3
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 60: 
 60: GPU-351:1139293:1141088 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 63, retcode 3
 63: 
 63: GPU-351:1139368:1141083 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 63: 
 63: GPU-351:1139368:1141083 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 62, retcode 3
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 61: 
 61: GPU-351:1139292:1141086 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 63, retcode 3
 62: 
 62: GPU-351:1139365:1141082 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 62: 
 62: GPU-351:1139365:1141082 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 63, retcode 3
 63: 
 63: GPU-351:1139368:1141083 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 63: 
 63: GPU-351:1139368:1141083 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 63, retcode 3
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 66, retcode 3
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 66, retcode 3
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 66, retcode 3
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 67, retcode 3
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 66, retcode 3
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 67, retcode 3
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 67, retcode 3
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 67, retcode 3
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 68, retcode 3
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 68, retcode 3
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 69, retcode 3
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 68, retcode 3
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 69, retcode 3
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 68, retcode 3
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 70, retcode 3
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 69, retcode 3
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 70, retcode 3
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 69, retcode 3
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 68, retcode 3
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 70, retcode 3
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 69, retcode 3
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 68, retcode 3
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 64: 
 64: GPU-338:1136347:1138085 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 71, retcode 3
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 70, retcode 3
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 68, retcode 3
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 65: 
 65: GPU-338:1136403:1138083 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 71, retcode 3
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 66: 
 66: GPU-338:1136307:1138082 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 71, retcode 3
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 70, retcode 3
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 70, retcode 3
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 69, retcode 3
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 68, retcode 3
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 67: 
 67: GPU-338:1136346:1138080 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 71, retcode 3
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 70, retcode 3
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 69, retcode 3
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 69, retcode 3
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 68: 
 68: GPU-338:1136345:1138078 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 71, retcode 3
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 70, retcode 3
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 69: 
 69: GPU-338:1136344:1138076 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 71, retcode 3
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 70: 
 70: GPU-338:1136387:1138074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 71, retcode 3
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 71: 
 71: GPU-338:1136291:1138072 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 71, retcode 3
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 75, retcode 3
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 76, retcode 3
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 75, retcode 3
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 76, retcode 3
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 75, retcode 3
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 75, retcode 3
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 76, retcode 3
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 75, retcode 3
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 76, retcode 3
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 76, retcode 3
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 80, retcode 3
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 81, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 81, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 80, retcode 3
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 88, retcode 3
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 82, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 82, retcode 3
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 82, retcode 3
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 84, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 84, retcode 3
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 84, retcode 3
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 82, retcode 3
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 84, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 90, retcode 3
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 91, retcode 3
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 90, retcode 3
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 89, retcode 3
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 89, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 90, retcode 3
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 91, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 90, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 91, retcode 3
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 89, retcode 3
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 89, retcode 3
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 91, retcode 3
 96: 
 96: GPU-245:1142890:1145070 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:1142890:1145070 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 96: 
 96: GPU-245:1142890:1145070 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 96, retcode 3
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 97, retcode 3
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 96, retcode 3
 77: 
 77: GPU-8:1862508:1864334 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 77, retcode 3
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 77, retcode 3
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 77, retcode 3
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 77, retcode 3
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 77, retcode 3
 77: 
 77: GPU-8:1862508:1864334 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 77: 
 77: GPU-8:1862508:1864334 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 77, retcode 3
 79: 
 79: GPU-8:1862556:1864332 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 79, retcode 3
 78: 
 78: GPU-8:1862594:1864333 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 79, retcode 3
 72: 
 72: GPU-8:1862492:1864330 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 78, retcode 3
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 79, retcode 3
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 73: 
 73: GPU-8:1862600:1864327 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 78, retcode 3
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 79, retcode 3
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 74: 
 74: GPU-8:1862524:1864328 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 78, retcode 3
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 75: 
 75: GPU-8:1862555:1864329 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 78, retcode 3
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 79, retcode 3
 77: 
 77: GPU-8:1862508:1864334 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 77: 
 77: GPU-8:1862508:1864334 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 79, retcode 3
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 76: 
 76: GPU-8:1862554:1864331 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 78, retcode 3
 78: 
 78: GPU-8:1862594:1864333 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 78: 
 78: GPU-8:1862594:1864333 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 79, retcode 3
 77: 
 77: GPU-8:1862508:1864334 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 77: 
 77: GPU-8:1862508:1864334 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 78, retcode 3
 79: 
 79: GPU-8:1862556:1864332 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 78: 
 78: GPU-8:1862594:1864333 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 78: 
 78: GPU-8:1862594:1864333 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 78, retcode 3
 79: 
 79: GPU-8:1862556:1864332 [7] proxy.cc:1521 NCCL WARN [Proxy Service 79] Failed to execute operation Close from rank 79, retcode 3
 79: 
 79: GPU-8:1862556:1864332 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 79: 
 79: GPU-8:1862556:1864332 [7] proxy.cc:1521 NCCL WARN [Proxy Service 79] Failed to execute operation Close from rank 78, retcode 3
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 83, retcode 3
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 85, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 83, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 85, retcode 3
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 83, retcode 3
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 85, retcode 3
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 83, retcode 3
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 85, retcode 3
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 85, retcode 3
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 83, retcode 3
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 85, retcode 3
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 83, retcode 3
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 98, retcode 3
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 98, retcode 3
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 104, retcode 3
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 104, retcode 3
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 105, retcode 3
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 105, retcode 3
 87: 
 87: GPU-342:1137178:1139283 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 87, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 87, retcode 3
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 87, retcode 3
 86: 
 86: GPU-342:1137084:1139282 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 80: 
 80: GPU-342:1137153:1139277 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 86, retcode 3
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 87, retcode 3
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 87, retcode 3
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 81: 
 81: GPU-342:1137107:1139278 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 86, retcode 3
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 87, retcode 3
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 82: 
 82: GPU-342:1137173:1139281 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 86, retcode 3
 86: 
 86: GPU-342:1137084:1139282 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 86: 
 86: GPU-342:1137084:1139282 [6] proxy.cc:1521 NCCL WARN [Proxy Service 86] Failed to execute operation Close from rank 87, retcode 3
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 83: 
 83: GPU-342:1137108:1139279 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 86, retcode 3
 87: 
 87: GPU-342:1137178:1139283 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 87: 
 87: GPU-342:1137178:1139283 [7] proxy.cc:1521 NCCL WARN [Proxy Service 87] Failed to execute operation Close from rank 87, retcode 3
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 84: 
 84: GPU-342:1137184:1139284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 86, retcode 3
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 85: 
 85: GPU-342:1137132:1139280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 86, retcode 3
 86: 
 86: GPU-342:1137084:1139282 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 86: 
 86: GPU-342:1137084:1139282 [6] proxy.cc:1521 NCCL WARN [Proxy Service 86] Failed to execute operation Close from rank 86, retcode 3
 87: 
 87: GPU-342:1137178:1139283 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 87: 
 87: GPU-342:1137178:1139283 [7] proxy.cc:1521 NCCL WARN [Proxy Service 87] Failed to execute operation Close from rank 86, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 113, retcode 3
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 92, retcode 3
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 92, retcode 3
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 93, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 92, retcode 3
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 94, retcode 3
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 93, retcode 3
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 92, retcode 3
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 94, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 93, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 94, retcode 3
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 93, retcode 3
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 92, retcode 3
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 94, retcode 3
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 93, retcode 3
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 94, retcode 3
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 92, retcode 3
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 93, retcode 3
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 92, retcode 3
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 94, retcode 3
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 93, retcode 3
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 94, retcode 3
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 115, retcode 3
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 115, retcode 3
 95: 
 95: GPU-234:1145475:1147609 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 88: 
 88: GPU-234:1145494:1147602 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 95, retcode 3
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 121, retcode 3
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 89: 
 89: GPU-234:1145405:1147601 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 95, retcode 3
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 90: 
 90: GPU-234:1145453:1147603 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 95, retcode 3
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 91: 
 91: GPU-234:1145421:1147607 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 95, retcode 3
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 92: 
 92: GPU-234:1145437:1147610 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 95, retcode 3
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 93: 
 93: GPU-234:1145492:1147611 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 95, retcode 3
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 94: 
 94: GPU-234:1145509:1147608 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 95, retcode 3
 95: 
 95: GPU-234:1145475:1147609 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 95: 
 95: GPU-234:1145475:1147609 [7] proxy.cc:1521 NCCL WARN [Proxy Service 95] Failed to execute operation Close from rank 95, retcode 3
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 99, retcode 3
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 99, retcode 3
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 99, retcode 3
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 112, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 112, retcode 3
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 112, retcode 3
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 106, retcode 3
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 106, retcode 3
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 106, retcode 3
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 120, retcode 3
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 120, retcode 3
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 107, retcode 3
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 107, retcode 3
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 107, retcode 3
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 107, retcode 3
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 123, retcode 3
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 123, retcode 3
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 123, retcode 3
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 101, retcode 3
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 101, retcode 3
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 102, retcode 3
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 101, retcode 3
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 102, retcode 3
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 102, retcode 3
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 101, retcode 3
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 103, retcode 3
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 102, retcode 3
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 101, retcode 3
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 97: 
 97: GPU-245:1142906:1145071 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 100, retcode 3
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 103, retcode 3
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 102, retcode 3
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 101, retcode 3
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 103, retcode 3
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 102, retcode 3
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1521 NCCL WARN [Proxy Service 103] Failed to execute operation Close from rank 101, retcode 3
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 98: 
 98: GPU-245:1142929:1145077 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 100, retcode 3
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 103, retcode 3
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 99: 
 99: GPU-245:1142976:1145072 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 100, retcode 3
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
100: 
100: GPU-245:1142966:1145079 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 100, retcode 3
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 103, retcode 3
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1521 NCCL WARN [Proxy Service 103] Failed to execute operation Close from rank 102, retcode 3
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
101: 
101: GPU-245:1143002:1145075 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 100, retcode 3
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 103, retcode 3
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
102: 
102: GPU-245:1142967:1145074 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 100, retcode 3
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1521 NCCL WARN [Proxy Service 103] Failed to execute operation Close from rank 103, retcode 3
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
103: 
103: GPU-245:1142930:1145076 [7] proxy.cc:1521 NCCL WARN [Proxy Service 103] Failed to execute operation Close from rank 100, retcode 3
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 117, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 117, retcode 3
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 117, retcode 3
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 116, retcode 3
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 117, retcode 3
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 117, retcode 3
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 117, retcode 3
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 114, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 116, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 116, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 114, retcode 3
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 116, retcode 3
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 114, retcode 3
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 114, retcode 3
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 116, retcode 3
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 114, retcode 3
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 116, retcode 3
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 114, retcode 3
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 122, retcode 3
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 122, retcode 3
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 122, retcode 3
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 122, retcode 3
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 109, retcode 3
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 109, retcode 3
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 110, retcode 3
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 110, retcode 3
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 109, retcode 3
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 110, retcode 3
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 109, retcode 3
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 110, retcode 3
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 109, retcode 3
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 108, retcode 3
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 110, retcode 3
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 109, retcode 3
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
104: 
104: GPU-186:1138374:1140176 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 111, retcode 3
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 108, retcode 3
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 110, retcode 3
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 110, retcode 3
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 109, retcode 3
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
105: 
105: GPU-186:1138393:1140178 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 111, retcode 3
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 108, retcode 3
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 109, retcode 3
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 110, retcode 3
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
106: 
106: GPU-186:1138438:1140177 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 111, retcode 3
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 108, retcode 3
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
107: 
107: GPU-186:1138464:1140179 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 111, retcode 3
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 108, retcode 3
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
108: 
108: GPU-186:1138472:1140182 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 111, retcode 3
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 108, retcode 3
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
109: 
109: GPU-186:1138422:1140184 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 111, retcode 3
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 108, retcode 3
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
110: 
110: GPU-186:1138403:1140180 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 111, retcode 3
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 108, retcode 3
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
111: 
111: GPU-186:1138473:1140183 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 111, retcode 3
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 125, retcode 3
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 125, retcode 3
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 125, retcode 3
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 124, retcode 3
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 125, retcode 3
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 125, retcode 3
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 124, retcode 3
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 125, retcode 3
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 124, retcode 3
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 124, retcode 3
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 124, retcode 3
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 124, retcode 3
118: 
118: GPU-792:1125552:1127277 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 118, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 118, retcode 3
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 118, retcode 3
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 118, retcode 3
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 118, retcode 3
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 118, retcode 3
118: 
118: GPU-792:1125552:1127277 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
118: 
118: GPU-792:1125552:1127277 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 118, retcode 3
119: 
119: GPU-792:1125565:1127281 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
112: 
112: GPU-792:1125520:1127279 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 119, retcode 3
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
113: 
113: GPU-792:1125486:1127276 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 119, retcode 3
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
114: 
114: GPU-792:1125514:1127278 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 119, retcode 3
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
115: 
115: GPU-792:1125554:1127282 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 119, retcode 3
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
116: 
116: GPU-792:1125483:1127283 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 119, retcode 3
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
117: 
117: GPU-792:1125461:1127280 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 119, retcode 3
118: 
118: GPU-792:1125552:1127277 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
118: 
118: GPU-792:1125552:1127277 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 119, retcode 3
119: 
119: GPU-792:1125565:1127281 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
119: 
119: GPU-792:1125565:1127281 [7] proxy.cc:1521 NCCL WARN [Proxy Service 119] Failed to execute operation Close from rank 119, retcode 3
126: 
126: GPU-332:1138546:1140328 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 126, retcode 3
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 126, retcode 3
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 126, retcode 3
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 126, retcode 3
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 126, retcode 3
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 126, retcode 3
126: 
126: GPU-332:1138546:1140328 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
126: 
126: GPU-332:1138546:1140328 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 126, retcode 3
127: 
127: GPU-332:1138580:1140331 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
120: 
120: GPU-332:1138583:1140327 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 127, retcode 3
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
121: 
121: GPU-332:1138554:1140333 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 127, retcode 3
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
122: 
122: GPU-332:1138606:1140326 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 127, retcode 3
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
123: 
123: GPU-332:1138630:1140334 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 127, retcode 3
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
124: 
124: GPU-332:1138629:1140329 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 127, retcode 3
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
125: 
125: GPU-332:1138526:1140330 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 127, retcode 3
126: 
126: GPU-332:1138546:1140328 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
126: 
126: GPU-332:1138546:1140328 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 127, retcode 3
127: 
127: GPU-332:1138580:1140331 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
127: 
127: GPU-332:1138580:1140331 [7] proxy.cc:1521 NCCL WARN [Proxy Service 127] Failed to execute operation Close from rank 127, retcode 3
  0: ENDING TIMING RUN AT 2024-10-11 02:54:45 AM
  0: RESULT,LLM_FINETUNING,,601,nvidia,2024-10-11 02:44:44 AM
++ date +%s
+ echo 'RUNANDTIME_STOP 1728615296'
RUNANDTIME_STOP 1728615296
+ set -e
