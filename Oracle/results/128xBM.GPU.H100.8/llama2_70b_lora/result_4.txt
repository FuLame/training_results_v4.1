+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 222 16 GPU-[94,180,146,457,993,130,881,351,338,8,342,234,245,186,792,332] BM.GPU.H100.8 Cluster DGXH100_16x8x1xtp4pp1cp2'
:::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 222 16 GPU-[94,180,146,457,993,130,881,351,338,8,342,234,245,186,792,332] BM.GPU.H100.8 Cluster DGXH100_16x8x1xtp4pp1cp2
++ srun --ntasks=1 --container-name=llama2_70b_lora_222 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"16","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ srun --ntasks=1 --container-name=llama2_70b_lora_222 bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
:::GITCOMMITID  
+ '[' 1 -eq 1 ']'
+ srun --ntasks=16 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on GPU-180
Clearing cache on GPU-342
Clearing cache on GPU-245
Clearing cache on GPU-881
Clearing cache on GPU-186
Clearing cache on GPU-234
Clearing cache on GPU-332
Clearing cache on GPU-338
Clearing cache on GPU-792
Clearing cache on GPU-130
Clearing cache on GPU-351
Clearing cache on GPU-457
Clearing cache on GPU-993
Clearing cache on GPU-94
Clearing cache on GPU-146
Clearing cache on GPU-8
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ export SEED=4905
+ SEED=4905
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1728604653'
RUNANDTIME_START 1728604653
+ srun -l --mpi=pmix --ntasks=128 --ntasks-per-node=8 --time=20 --container-name=llama2_70b_lora_222 --container-mounts=/mnt/orangefs/mlperf/llama/data_model/gov_report:/data:ro,/mnt/orangefs/mlperf/llama/data_model/model:/ckpt:ro,/mnt/orangefs/mlperf/llama/logs:/results:rw --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
 22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 70: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
127: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
118: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
119: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 91: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
125: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 75: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 78: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
112: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
120: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 88: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 96: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
113: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  0: STARTING TIMING RUN AT 2024-10-10 11:57:42 PM
 76: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 87: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 93: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 79: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 74: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 84: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 81: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
108: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
103: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 99: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
109: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
114: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
116: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
117: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
115: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
122: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 90: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 94: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
105: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 95: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 92: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 89: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
110: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
124: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 72: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 85: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 73: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 77: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 86: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 82: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 83: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 80: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
100: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
101: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 97: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 98: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
102: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
121: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
123: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
126: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
106: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
104: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
107: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
111: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 71: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 65: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 67: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 66: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 68: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 64: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 69: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
  2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: 
  0: 
  0: ************** Experiment configuration ***********
  0: 
  0: model:
  0:   ub_tp_comm_overlap_cfg:
  0:     qkv_fprop:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     fc1_fprop:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     proj_dgrad:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     fc2_dgrad:
  0:       method: ring_exchange
  0:       aggregate: 0
  0:     proj_fprop:
  0:       method: pipeline
  0:       num_sm: 32
  0:       cga_size: 2
  0:       num_splits: 4
  0:       set_sm_margin: 1
  0:       atomic_gemm: 1
  0:       fp8_buf: 1
  0:     fc2_fprop:
  0:       method: pipeline
  0:       num_sm: 16
  0:       cga_size: 2
  0:       num_splits: 4
  0:       set_sm_margin: 1
  0:       atomic_gemm: 1
  0:       fp8_buf: 0
  0:     qkv_dgrad:
  0:       method: bulk
  0:       num_sm: 4
  0:       cga_size: 2
  0:       set_sm_margin: 0
  0:     fc1_dgrad:
  0:       method: ring_exchange
  0:       num_sm: 1
  0:       cga_size: 2
  0:       set_sm_margin: 1
  0:       atomic_gemm: 0
  0:       fp8_buf: 0
  0:   mcore_gpt: true
  0:   seed: 4905
  0:   tensor_model_parallel_size: 4
  0:   pipeline_model_parallel_size: 1
  0:   context_parallel_size: 2
  0:   cpu_offloading: false
  0:   dist_ckpt_load_strictness: log_all
  0:   global_batch_size: 16
  0:   micro_batch_size: 1
  0:   max_position_embeddings: 8192
  0:   encoder_seq_length: 8192
  0:   restore_from_path: /ckpt
  0:   resume_from_checkpoint: null
  0:   save_nemo_on_validation_end: false
  0:   sync_batch_comm: false
  0:   megatron_amp_O2: true
  0:   sequence_parallel: 1
  0:   activations_checkpoint_granularity: null
  0:   activations_checkpoint_method: null
  0:   activations_checkpoint_num_layers: null
  0:   activations_checkpoint_layers_per_pipeline: null
  0:   answer_only_loss: true
  0:   gradient_as_bucket_view: false
  0:   hidden_dropout: 0.0
  0:   attention_dropout: 0.0
  0:   ffn_dropout: 0.0
  0:   bias_activation_fusion: true
  0:   bias_dropout_add_fusion: false
  0:   transformer_engine: true
  0:   fp8: true
  0:   fp8_params: true
  0:   fp8_hybrid: true
  0:   fp8_amax_history_len: 32
  0:   fp8_amax_compute_algo: max
  0:   reduce_amax: false
  0:   fp8_e4m3: false
  0:   fp8_interval: 1
  0:   fp8_margin: 0
  0:   fp8_dot_product_attention: 0
  0:   activation_func_fp8_input_store: 0
  0:   apply_rope_fusion: true
  0:   disable_parameter_transpose_cache: true
  0:   ub_tp_comm_overlap: 1
  0:   tp_comm_overlap_ag: true
  0:   tp_comm_overlap_rs: true
  0:   tp_comm_overlap_rs_dgrad: false
  0:   tp_comm_overlap_disable_qkv: true
  0:   batch_p2p_comm: 'False'
  0:   virtual_pipeline_model_parallel_size: 1
  0:   sharp: true
  0:   nccl_communicator_config_path: null
  0:   peft:
  0:     peft_scheme: lora
  0:     restore_from_path: null
  0:     lora_tuning:
  0:       adapter_dim: 16
  0:       alpha: 32
  0:       adapter_dropout: 0.1
  0:       dropout_position: pre
  0:       target_modules:
  0:       - attention
  0:       column_init_method: kaiming
  0:       row_init_method: zero
  0:       layer_selection: null
  0:       weight_tying: false
  0:       position_embedding_strategy: null
  0:       a2a_experimental: 1
  0:   data:
  0:     multiprocessing_context: spawn
  0:     pin_memory: true
  0:     sample_weight: constant
  0:     validation_drop_last: false
  0:     train_ds:
  0:       file_names:
  0:       - /data/train.npy
  0:       packed_sequence: true
  0:       packed_sequence_return_cu_seqlen: false
  0:       index_mapping_dir: /results/data_index/train
  0:       global_batch_size: 16
  0:       micro_batch_size: 1
  0:       shuffle: true
  0:       num_workers: 1
  0:       memmap_workers: 2
  0:       pin_memory: true
  0:       max_seq_length: 8192
  0:       min_seq_length: 1
  0:       drop_last: true
  0:       concat_sampling_probabilities:
  0:       - 1.0
  0:       label_key: output
  0:       add_eos: true
  0:       add_sep: false
  0:       add_bos: false
  0:       truncation_field: input
  0:       prompt_template: '{input} {output}'
  0:       truncation_method: right
  0:       seed: 4905
  0:     validation_ds:
  0:       file_names:
  0:       - /data/validation.npy
  0:       packed_sequence: true
  0:       packed_sequence_return_cu_seqlen: false
  0:       index_mapping_dir: /results/data_index/val
  0:       names: null
  0:       global_batch_size: 16
  0:       micro_batch_size: 1
  0:       shuffle: false
  0:       num_workers: 1
  0:       memmap_workers: 2
  0:       pin_memory: true
  0:       max_seq_length: 8192
  0:       min_seq_length: 1
  0:       drop_last: false
  0:       label_key: output
  0:       add_eos: true
  0:       add_sep: false
  0:       add_bos: false
  0:       write_predictions_to_file: false
  0:       output_file_path_prefix: null
  0:       truncation_field: input
  0:       prompt_template: '{input} {output}'
  0:       tokens_to_generate: 32
  0:       truncation_method: right
  0:       metric:
  0:         name: loss
  0:         average: null
  0:         num_classes: null
  0:   optim:
  0:     name: mcore_distributed_optim
  0:     overlap_grad_sync: true
  0:     overlap_param_sync: true
  0:     delay_grad_reduce: true
  0:     delay_param_gather: true
  0:     average_in_collective: false
  0:     lr: 0.00035
  0:     min_lr: 0
  0:     weight_decay: 0.0001
  0:     betas:
  0:     - 0.9
  0:     - 0.999
  0:     eps: 1.0e-08
  0:     amsgrad: false
  0:     sched:
  0:       name: CosineAnnealing
  0:       warmup_ratio: 0.0
  0:       min_lr: 0.0
  0:       constant_steps: 0
  0:       monitor: val_loss
  0:       reduce_on_plateau: false
  0:   enable_cuda_graph: 1
  0:   enable_cg_fp8_weight_caching: true
  0:   custom:
  0:     warmup: true
  0:     warmup_train_steps: 5
  0:     warmup_validation_steps: 5
  0:     reset_fp8_stats_after_warmup: 1
  0: name: megatron_gpt_peft_lora_tuning
  0: trainer:
  0:   devices: 8
  0:   num_nodes: 16
  0:   accelerator: gpu
  0:   precision: bf16-mixed
  0:   max_steps: 1024
  0:   val_check_interval: 120
  0:   check_val_every_n_epoch: null
  0:   log_every_n_steps: 0
  0:   gradient_clip_val: 0.3
  0:   gradient_clip_algorithm: norm
  0:   num_sanity_val_steps: 0
  0:   max_epochs: 1000
  0:   limit_val_batches: 1.0
  0:   limit_train_batches: 1.0
  0:   limit_test_batches: 0
  0:   logger: false
  0:   enable_checkpointing: false
  0:   use_distributed_sampler: false
  0:   enable_progress_bar: false
  0: exp_manager:
  0:   log_tflops_per_sec_per_gpu: false
  0:   explicit_log_dir: null
  0:   exp_dir: /results
  0:   create_wandb_logger: false
  0:   resume_if_exists: false
  0:   resume_ignore_no_checkpoint: true
  0:   create_checkpoint_callback: false
  0:   log_global_rank_0_only: true
  0:   create_early_stopping_callback: false
  0:   create_tensorboard_logger: false
  0: 
  0: GPU available: True (cuda), used: True
  0: TPU available: False, using: 0 TPU cores
  0: HPU available: False, using: 0 HPUs
  0: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
  0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
 71: Initializing distributed: GLOBAL_RANK: 71, MEMBER: 72/128
 37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/128
 38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/128
 32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/128
 39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/128
 68: Initializing distributed: GLOBAL_RANK: 68, MEMBER: 69/128
 33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/128
 65: Initializing distributed: GLOBAL_RANK: 65, MEMBER: 66/128
 70: Initializing distributed: GLOBAL_RANK: 70, MEMBER: 71/128
 22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/128
 18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/128
 16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/128
 21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/128
 66: Initializing distributed: GLOBAL_RANK: 66, MEMBER: 67/128
 35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/128
 36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/128
 34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/128
 67: Initializing distributed: GLOBAL_RANK: 67, MEMBER: 68/128
 64: Initializing distributed: GLOBAL_RANK: 64, MEMBER: 65/128
 69: Initializing distributed: GLOBAL_RANK: 69, MEMBER: 70/128
 20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/128
 17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/128
 23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/128
 19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/128
114: Initializing distributed: GLOBAL_RANK: 114, MEMBER: 115/128
112: Initializing distributed: GLOBAL_RANK: 112, MEMBER: 113/128
117: Initializing distributed: GLOBAL_RANK: 117, MEMBER: 118/128
113: Initializing distributed: GLOBAL_RANK: 113, MEMBER: 114/128
116: Initializing distributed: GLOBAL_RANK: 116, MEMBER: 117/128
126: Initializing distributed: GLOBAL_RANK: 126, MEMBER: 127/128
127: Initializing distributed: GLOBAL_RANK: 127, MEMBER: 128/128
121: Initializing distributed: GLOBAL_RANK: 121, MEMBER: 122/128
 12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/128
 13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/128
  8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/128
122: Initializing distributed: GLOBAL_RANK: 122, MEMBER: 123/128
115: Initializing distributed: GLOBAL_RANK: 115, MEMBER: 116/128
119: Initializing distributed: GLOBAL_RANK: 119, MEMBER: 120/128
118: Initializing distributed: GLOBAL_RANK: 118, MEMBER: 119/128
 43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/128
 45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/128
 46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/128
124: Initializing distributed: GLOBAL_RANK: 124, MEMBER: 125/128
120: Initializing distributed: GLOBAL_RANK: 120, MEMBER: 121/128
 40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/128
123: Initializing distributed: GLOBAL_RANK: 123, MEMBER: 124/128
 14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/128
125: Initializing distributed: GLOBAL_RANK: 125, MEMBER: 126/128
 41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/128
 15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/128
 10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/128
 47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/128
 11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/128
 58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/128
  9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/128
 44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/128
 42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/128
 61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/128
 63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/128
 57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/128
 59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/128
 91: Initializing distributed: GLOBAL_RANK: 91, MEMBER: 92/128
 88: Initializing distributed: GLOBAL_RANK: 88, MEMBER: 89/128
 95: Initializing distributed: GLOBAL_RANK: 95, MEMBER: 96/128
 92: Initializing distributed: GLOBAL_RANK: 92, MEMBER: 93/128
 90: Initializing distributed: GLOBAL_RANK: 90, MEMBER: 91/128
 93: Initializing distributed: GLOBAL_RANK: 93, MEMBER: 94/128
 56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/128
 25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/128
 24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/128
 27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/128
 31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/128
 60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/128
 89: Initializing distributed: GLOBAL_RANK: 89, MEMBER: 90/128
 30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/128
 62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/128
 94: Initializing distributed: GLOBAL_RANK: 94, MEMBER: 95/128
 77: Initializing distributed: GLOBAL_RANK: 77, MEMBER: 78/128
 79: Initializing distributed: GLOBAL_RANK: 79, MEMBER: 80/128
 76: Initializing distributed: GLOBAL_RANK: 76, MEMBER: 77/128
 29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/128
 28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/128
 26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/128
101: Initializing distributed: GLOBAL_RANK: 101, MEMBER: 102/128
 99: Initializing distributed: GLOBAL_RANK: 99, MEMBER: 100/128
 96: Initializing distributed: GLOBAL_RANK: 96, MEMBER: 97/128
110: Initializing distributed: GLOBAL_RANK: 110, MEMBER: 111/128
111: Initializing distributed: GLOBAL_RANK: 111, MEMBER: 112/128
105: Initializing distributed: GLOBAL_RANK: 105, MEMBER: 106/128
 75: Initializing distributed: GLOBAL_RANK: 75, MEMBER: 76/128
 74: Initializing distributed: GLOBAL_RANK: 74, MEMBER: 75/128
 78: Initializing distributed: GLOBAL_RANK: 78, MEMBER: 79/128
107: Initializing distributed: GLOBAL_RANK: 107, MEMBER: 108/128
 72: Initializing distributed: GLOBAL_RANK: 72, MEMBER: 73/128
 97: Initializing distributed: GLOBAL_RANK: 97, MEMBER: 98/128
 83: Initializing distributed: GLOBAL_RANK: 83, MEMBER: 84/128
 80: Initializing distributed: GLOBAL_RANK: 80, MEMBER: 81/128
 73: Initializing distributed: GLOBAL_RANK: 73, MEMBER: 74/128
103: Initializing distributed: GLOBAL_RANK: 103, MEMBER: 104/128
108: Initializing distributed: GLOBAL_RANK: 108, MEMBER: 109/128
 87: Initializing distributed: GLOBAL_RANK: 87, MEMBER: 88/128
109: Initializing distributed: GLOBAL_RANK: 109, MEMBER: 110/128
 86: Initializing distributed: GLOBAL_RANK: 86, MEMBER: 87/128
 98: Initializing distributed: GLOBAL_RANK: 98, MEMBER: 99/128
104: Initializing distributed: GLOBAL_RANK: 104, MEMBER: 105/128
102: Initializing distributed: GLOBAL_RANK: 102, MEMBER: 103/128
106: Initializing distributed: GLOBAL_RANK: 106, MEMBER: 107/128
100: Initializing distributed: GLOBAL_RANK: 100, MEMBER: 101/128
 85: Initializing distributed: GLOBAL_RANK: 85, MEMBER: 86/128
 84: Initializing distributed: GLOBAL_RANK: 84, MEMBER: 85/128
 81: Initializing distributed: GLOBAL_RANK: 81, MEMBER: 82/128
 82: Initializing distributed: GLOBAL_RANK: 82, MEMBER: 83/128
  0: setting number of microbatches to constant 1
 52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/128
 49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/128
 54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/128
 53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/128
 55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/128
 48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/128
 50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/128
 51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/128
  1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/128
  6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/128
  3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/128
  2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/128
  4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/128
  0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/128
  7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/128
  5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/128
  3: NCCL version 2.22.3+cuda12.6
  2: NCCL version 2.22.3+cuda12.6
  0: ----------------------------------------------------------------------------------------------------
  0: distributed_backend=nccl
  0: All distributed processes registered. Starting with 128 processes
  0: ----------------------------------------------------------------------------------------------------
  0: 
  1: NCCL version 2.22.3+cuda12.6
  0: The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
  0: NCCL version 2.22.3+cuda12.6
  0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
  0: Loading distributed checkpoint directly on the GPU
 96: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 80: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 72: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 32: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 56: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 24: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 64: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 16: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 40: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
120: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 88: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 48: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
112: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  8: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
104: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 80: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 16: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 40: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
120: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
112: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 56: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 72: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
  0: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 88: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 48: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 32: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
  8: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 64: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 24: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 96: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
104: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 80: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 16: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
112: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 40: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 56: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 72: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 48: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 32: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 88: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
120: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  8: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 64: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 96: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 24: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
104: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
  0: > building indices for blendable datasets ...
  0:  > sample ratios:
  0:    dataset 0, input: 1, achieved: 1
  0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
112: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 73: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 64: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
120: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
114: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 72: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
121: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
115: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 74: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 80: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 65: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
122: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
116: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 75: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 96: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 66: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
123: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
117: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 76: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 97: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 88: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 68: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
124: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
118: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 98: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 81: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 69: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
125: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
119: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 99: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 82: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 71: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
126: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
113: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 77: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
104: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
100: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 89: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 83: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 67: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
127: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 78: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
105: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
101: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 90: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 84: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 70: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 79: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
106: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
102: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 91: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 85: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
107: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
103: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 92: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 86: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
108: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 93: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 87: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
109: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 94: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
110: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 95: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
111: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
  0: Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
  0: Number of buckets for gradient all-reduce / reduce-scatter: 1
  0: Params for bucket 1 (11141120 elements):
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
  0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
  0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
  0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
  0: Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00035, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
  0: 
  0:   | Name         | Type | Params | Mode
  0: ---------------------------------------------
  0:   | other params | n/a  | 17.3 B | n/a 
  0: ---------------------------------------------
  0: 11.1 M    Trainable params
  0: 17.2 B    Non-trainable params
  0: 17.3 B    Total params
  0: 69,029.364Total estimated model params size (MB)
  0: 0         Modules in train mode
  0: 0         Modules in eval mode
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956190, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 332}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 333}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "16xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4905, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 335}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956191, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 341}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956846, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 346}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956880, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 350}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956880, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 354}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956881, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 358}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956881, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 362}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956881, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 367}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956881, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 368}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956881, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 369}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956881, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 370}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728604956881, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 371}}
  0: SLURM auto-requeueing enabled. Setting signal handlers.
  4: SLURM auto-requeueing enabled. Setting signal handlers.
  5: SLURM auto-requeueing enabled. Setting signal handlers.
  7: SLURM auto-requeueing enabled. Setting signal handlers.
  6: SLURM auto-requeueing enabled. Setting signal handlers.
  2: SLURM auto-requeueing enabled. Setting signal handlers.
 16: SLURM auto-requeueing enabled. Setting signal handlers.
 80: SLURM auto-requeueing enabled. Setting signal handlers.
 48: SLURM auto-requeueing enabled. Setting signal handlers.
 10: SLURM auto-requeueing enabled. Setting signal handlers.
 40: SLURM auto-requeueing enabled. Setting signal handlers.
 32: SLURM auto-requeueing enabled. Setting signal handlers.
112: SLURM auto-requeueing enabled. Setting signal handlers.
 56: SLURM auto-requeueing enabled. Setting signal handlers.
 49: SLURM auto-requeueing enabled. Setting signal handlers.
 24: SLURM auto-requeueing enabled. Setting signal handlers.
 13: SLURM auto-requeueing enabled. Setting signal handlers.
 41: SLURM auto-requeueing enabled. Setting signal handlers.
 33: SLURM auto-requeueing enabled. Setting signal handlers.
121: SLURM auto-requeueing enabled. Setting signal handlers.
113: SLURM auto-requeueing enabled. Setting signal handlers.
 72: SLURM auto-requeueing enabled. Setting signal handlers.
 59: SLURM auto-requeueing enabled. Setting signal handlers.
  1: SLURM auto-requeueing enabled. Setting signal handlers.
 81: SLURM auto-requeueing enabled. Setting signal handlers.
 50: SLURM auto-requeueing enabled. Setting signal handlers.
 25: SLURM auto-requeueing enabled. Setting signal handlers.
 11: SLURM auto-requeueing enabled. Setting signal handlers.
 42: SLURM auto-requeueing enabled. Setting signal handlers.
 34: SLURM auto-requeueing enabled. Setting signal handlers.
122: SLURM auto-requeueing enabled. Setting signal handlers.
114: SLURM auto-requeueing enabled. Setting signal handlers.
 73: SLURM auto-requeueing enabled. Setting signal handlers.
 96: SLURM auto-requeueing enabled. Setting signal handlers.
 88: SLURM auto-requeueing enabled. Setting signal handlers.
 60: SLURM auto-requeueing enabled. Setting signal handlers.
  3: SLURM auto-requeueing enabled. Setting signal handlers.
 17: SLURM auto-requeueing enabled. Setting signal handlers.
 82: SLURM auto-requeueing enabled. Setting signal handlers.
 51: SLURM auto-requeueing enabled. Setting signal handlers.
 26: SLURM auto-requeueing enabled. Setting signal handlers.
  8: SLURM auto-requeueing enabled. Setting signal handlers.
 45: SLURM auto-requeueing enabled. Setting signal handlers.
 35: SLURM auto-requeueing enabled. Setting signal handlers.
 65: SLURM auto-requeueing enabled. Setting signal handlers.
123: SLURM auto-requeueing enabled. Setting signal handlers.
115: SLURM auto-requeueing enabled. Setting signal handlers.
 74: SLURM auto-requeueing enabled. Setting signal handlers.
 97: SLURM auto-requeueing enabled. Setting signal handlers.
 63: SLURM auto-requeueing enabled. Setting signal handlers.
 18: SLURM auto-requeueing enabled. Setting signal handlers.
 84: SLURM auto-requeueing enabled. Setting signal handlers.
 52: SLURM auto-requeueing enabled. Setting signal handlers.
 27: SLURM auto-requeueing enabled. Setting signal handlers.
  9: SLURM auto-requeueing enabled. Setting signal handlers.
 46: SLURM auto-requeueing enabled. Setting signal handlers.
 36: SLURM auto-requeueing enabled. Setting signal handlers.
 66: SLURM auto-requeueing enabled. Setting signal handlers.
125: SLURM auto-requeueing enabled. Setting signal handlers.
118: SLURM auto-requeueing enabled. Setting signal handlers.
 75: SLURM auto-requeueing enabled. Setting signal handlers.
 98: SLURM auto-requeueing enabled. Setting signal handlers.
 57: SLURM auto-requeueing enabled. Setting signal handlers.
 19: SLURM auto-requeueing enabled. Setting signal handlers.
 83: SLURM auto-requeueing enabled. Setting signal handlers.
 53: SLURM auto-requeueing enabled. Setting signal handlers.
 28: SLURM auto-requeueing enabled. Setting signal handlers.
 12: SLURM auto-requeueing enabled. Setting signal handlers.
 47: SLURM auto-requeueing enabled. Setting signal handlers.
 39: SLURM auto-requeueing enabled. Setting signal handlers.
 67: SLURM auto-requeueing enabled. Setting signal handlers.
120: SLURM auto-requeueing enabled. Setting signal handlers.
119: SLURM auto-requeueing enabled. Setting signal handlers.
 76: SLURM auto-requeueing enabled. Setting signal handlers.
 99: SLURM auto-requeueing enabled. Setting signal handlers.
 58: SLURM auto-requeueing enabled. Setting signal handlers.
 20: SLURM auto-requeueing enabled. Setting signal handlers.
 85: SLURM auto-requeueing enabled. Setting signal handlers.
 55: SLURM auto-requeueing enabled. Setting signal handlers.
 29: SLURM auto-requeueing enabled. Setting signal handlers.
 14: SLURM auto-requeueing enabled. Setting signal handlers.
 44: SLURM auto-requeueing enabled. Setting signal handlers.
 37: SLURM auto-requeueing enabled. Setting signal handlers.
 68: SLURM auto-requeueing enabled. Setting signal handlers.
124: SLURM auto-requeueing enabled. Setting signal handlers.
116: SLURM auto-requeueing enabled. Setting signal handlers.
 77: SLURM auto-requeueing enabled. Setting signal handlers.
100: SLURM auto-requeueing enabled. Setting signal handlers.
 62: SLURM auto-requeueing enabled. Setting signal handlers.
 21: SLURM auto-requeueing enabled. Setting signal handlers.
 86: SLURM auto-requeueing enabled. Setting signal handlers.
 54: SLURM auto-requeueing enabled. Setting signal handlers.
 30: SLURM auto-requeueing enabled. Setting signal handlers.
 15: SLURM auto-requeueing enabled. Setting signal handlers.
 43: SLURM auto-requeueing enabled. Setting signal handlers.
 38: SLURM auto-requeueing enabled. Setting signal handlers.
 64: SLURM auto-requeueing enabled. Setting signal handlers.
126: SLURM auto-requeueing enabled. Setting signal handlers.
117: SLURM auto-requeueing enabled. Setting signal handlers.
 78: SLURM auto-requeueing enabled. Setting signal handlers.
104: SLURM auto-requeueing enabled. Setting signal handlers.
101: SLURM auto-requeueing enabled. Setting signal handlers.
 89: SLURM auto-requeueing enabled. Setting signal handlers.
 61: SLURM auto-requeueing enabled. Setting signal handlers.
 23: SLURM auto-requeueing enabled. Setting signal handlers.
 87: SLURM auto-requeueing enabled. Setting signal handlers.
 31: SLURM auto-requeueing enabled. Setting signal handlers.
 69: SLURM auto-requeueing enabled. Setting signal handlers.
127: SLURM auto-requeueing enabled. Setting signal handlers.
 79: SLURM auto-requeueing enabled. Setting signal handlers.
105: SLURM auto-requeueing enabled. Setting signal handlers.
102: SLURM auto-requeueing enabled. Setting signal handlers.
 90: SLURM auto-requeueing enabled. Setting signal handlers.
 22: SLURM auto-requeueing enabled. Setting signal handlers.
 70: SLURM auto-requeueing enabled. Setting signal handlers.
106: SLURM auto-requeueing enabled. Setting signal handlers.
103: SLURM auto-requeueing enabled. Setting signal handlers.
 91: SLURM auto-requeueing enabled. Setting signal handlers.
 71: SLURM auto-requeueing enabled. Setting signal handlers.
108: SLURM auto-requeueing enabled. Setting signal handlers.
 92: SLURM auto-requeueing enabled. Setting signal handlers.
109: SLURM auto-requeueing enabled. Setting signal handlers.
 93: SLURM auto-requeueing enabled. Setting signal handlers.
110: SLURM auto-requeueing enabled. Setting signal handlers.
111: SLURM auto-requeueing enabled. Setting signal handlers.
107: SLURM auto-requeueing enabled. Setting signal handlers.
 94: SLURM auto-requeueing enabled. Setting signal handlers.
 95: SLURM auto-requeueing enabled. Setting signal handlers.
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: !!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
  0: !!! [UB] Number of physical nodes: 16
  0: !!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
 16: !!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
  0: !!! [UB] Create Userbuffers Communicator
120: !!! [UB] Global ranks on node 15: [120, 121, 122, 123, 124, 125, 126, 127]
112: !!! [UB] Global ranks on node 14: [112, 113, 114, 115, 116, 117, 118, 119]
 24: !!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
  0: UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
 32: !!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
104: !!! [UB] Global ranks on node 13: [104, 105, 106, 107, 108, 109, 110, 111]
 96: !!! [UB] Global ranks on node 12: [96, 97, 98, 99, 100, 101, 102, 103]
 40: !!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
 88: !!! [UB] Global ranks on node 11: [88, 89, 90, 91, 92, 93, 94, 95]
 80: !!! [UB] Global ranks on node 10: [80, 81, 82, 83, 84, 85, 86, 87]
 48: !!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
 56: !!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
 72: !!! [UB] Global ranks on node 9: [72, 73, 74, 75, 76, 77, 78, 79]
 64: !!! [UB] Global ranks on node 8: [64, 65, 66, 67, 68, 69, 70, 71]
  0: MC initialized succesfully, window size = 549755813888
  0: !!! [UBP2P] Register UBuf 1
  0: !!! [UBP2P] Register UBuf 2
  0: !!! [UBP2P] Register UBuf 3
  0: !!! [UBP2P] Register UBuf 4
  0: !!! [UBP2P] Register UBuf 5
  0: !!! [UB] Register UBuf 6
  0: !!! [UB] Register UBuf 7
  0: !!! [UB] Register UBuf 8
  0: !!! [UB] Register UBuf 9
  0: !!! [UB] Register UBuf 10
 36: NCCL version 2.22.3+cuda12.6
 32: NCCL version 2.22.3+cuda12.6
 24: NCCL version 2.22.3+cuda12.6
124: NCCL version 2.22.3+cuda12.6
 52: NCCL version 2.22.3+cuda12.6
 88: NCCL version 2.22.3+cuda12.6
 44: NCCL version 2.22.3+cuda12.6
108: NCCL version 2.22.3+cuda12.6
 48: NCCL version 2.22.3+cuda12.6
 40: NCCL version 2.22.3+cuda12.6
  8: NCCL version 2.22.3+cuda12.6
100: NCCL version 2.22.3+cuda12.6
 96: NCCL version 2.22.3+cuda12.6
 12: NCCL version 2.22.3+cuda12.6
 92: NCCL version 2.22.3+cuda12.6
 84: NCCL version 2.22.3+cuda12.6
 60: NCCL version 2.22.3+cuda12.6
 80: NCCL version 2.22.3+cuda12.6
 72: NCCL version 2.22.3+cuda12.6
  4: NCCL version 2.22.3+cuda12.6
 56: NCCL version 2.22.3+cuda12.6
116: NCCL version 2.22.3+cuda12.6
112: NCCL version 2.22.3+cuda12.6
 20: NCCL version 2.22.3+cuda12.6
 16: NCCL version 2.22.3+cuda12.6
 64: NCCL version 2.22.3+cuda12.6
 68: NCCL version 2.22.3+cuda12.6
 76: NCCL version 2.22.3+cuda12.6
 28: NCCL version 2.22.3+cuda12.6
120: NCCL version 2.22.3+cuda12.6
104: NCCL version 2.22.3+cuda12.6
 35: NCCL version 2.22.3+cuda12.6
 34: NCCL version 2.22.3+cuda12.6
 33: NCCL version 2.22.3+cuda12.6
 25: NCCL version 2.22.3+cuda12.6
 27: NCCL version 2.22.3+cuda12.6
 26: NCCL version 2.22.3+cuda12.6
 50: NCCL version 2.22.3+cuda12.6
 51: NCCL version 2.22.3+cuda12.6
 49: NCCL version 2.22.3+cuda12.6
 97: NCCL version 2.22.3+cuda12.6
 98: NCCL version 2.22.3+cuda12.6
 99: NCCL version 2.22.3+cuda12.6
 89: NCCL version 2.22.3+cuda12.6
 90: NCCL version 2.22.3+cuda12.6
 91: NCCL version 2.22.3+cuda12.6
 41: NCCL version 2.22.3+cuda12.6
 43: NCCL version 2.22.3+cuda12.6
 42: NCCL version 2.22.3+cuda12.6
121: NCCL version 2.22.3+cuda12.6
122: NCCL version 2.22.3+cuda12.6
 66: NCCL version 2.22.3+cuda12.6
 81: NCCL version 2.22.3+cuda12.6
123: NCCL version 2.22.3+cuda12.6
 67: NCCL version 2.22.3+cuda12.6
 65: NCCL version 2.22.3+cuda12.6
 83: NCCL version 2.22.3+cuda12.6
 82: NCCL version 2.22.3+cuda12.6
113: NCCL version 2.22.3+cuda12.6
114: NCCL version 2.22.3+cuda12.6
115: NCCL version 2.22.3+cuda12.6
  9: NCCL version 2.22.3+cuda12.6
 10: NCCL version 2.22.3+cuda12.6
 11: NCCL version 2.22.3+cuda12.6
106: NCCL version 2.22.3+cuda12.6
105: NCCL version 2.22.3+cuda12.6
107: NCCL version 2.22.3+cuda12.6
 59: NCCL version 2.22.3+cuda12.6
 57: NCCL version 2.22.3+cuda12.6
 17: NCCL version 2.22.3+cuda12.6
 58: NCCL version 2.22.3+cuda12.6
 18: NCCL version 2.22.3+cuda12.6
 19: NCCL version 2.22.3+cuda12.6
 73: NCCL version 2.22.3+cuda12.6
 74: NCCL version 2.22.3+cuda12.6
 75: NCCL version 2.22.3+cuda12.6
  5: NCCL version 2.22.3+cuda12.6
  6: NCCL version 2.22.3+cuda12.6
  7: NCCL version 2.22.3+cuda12.6
  0: :::MLLOG {"namespace": "", "time_ms": 1728605071811, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605071811, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605071811, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605077335, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.2685353755950928, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.0003499176480626913}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605082859, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4630149602890015, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.0003496706697575261}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605088380, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.395792007446289, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.00034925929753184046}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605093912, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.276472806930542, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00034868391855477426}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605099454, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2819502353668213, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.00034794507435288117}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605105004, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3021283149719238, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.00034704346030046284}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605110559, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3550575971603394, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.000345979924965107}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605116124, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3454989194869995, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.0003447554693090452}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605121676, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3471577167510986, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.0003433712457470823}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605127236, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2429933547973633, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.0003418285570619839}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605132777, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2756140232086182, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.00034012885517834305}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605138316, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2790061235427856, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.0003382737397960793}}
 30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 65: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 69: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 71: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 67: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 73: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 64: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 66: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 72: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 68: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 95: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 88: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 89: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 93: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 92: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 94: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 90: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 91: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 70: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 84: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 74: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
105: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 75: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
109: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
111: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
108: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
107: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
106: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
104: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
110: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 77: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
121: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
120: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 99: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 82: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
118: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
113: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 81: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
126: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
102: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
100: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
123: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 83: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 96: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 97: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 80: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
122: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
101: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 86: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 98: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
116: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
114: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
125: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
112: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 87: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
127: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
103: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
119: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
117: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
115: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
124: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 85: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 76: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 78: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 79: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
  0: :::MLLOG {"namespace": "", "time_ms": 1728605148032, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.88406374968408}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605148032, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605148032, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728605153585, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9397983551025391, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605153585, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605153585, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605159118, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3363615274429321, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.00033626495688485734}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605164677, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3004037141799927, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0003341043970408414}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605166899, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.868544057161632}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605166899, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605166899, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728605171993, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9368801116943359, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605171993, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605171993, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605175337, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2957555055618286, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.00033179409370733237}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605180882, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2835884094238281, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.0003293362212609621}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605185327, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.82588179323487}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605185327, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605185327, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728605190653, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9317494630813599, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605190653, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605190653, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605191761, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3043839931488037, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.00032673309296524624}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605197323, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.338393211364746, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.0003239871587934214}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605202883, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2877936363220215, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.0003211010031226165}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605204003, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.792528145966415}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605204003, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605204003, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728605209082, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9284921884536743, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605209082, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605209082, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3072}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605213525, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3087613582611084, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3200, "lr": 0.0003180773423015271}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605219090, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2939403057098389, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3360, "lr": 0.00031491902209388335}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605222432, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.791400959972727}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605222432, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605222432, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3456}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728605227649, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9265576601028442, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605227649, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605227649, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 3456}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605229861, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.302345871925354, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3520, "lr": 0.0003116290150001165}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605235412, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3468987941741943, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3680, "lr": 0.0003082104174597458}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605240994, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3165929317474365, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3840, "lr": 0.00030466644693711784}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605241000, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 28.788976007851073}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605241000, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605241000, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3840}}
  0: setting number of microbatches to constant 1
  0: setting number of microbatches to constant 1
  0: :::MLLOG {"namespace": "", "time_ms": 1728605246172, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9246997237205505, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605246172, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3840}}
  0: :::MLLOG {"namespace": "", "time_ms": 1728605246172, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.9246997237205505, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 321, "samples_count": 3840, "status": "success"}}
 12: [rank12]:[W1011 00:07:30.350399489 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 23: [rank23]:[W1011 00:07:30.361057278 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  5: [rank5]:[W1011 00:07:30.865522450 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 66: [rank66]:[W1011 00:07:30.347834500 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
113: [rank113]:[W1011 00:07:30.047904614 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 39: [rank39]:[W1011 00:07:30.896778619 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 81: [rank81]:[W1011 00:07:30.345526355 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
125: [rank125]:[W1011 00:07:30.417840653 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 75: [rank75]:[W1011 00:07:30.716530168 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 43: [rank43]:[W1011 00:07:30.823511081 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 98: [rank98]:[W1011 00:07:30.363915972 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 60: [rank60]:[W1011 00:07:30.922605348 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 48: [rank48]:[W1011 00:07:30.177779168 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  7: [rank7]:[W1011 00:07:30.910342294 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 18: [rank18]:[W1011 00:07:30.430368209 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 31: [rank31]:[W1011 00:07:30.783937588 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  4: [rank4]:[W1011 00:07:30.930813242 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 16: [rank16]:[W1011 00:07:30.432965197 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 29: [rank29]:[W1011 00:07:30.789396432 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 13: [rank13]:[W1011 00:07:30.431187042 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  3: [rank3]:[W1011 00:07:30.936625286 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  6: [rank6]:[W1011 00:07:30.937245512 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  2: [rank2]:[W1011 00:07:30.938941804 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 10: [rank10]:[W1011 00:07:30.434178543 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 17: [rank17]:[W1011 00:07:30.441943361 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  1: [rank1]:[W1011 00:07:30.940508561 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 15: [rank15]:[W1011 00:07:30.435621330 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 28: [rank28]:[W1011 00:07:30.794382263 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 20: [rank20]:[W1011 00:07:30.442330117 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  0: [rank0]:[W1011 00:07:30.941718143 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 21: [rank21]:[W1011 00:07:30.447388676 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 19: [rank19]:[W1011 00:07:30.448118156 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 26: [rank26]:[W1011 00:07:30.802496131 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 25: [rank25]:[W1011 00:07:30.804055108 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 47: [rank47]:[W1011 00:07:30.887908381 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 33: [rank33]:[W1011 00:07:30.967360361 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 36: [rank36]:[W1011 00:07:30.967715771 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
116: [rank116]:[W1011 00:07:30.118876321 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
104: [rank104]:[W1011 00:07:30.241753553 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 32: [rank32]:[W1011 00:07:30.968402322 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  8: [rank8]:[W1011 00:07:30.446749263 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 14: [rank14]:[W1011 00:07:30.450324441 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 30: [rank30]:[W1011 00:07:30.809477259 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 37: [rank37]:[W1011 00:07:30.972776880 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 35: [rank35]:[W1011 00:07:30.973460209 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 27: [rank27]:[W1011 00:07:30.810217518 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 11: [rank11]:[W1011 00:07:30.451963955 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 34: [rank34]:[W1011 00:07:30.975811047 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 42: [rank42]:[W1011 00:07:30.896431084 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  9: [rank9]:[W1011 00:07:30.455737655 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 63: [rank63]:[W1011 00:07:30.992800385 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 55: [rank55]:[W1011 00:07:30.247294572 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 71: [rank71]:[W1011 00:07:30.429970554 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 52: [rank52]:[W1011 00:07:30.247813170 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 44: [rank44]:[W1011 00:07:30.898458691 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 24: [rank24]:[W1011 00:07:30.815048590 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 56: [rank56]:[W1011 00:07:30.993705380 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 40: [rank40]:[W1011 00:07:30.899013249 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 22: [rank22]:[W1011 00:07:30.466785857 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 45: [rank45]:[W1011 00:07:30.903380496 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
119: [rank119]:[W1011 00:07:30.138583793 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 57: [rank57]:[W1011 00:07:30.002863390 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 41: [rank41]:[W1011 00:07:30.908038719 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 49: [rank49]:[W1011 00:07:30.257516906 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 64: [rank64]:[W1011 00:07:30.440956826 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 38: [rank38]:[W1011 00:07:30.992875408 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 61: [rank61]:[W1011 00:07:30.008304732 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 53: [rank53]:[W1011 00:07:30.262979446 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 51: [rank51]:[W1011 00:07:30.263467124 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 59: [rank59]:[W1011 00:07:30.008977316 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 50: [rank50]:[W1011 00:07:30.265815502 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 58: [rank58]:[W1011 00:07:30.011358741 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
127: [rank127]:[W1011 00:07:30.513044202 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 95: [rank95]:[W1011 00:07:30.858051157 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 87: [rank87]:[W1011 00:07:30.446163547 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 79: [rank79]:[W1011 00:07:30.811228580 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 76: [rank76]:[W1011 00:07:30.811730074 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 68: [rank68]:[W1011 00:07:30.450461331 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
117: [rank117]:[W1011 00:07:30.154085079 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 46: [rank46]:[W1011 00:07:30.923454351 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 77: [rank77]:[W1011 00:07:30.816677547 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 69: [rank69]:[W1011 00:07:30.455483560 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 54: [rank54]:[W1011 00:07:30.272992189 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 67: [rank67]:[W1011 00:07:30.456079264 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 74: [rank74]:[W1011 00:07:30.819728018 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 65: [rank65]:[W1011 00:07:30.460011121 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 73: [rank73]:[W1011 00:07:30.821310952 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
103: [rank103]:[W1011 00:07:30.466201479 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
111: [rank111]:[W1011 00:07:30.281277913 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 84: [rank84]:[W1011 00:07:30.456749977 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 72: [rank72]:[W1011 00:07:30.822210379 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 88: [rank88]:[W1011 00:07:30.868996816 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 85: [rank85]:[W1011 00:07:30.461692238 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 83: [rank83]:[W1011 00:07:30.462372136 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 90: [rank90]:[W1011 00:07:30.876565627 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 82: [rank82]:[W1011 00:07:30.464779847 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
106: [rank106]:[W1011 00:07:30.289713905 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
112: [rank112]:[W1011 00:07:30.169658836 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 80: [rank80]:[W1011 00:07:30.467195189 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
120: [rank120]:[W1011 00:07:30.534158515 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 96: [rank96]:[W1011 00:07:30.477167746 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
118: [rank118]:[W1011 00:07:30.173760098 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 78: [rank78]:[W1011 00:07:30.836371812 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 62: [rank62]:[W1011 00:07:30.038047269 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 86: [rank86]:[W1011 00:07:30.471414939 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 70: [rank70]:[W1011 00:07:30.475191231 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 94: [rank94]:[W1011 00:07:30.883395518 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 93: [rank93]:[W1011 00:07:30.883656357 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 91: [rank91]:[W1011 00:07:30.884283256 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 99: [rank99]:[W1011 00:07:30.482423402 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
107: [rank107]:[W1011 00:07:30.297500486 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
115: [rank115]:[W1011 00:07:30.175130970 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
123: [rank123]:[W1011 00:07:30.539627139 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
114: [rank114]:[W1011 00:07:30.177307715 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
122: [rank122]:[W1011 00:07:30.541845350 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 89: [rank89]:[W1011 00:07:30.888197630 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 97: [rank97]:[W1011 00:07:30.486320333 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 92: [rank92]:[W1011 00:07:30.888672290 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
102: [rank102]:[W1011 00:07:30.491523027 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
101: [rank101]:[W1011 00:07:30.491726232 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
121: [rank121]:[W1011 00:07:30.553278131 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
105: [rank105]:[W1011 00:07:30.311315747 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
124: [rank124]:[W1011 00:07:30.553751275 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
100: [rank100]:[W1011 00:07:30.496867166 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
110: [rank110]:[W1011 00:07:30.316619261 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
109: [rank109]:[W1011 00:07:30.316796449 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
108: [rank108]:[W1011 00:07:30.321862390 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
126: [rank126]:[W1011 00:07:30.568590052 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 1, retcode 3
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 2, retcode 3
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 2, retcode 3
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 0, retcode 3
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 0, retcode 3
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 2, retcode 3
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 0, retcode 3
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 2, retcode 3
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 0, retcode 3
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 3, retcode 3
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 3, retcode 3
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 3, retcode 3
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 3, retcode 3
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 4, retcode 3
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 4, retcode 3
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 5, retcode 3
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 5, retcode 3
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 4, retcode 3
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 6, retcode 3
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 5, retcode 3
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 5, retcode 3
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 6, retcode 3
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 5, retcode 3
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 6, retcode 3
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 5, retcode 3
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  0: 
  0: GPU-94:3240542:3242345 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 7, retcode 3
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 6, retcode 3
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 5, retcode 3
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  1: 
  1: GPU-94:3240449:3242343 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 7, retcode 3
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 4, retcode 3
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 6, retcode 3
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 5, retcode 3
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  2: 
  2: GPU-94:3240479:3242341 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 7, retcode 3
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 4, retcode 3
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 6, retcode 3
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 4, retcode 3
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  3: 
  3: GPU-94:3240563:3242339 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 7, retcode 3
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 6, retcode 3
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  4: 
  4: GPU-94:3240480:3242336 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 7, retcode 3
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 4, retcode 3
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 6, retcode 3
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  5: 
  5: GPU-94:3240513:3242334 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 7, retcode 3
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 4, retcode 3
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  6: 
  6: GPU-94:3240536:3242332 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 7, retcode 3
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  7: 
  7: GPU-94:3240481:3242331 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 7, retcode 3
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 10, retcode 3
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 10, retcode 3
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 8, retcode 3
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 9, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 9, retcode 3
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 11, retcode 3
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 11, retcode 3
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 9, retcode 3
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 11, retcode 3
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 8, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 11, retcode 3
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 10, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 8, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 10, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 8, retcode 3
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 9, retcode 3
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 12, retcode 3
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 13, retcode 3
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 13, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 13, retcode 3
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 12, retcode 3
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 14, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 12, retcode 3
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 13, retcode 3
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 14, retcode 3
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 12, retcode 3
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 12, retcode 3
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 14, retcode 3
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 13, retcode 3
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 13, retcode 3
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  8: 
  8: GPU-180:1514858:1516549 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 15, retcode 3
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 14, retcode 3
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 12, retcode 3
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
  9: 
  9: GPU-180:1514755:1516550 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 15, retcode 3
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 14, retcode 3
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 13, retcode 3
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 12, retcode 3
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 10: 
 10: GPU-180:1514859:1516551 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 15, retcode 3
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 14, retcode 3
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 13, retcode 3
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 12, retcode 3
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 11: 
 11: GPU-180:1514835:1516556 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 15, retcode 3
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 14, retcode 3
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 14, retcode 3
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 12: 
 12: GPU-180:1514782:1516558 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 15, retcode 3
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 13: 
 13: GPU-180:1514777:1516557 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 15, retcode 3
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 14: 
 14: GPU-180:1514807:1516559 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 15, retcode 3
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 15: 
 15: GPU-180:1514815:1516555 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 15, retcode 3
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 16, retcode 3
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 17, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 18, retcode 3
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 17, retcode 3
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 19, retcode 3
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 18, retcode 3
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 18, retcode 3
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 20, retcode 3
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 19, retcode 3
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 19, retcode 3
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 17, retcode 3
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 19, retcode 3
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 17, retcode 3
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 20, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 17, retcode 3
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 20, retcode 3
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 18, retcode 3
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 20, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 18, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 19, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 20, retcode 3
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 24, retcode 3
 21: 
 21: GPU-146:1019651:1021406 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 21, retcode 3
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 21, retcode 3
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 21, retcode 3
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 21, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 21, retcode 3
 21: 
 21: GPU-146:1019651:1021406 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 21: 
 21: GPU-146:1019651:1021406 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 21, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 25, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 26, retcode 3
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 25, retcode 3
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 27, retcode 3
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 26, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 26, retcode 3
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 27, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 25, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 25, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 27, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 26, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 27, retcode 3
 23: 
 23: GPU-146:1019600:1021408 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 23, retcode 3
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 23, retcode 3
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 23, retcode 3
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 23, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 23, retcode 3
 21: 
 21: GPU-146:1019651:1021406 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 21: 
 21: GPU-146:1019651:1021406 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 23, retcode 3
 22: 
 22: GPU-146:1019572:1021404 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 22: 
 22: GPU-146:1019572:1021404 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 22: 
 22: GPU-146:1019572:1021404 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 23, retcode 3
 23: 
 23: GPU-146:1019600:1021408 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 23: 
 23: GPU-146:1019600:1021408 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 23, retcode 3
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 16: 
 16: GPU-146:1019664:1021402 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 22, retcode 3
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 17: 
 17: GPU-146:1019591:1021400 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 22, retcode 3
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 18: 
 18: GPU-146:1019620:1021401 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 22, retcode 3
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 19: 
 19: GPU-146:1019661:1021403 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 22, retcode 3
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 20: 
 20: GPU-146:1019662:1021405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 22, retcode 3
 21: 
 21: GPU-146:1019651:1021406 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 21: 
 21: GPU-146:1019651:1021406 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 22, retcode 3
 22: 
 22: GPU-146:1019572:1021404 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 22: 
 22: GPU-146:1019572:1021404 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 22, retcode 3
 23: 
 23: GPU-146:1019600:1021408 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 23: 
 23: GPU-146:1019600:1021408 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 22, retcode 3
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 32, retcode 3
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 28, retcode 3
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 29, retcode 3
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 28, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 28, retcode 3
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 30, retcode 3
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 29, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 28, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 29, retcode 3
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 30, retcode 3
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 28, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 30, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 29, retcode 3
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 28, retcode 3
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 24: 
 24: GPU-457:1000722:1002427 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 31, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 30, retcode 3
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 29, retcode 3
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 30, retcode 3
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 25: 
 25: GPU-457:1000674:1002426 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 31, retcode 3
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 26: 
 26: GPU-457:1000673:1002425 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 31, retcode 3
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 29, retcode 3
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 28, retcode 3
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 28, retcode 3
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 27: 
 27: GPU-457:1000636:1002423 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 31, retcode 3
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 30, retcode 3
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 29, retcode 3
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 28: 
 28: GPU-457:1000723:1002421 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 31, retcode 3
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 30, retcode 3
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 29, retcode 3
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 29: 
 29: GPU-457:1000635:1002422 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 31, retcode 3
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 30, retcode 3
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 30: 
 30: GPU-457:1000724:1002424 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 31, retcode 3
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 31: 
 31: GPU-457:1000681:1002420 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 31, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 33, retcode 3
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 34, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 33, retcode 3
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 35, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 34, retcode 3
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 33, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 35, retcode 3
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 33, retcode 3
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 35, retcode 3
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 34, retcode 3
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 35, retcode 3
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 34, retcode 3
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 40, retcode 3
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 36, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 37, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 36, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 37, retcode 3
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 36, retcode 3
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 36, retcode 3
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 37, retcode 3
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 37, retcode 3
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 37, retcode 3
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 36, retcode 3
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 37, retcode 3
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 36, retcode 3
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 48, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 41, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 41, retcode 3
 39: 
 39: GPU-993:999111:1000823 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 39, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 39, retcode 3
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 39, retcode 3
 38: 
 38: GPU-993:999059:1000824 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 39, retcode 3
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 32: 
 32: GPU-993:999041:1000818 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 38, retcode 3
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 39, retcode 3
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 33: 
 33: GPU-993:999025:1000820 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 38, retcode 3
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 39, retcode 3
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 34: 
 34: GPU-993:999102:1000819 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 38, retcode 3
 38: 
 38: GPU-993:999059:1000824 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 38: 
 38: GPU-993:999059:1000824 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 39, retcode 3
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 35: 
 35: GPU-993:999073:1000817 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 38, retcode 3
 39: 
 39: GPU-993:999111:1000823 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 39: 
 39: GPU-993:999111:1000823 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 39, retcode 3
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 36: 
 36: GPU-993:999103:1000822 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 38, retcode 3
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 37: 
 37: GPU-993:999137:1000821 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 38, retcode 3
 38: 
 38: GPU-993:999059:1000824 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 38: 
 38: GPU-993:999059:1000824 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 38, retcode 3
 39: 
 39: GPU-993:999111:1000823 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 39: 
 39: GPU-993:999111:1000823 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 38, retcode 3
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 42, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 42, retcode 3
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 42, retcode 3
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 43, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 43, retcode 3
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 43, retcode 3
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 43, retcode 3
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 49, retcode 3
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 49, retcode 3
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 56, retcode 3
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 50, retcode 3
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 51, retcode 3
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 50, retcode 3
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 51, retcode 3
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 50, retcode 3
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 50, retcode 3
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 51, retcode 3
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 51, retcode 3
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 44, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 44, retcode 3
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 44, retcode 3
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 45, retcode 3
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 44, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 45, retcode 3
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 44, retcode 3
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 45, retcode 3
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 44, retcode 3
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 45, retcode 3
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 46, retcode 3
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 44, retcode 3
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 45, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 46, retcode 3
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 46, retcode 3
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 45, retcode 3
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 44, retcode 3
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 46, retcode 3
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 45, retcode 3
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 46, retcode 3
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 45, retcode 3
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 40: 
 40: GPU-130:2608656:2610396 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 47, retcode 3
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 46, retcode 3
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 41: 
 41: GPU-130:2608631:2610397 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 47, retcode 3
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 46, retcode 3
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 42: 
 42: GPU-130:2608567:2610398 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 47, retcode 3
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 46, retcode 3
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 43: 
 43: GPU-130:2608544:2610402 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 47, retcode 3
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 44: 
 44: GPU-130:2608608:2610405 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 47, retcode 3
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 45: 
 45: GPU-130:2608592:2610406 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 47, retcode 3
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 46: 
 46: GPU-130:2608568:2610403 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 47, retcode 3
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 47: 
 47: GPU-130:2608632:2610404 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 47, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 57, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 57, retcode 3
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 52, retcode 3
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 52, retcode 3
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 52, retcode 3
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 52, retcode 3
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 53, retcode 3
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 53, retcode 3
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 52, retcode 3
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 53, retcode 3
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 52, retcode 3
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 54, retcode 3
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 52, retcode 3
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 53, retcode 3
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 54, retcode 3
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 53, retcode 3
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 54, retcode 3
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 52, retcode 3
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 53, retcode 3
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 48: 
 48: GPU-881:996122:998282 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 55, retcode 3
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 54, retcode 3
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 54, retcode 3
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 53, retcode 3
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 54, retcode 3
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 53, retcode 3
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 49: 
 49: GPU-881:996162:998281 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 55, retcode 3
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 50: 
 50: GPU-881:996077:998280 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 55, retcode 3
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 54, retcode 3
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 51: 
 51: GPU-881:996172:998283 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 55, retcode 3
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 54, retcode 3
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 52: 
 52: GPU-881:996185:998284 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 55, retcode 3
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 53: 
 53: GPU-881:996131:998289 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 55, retcode 3
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 54: 
 54: GPU-881:996093:998285 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 55, retcode 3
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 55: 
 55: GPU-881:996121:998287 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 55, retcode 3
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 64, retcode 3
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 58, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 58, retcode 3
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 58, retcode 3
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 59, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 59, retcode 3
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 59, retcode 3
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 59, retcode 3
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 72, retcode 3
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 66, retcode 3
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 65, retcode 3
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 66, retcode 3
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 65, retcode 3
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 66, retcode 3
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 65, retcode 3
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 60, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 60, retcode 3
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 60, retcode 3
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 60, retcode 3
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 60, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 73, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 73, retcode 3
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 67, retcode 3
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 67, retcode 3
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 67, retcode 3
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 67, retcode 3
 62: 
 62: GPU-351:996916:998642 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 62, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 62, retcode 3
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 62, retcode 3
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 62, retcode 3
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 62, retcode 3
 62: 
 62: GPU-351:996916:998642 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 62: 
 62: GPU-351:996916:998642 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 62, retcode 3
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 81, retcode 3
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 74, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 74, retcode 3
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 74, retcode 3
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 89, retcode 3
 61: 
 61: GPU-351:996917:998643 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 61, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 61, retcode 3
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 61, retcode 3
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 61, retcode 3
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 61, retcode 3
 61: 
 61: GPU-351:996917:998643 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 61: 
 61: GPU-351:996917:998643 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 61, retcode 3
 62: 
 62: GPU-351:996916:998642 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 62: 
 62: GPU-351:996916:998642 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 61, retcode 3
 63: 
 63: GPU-351:996948:998644 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 56: 
 56: GPU-351:996876:998635 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 63, retcode 3
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 57: 
 57: GPU-351:996949:998634 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 63, retcode 3
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 58: 
 58: GPU-351:996845:998636 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 63, retcode 3
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 59: 
 59: GPU-351:996864:998638 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 63, retcode 3
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 60: 
 60: GPU-351:996893:998640 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 63, retcode 3
 61: 
 61: GPU-351:996917:998643 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 61: 
 61: GPU-351:996917:998643 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 63, retcode 3
 62: 
 62: GPU-351:996916:998642 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 62: 
 62: GPU-351:996916:998642 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 63, retcode 3
 63: 
 63: GPU-351:996948:998644 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 63: 
 63: GPU-351:996948:998644 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 63, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 82, retcode 3
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 82, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 82, retcode 3
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 80, retcode 3
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 80, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 80, retcode 3
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 68, retcode 3
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 68, retcode 3
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 69, retcode 3
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 68, retcode 3
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 69, retcode 3
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 68, retcode 3
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 69, retcode 3
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 68, retcode 3
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 69, retcode 3
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 68, retcode 3
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 69, retcode 3
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 69, retcode 3
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 90, retcode 3
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 90, retcode 3
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 75, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 75, retcode 3
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 75, retcode 3
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 75, retcode 3
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 76, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 76, retcode 3
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 76, retcode 3
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 76, retcode 3
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 76, retcode 3
 70: 
 70: GPU-338:995203:996976 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 70, retcode 3
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 70, retcode 3
 71: 
 71: GPU-338:995233:996972 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 70, retcode 3
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 64: 
 64: GPU-338:995267:996970 [0] proxy.cc:1521 NCCL WARN [Proxy Service 64] Failed to execute operation Close from rank 71, retcode 3
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 70, retcode 3
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 65: 
 65: GPU-338:995234:996969 [1] proxy.cc:1521 NCCL WARN [Proxy Service 65] Failed to execute operation Close from rank 71, retcode 3
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 70, retcode 3
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 66: 
 66: GPU-338:995187:996971 [2] proxy.cc:1521 NCCL WARN [Proxy Service 66] Failed to execute operation Close from rank 71, retcode 3
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 67: 
 67: GPU-338:995283:996973 [3] proxy.cc:1521 NCCL WARN [Proxy Service 67] Failed to execute operation Close from rank 71, retcode 3
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 70, retcode 3
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 68: 
 68: GPU-338:995299:996975 [4] proxy.cc:1521 NCCL WARN [Proxy Service 68] Failed to execute operation Close from rank 71, retcode 3
 70: 
 70: GPU-338:995203:996976 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 70: 
 70: GPU-338:995203:996976 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 70, retcode 3
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 69: 
 69: GPU-338:995235:996974 [5] proxy.cc:1521 NCCL WARN [Proxy Service 69] Failed to execute operation Close from rank 71, retcode 3
 71: 
 71: GPU-338:995233:996972 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 71: 
 71: GPU-338:995233:996972 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 70, retcode 3
 70: 
 70: GPU-338:995203:996976 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 70: 
 70: GPU-338:995203:996976 [6] proxy.cc:1521 NCCL WARN [Proxy Service 70] Failed to execute operation Close from rank 71, retcode 3
 71: 
 71: GPU-338:995233:996972 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 71: 
 71: GPU-338:995233:996972 [7] proxy.cc:1521 NCCL WARN [Proxy Service 71] Failed to execute operation Close from rank 71, retcode 3
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 97, retcode 3
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 83, retcode 3
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 83, retcode 3
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 85, retcode 3
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 85, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 83, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 85, retcode 3
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 83, retcode 3
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 84, retcode 3
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 85, retcode 3
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 83, retcode 3
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 84, retcode 3
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 85, retcode 3
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 85, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 84, retcode 3
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 83, retcode 3
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 84, retcode 3
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 84, retcode 3
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 84, retcode 3
 77: 
 77: GPU-8:2815452:2817248 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 77, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 77, retcode 3
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 77, retcode 3
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 77, retcode 3
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 77, retcode 3
 77: 
 77: GPU-8:2815452:2817248 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 77: 
 77: GPU-8:2815452:2817248 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 77, retcode 3
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 93, retcode 3
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 93, retcode 3
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 91, retcode 3
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 93, retcode 3
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 91, retcode 3
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 93, retcode 3
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 91, retcode 3
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 93, retcode 3
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 91, retcode 3
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 88, retcode 3
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 91, retcode 3
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 88, retcode 3
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 88, retcode 3
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 88, retcode 3
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 88, retcode 3
 78: 
 78: GPU-8:2815417:2817249 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 78, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 78, retcode 3
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 78, retcode 3
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 78, retcode 3
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 78, retcode 3
 77: 
 77: GPU-8:2815452:2817248 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 77: 
 77: GPU-8:2815452:2817248 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 78, retcode 3
 78: 
 78: GPU-8:2815417:2817249 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 78: 
 78: GPU-8:2815417:2817249 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 78, retcode 3
 86: 
 86: GPU-342:995857:998284 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 86, retcode 3
 87: 
 87: GPU-342:995895:998286 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 86, retcode 3
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 80: 
 80: GPU-342:995934:998297 [0] proxy.cc:1521 NCCL WARN [Proxy Service 80] Failed to execute operation Close from rank 87, retcode 3
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 81: 
 81: GPU-342:995856:998293 [1] proxy.cc:1521 NCCL WARN [Proxy Service 81] Failed to execute operation Close from rank 87, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 86, retcode 3
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 82: 
 82: GPU-342:995940:998291 [2] proxy.cc:1521 NCCL WARN [Proxy Service 82] Failed to execute operation Close from rank 87, retcode 3
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 86, retcode 3
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 83: 
 83: GPU-342:995961:998292 [3] proxy.cc:1521 NCCL WARN [Proxy Service 83] Failed to execute operation Close from rank 87, retcode 3
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 86, retcode 3
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 86, retcode 3
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 84: 
 84: GPU-342:995896:998285 [4] proxy.cc:1521 NCCL WARN [Proxy Service 84] Failed to execute operation Close from rank 87, retcode 3
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 85: 
 85: GPU-342:995897:998283 [5] proxy.cc:1521 NCCL WARN [Proxy Service 85] Failed to execute operation Close from rank 87, retcode 3
 86: 
 86: GPU-342:995857:998284 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 86: 
 86: GPU-342:995857:998284 [6] proxy.cc:1521 NCCL WARN [Proxy Service 86] Failed to execute operation Close from rank 86, retcode 3
 86: 
 86: GPU-342:995857:998284 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 86: 
 86: GPU-342:995857:998284 [6] proxy.cc:1521 NCCL WARN [Proxy Service 86] Failed to execute operation Close from rank 87, retcode 3
 87: 
 87: GPU-342:995895:998286 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 87: 
 87: GPU-342:995895:998286 [7] proxy.cc:1521 NCCL WARN [Proxy Service 87] Failed to execute operation Close from rank 86, retcode 3
 87: 
 87: GPU-342:995895:998286 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 87: 
 87: GPU-342:995895:998286 [7] proxy.cc:1521 NCCL WARN [Proxy Service 87] Failed to execute operation Close from rank 87, retcode 3
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 105, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 98, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 98, retcode 3
 94: 
 94: GPU-234:1002469:1004702 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 94, retcode 3
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 94, retcode 3
 95: 
 95: GPU-234:1002468:1004704 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 94, retcode 3
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 94, retcode 3
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 95, retcode 3
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 95, retcode 3
 92: 
 92: GPU-234:1002397:1004701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 92: 
 92: GPU-234:1002397:1004701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 94, retcode 3
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 94, retcode 3
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 95, retcode 3
 94: 
 94: GPU-234:1002469:1004702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 94: 
 94: GPU-234:1002469:1004702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 94, retcode 3
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 95, retcode 3
 92: 
 92: GPU-234:1002397:1004701 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 92: 
 92: GPU-234:1002397:1004701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 92: 
 92: GPU-234:1002397:1004701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 95, retcode 3
 95: 
 95: GPU-234:1002468:1004704 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 95: 
 95: GPU-234:1002468:1004704 [7] proxy.cc:1521 NCCL WARN [Proxy Service 95] Failed to execute operation Close from rank 94, retcode 3
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 95, retcode 3
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 88: 
 88: GPU-234:1002427:1004697 [0] proxy.cc:1521 NCCL WARN [Proxy Service 88] Failed to execute operation Close from rank 92, retcode 3
 94: 
 94: GPU-234:1002469:1004702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 94: 
 94: GPU-234:1002469:1004702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 95, retcode 3
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 89: 
 89: GPU-234:1002429:1004698 [1] proxy.cc:1521 NCCL WARN [Proxy Service 89] Failed to execute operation Close from rank 92, retcode 3
 95: 
 95: GPU-234:1002468:1004704 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 95: 
 95: GPU-234:1002468:1004704 [7] proxy.cc:1521 NCCL WARN [Proxy Service 95] Failed to execute operation Close from rank 95, retcode 3
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 90: 
 90: GPU-234:1002381:1004699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 90] Failed to execute operation Close from rank 92, retcode 3
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 91: 
 91: GPU-234:1002365:1004703 [3] proxy.cc:1521 NCCL WARN [Proxy Service 91] Failed to execute operation Close from rank 92, retcode 3
 92: 
 92: GPU-234:1002397:1004701 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 92: 
 92: GPU-234:1002397:1004701 [4] proxy.cc:1521 NCCL WARN [Proxy Service 92] Failed to execute operation Close from rank 92, retcode 3
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 93: 
 93: GPU-234:1002428:1004700 [5] proxy.cc:1521 NCCL WARN [Proxy Service 93] Failed to execute operation Close from rank 92, retcode 3
 94: 
 94: GPU-234:1002469:1004702 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 94: 
 94: GPU-234:1002469:1004702 [6] proxy.cc:1521 NCCL WARN [Proxy Service 94] Failed to execute operation Close from rank 92, retcode 3
 95: 
 95: GPU-234:1002468:1004704 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 95: 
 95: GPU-234:1002468:1004704 [7] proxy.cc:1521 NCCL WARN [Proxy Service 95] Failed to execute operation Close from rank 92, retcode 3
 79: 
 79: GPU-8:2815488:2817247 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 72: 
 72: GPU-8:2815433:2817253 [0] proxy.cc:1521 NCCL WARN [Proxy Service 72] Failed to execute operation Close from rank 79, retcode 3
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 73: 
 73: GPU-8:2815461:2817252 [1] proxy.cc:1521 NCCL WARN [Proxy Service 73] Failed to execute operation Close from rank 79, retcode 3
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 74: 
 74: GPU-8:2815513:2817251 [2] proxy.cc:1521 NCCL WARN [Proxy Service 74] Failed to execute operation Close from rank 79, retcode 3
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 75: 
 75: GPU-8:2815401:2817254 [3] proxy.cc:1521 NCCL WARN [Proxy Service 75] Failed to execute operation Close from rank 79, retcode 3
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 76: 
 76: GPU-8:2815489:2817250 [4] proxy.cc:1521 NCCL WARN [Proxy Service 76] Failed to execute operation Close from rank 79, retcode 3
 77: 
 77: GPU-8:2815452:2817248 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 77: 
 77: GPU-8:2815452:2817248 [5] proxy.cc:1521 NCCL WARN [Proxy Service 77] Failed to execute operation Close from rank 79, retcode 3
 78: 
 78: GPU-8:2815417:2817249 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 78: 
 78: GPU-8:2815417:2817249 [6] proxy.cc:1521 NCCL WARN [Proxy Service 78] Failed to execute operation Close from rank 79, retcode 3
 79: 
 79: GPU-8:2815488:2817247 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 79: 
 79: GPU-8:2815488:2817247 [7] proxy.cc:1521 NCCL WARN [Proxy Service 79] Failed to execute operation Close from rank 79, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 109, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 109, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 121, retcode 3
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 101, retcode 3
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 102, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 101, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 102, retcode 3
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 100, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 100, retcode 3
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 101, retcode 3
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 100, retcode 3
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 101, retcode 3
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 102, retcode 3
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 102, retcode 3
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 102, retcode 3
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 100, retcode 3
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 101, retcode 3
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 100, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 113, retcode 3
 99: 
 99: GPU-245:999322:1001068 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:999297:1001071 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:999297:1001071 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 96: 
 96: GPU-245:999297:1001071 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 99, retcode 3
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 99, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 99, retcode 3
 96: 
 96: GPU-245:999297:1001071 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 96: 
 96: GPU-245:999297:1001071 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 96, retcode 3
 99: 
 99: GPU-245:999322:1001068 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 99: 
 99: GPU-245:999322:1001068 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 99, retcode 3
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 96, retcode 3
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 99, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 96, retcode 3
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 99, retcode 3
 99: 
 99: GPU-245:999322:1001068 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 99: 
 99: GPU-245:999322:1001068 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 96, retcode 3
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 99, retcode 3
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 96, retcode 3
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 96, retcode 3
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 96, retcode 3
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 106, retcode 3
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 106, retcode 3
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 106, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 106, retcode 3
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 104, retcode 3
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 104, retcode 3
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 104, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 104, retcode 3
103: 
103: GPU-245:999298:1001070 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 96: 
 96: GPU-245:999297:1001071 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 96: 
 96: GPU-245:999297:1001071 [0] proxy.cc:1521 NCCL WARN [Proxy Service 96] Failed to execute operation Close from rank 103, retcode 3
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 97: 
 97: GPU-245:999386:1001067 [1] proxy.cc:1521 NCCL WARN [Proxy Service 97] Failed to execute operation Close from rank 103, retcode 3
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 98: 
 98: GPU-245:999273:1001069 [2] proxy.cc:1521 NCCL WARN [Proxy Service 98] Failed to execute operation Close from rank 103, retcode 3
 99: 
 99: GPU-245:999322:1001068 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 99: 
 99: GPU-245:999322:1001068 [3] proxy.cc:1521 NCCL WARN [Proxy Service 99] Failed to execute operation Close from rank 103, retcode 3
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
100: 
100: GPU-245:999343:1001072 [4] proxy.cc:1521 NCCL WARN [Proxy Service 100] Failed to execute operation Close from rank 103, retcode 3
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
101: 
101: GPU-245:999349:1001065 [5] proxy.cc:1521 NCCL WARN [Proxy Service 101] Failed to execute operation Close from rank 103, retcode 3
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
102: 
102: GPU-245:999370:1001066 [6] proxy.cc:1521 NCCL WARN [Proxy Service 102] Failed to execute operation Close from rank 103, retcode 3
103: 
103: GPU-245:999298:1001070 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
103: 
103: GPU-245:999298:1001070 [7] proxy.cc:1521 NCCL WARN [Proxy Service 103] Failed to execute operation Close from rank 103, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 117, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 117, retcode 3
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 112, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 117, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 112, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 112, retcode 3
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 110, retcode 3
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 110, retcode 3
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 108, retcode 3
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 108, retcode 3
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 110, retcode 3
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 108, retcode 3
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 110, retcode 3
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 110, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 110, retcode 3
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 108, retcode 3
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 108, retcode 3
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 107, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 108, retcode 3
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 110, retcode 3
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 110, retcode 3
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 107, retcode 3
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 108, retcode 3
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
104: 
104: GPU-186:997612:999314 [0] proxy.cc:1521 NCCL WARN [Proxy Service 104] Failed to execute operation Close from rank 111, retcode 3
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 107, retcode 3
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 108, retcode 3
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 107, retcode 3
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 107, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 107, retcode 3
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
105: 
105: GPU-186:997516:999310 [1] proxy.cc:1521 NCCL WARN [Proxy Service 105] Failed to execute operation Close from rank 111, retcode 3
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 107, retcode 3
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
106: 
106: GPU-186:997546:999309 [2] proxy.cc:1521 NCCL WARN [Proxy Service 106] Failed to execute operation Close from rank 111, retcode 3
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 107, retcode 3
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
107: 
107: GPU-186:997548:999317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 107] Failed to execute operation Close from rank 111, retcode 3
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
108: 
108: GPU-186:997580:999316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 108] Failed to execute operation Close from rank 111, retcode 3
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
109: 
109: GPU-186:997611:999311 [5] proxy.cc:1521 NCCL WARN [Proxy Service 109] Failed to execute operation Close from rank 111, retcode 3
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
110: 
110: GPU-186:997547:999313 [6] proxy.cc:1521 NCCL WARN [Proxy Service 110] Failed to execute operation Close from rank 111, retcode 3
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
111: 
111: GPU-186:997610:999312 [7] proxy.cc:1521 NCCL WARN [Proxy Service 111] Failed to execute operation Close from rank 111, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 125, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 125, retcode 3
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 120, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 120, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 125, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 120, retcode 3
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 114, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 114, retcode 3
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 114, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 114, retcode 3
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 122, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 122, retcode 3
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 122, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 122, retcode 3
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 115, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 115, retcode 3
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 115, retcode 3
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 115, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 115, retcode 3
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 124, retcode 3
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 126, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 124, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 126, retcode 3
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 123, retcode 3
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 124, retcode 3
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 124, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 123, retcode 3
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 126, retcode 3
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 126, retcode 3
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 124, retcode 3
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 123, retcode 3
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 126, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 124, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 126, retcode 3
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 123, retcode 3
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 124, retcode 3
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 126, retcode 3
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 123, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 123, retcode 3
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 123, retcode 3
116: 
116: GPU-792:989596:991369 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
118: 
118: GPU-792:989620:991361 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
119: 
119: GPU-792:989564:991360 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 116, retcode 3
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 118, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 116, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 118, retcode 3
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 116, retcode 3
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 118, retcode 3
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 116, retcode 3
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 118, retcode 3
116: 
116: GPU-792:989596:991369 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
116: 
116: GPU-792:989596:991369 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 116, retcode 3
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
112: 
112: GPU-792:989619:991362 [0] proxy.cc:1521 NCCL WARN [Proxy Service 112] Failed to execute operation Close from rank 119, retcode 3
116: 
116: GPU-792:989596:991369 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
116: 
116: GPU-792:989596:991369 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 118, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 116, retcode 3
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
113: 
113: GPU-792:989648:991364 [1] proxy.cc:1521 NCCL WARN [Proxy Service 113] Failed to execute operation Close from rank 119, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 118, retcode 3
118: 
118: GPU-792:989620:991361 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
118: 
118: GPU-792:989620:991361 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 118, retcode 3
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
114: 
114: GPU-792:989672:991363 [2] proxy.cc:1521 NCCL WARN [Proxy Service 114] Failed to execute operation Close from rank 119, retcode 3
118: 
118: GPU-792:989620:991361 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
118: 
118: GPU-792:989620:991361 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 116, retcode 3
119: 
119: GPU-792:989564:991360 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
119: 
119: GPU-792:989564:991360 [7] proxy.cc:1521 NCCL WARN [Proxy Service 119] Failed to execute operation Close from rank 116, retcode 3
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
115: 
115: GPU-792:989659:991365 [3] proxy.cc:1521 NCCL WARN [Proxy Service 115] Failed to execute operation Close from rank 119, retcode 3
119: 
119: GPU-792:989564:991360 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
119: 
119: GPU-792:989564:991360 [7] proxy.cc:1521 NCCL WARN [Proxy Service 119] Failed to execute operation Close from rank 118, retcode 3
116: 
116: GPU-792:989596:991369 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
116: 
116: GPU-792:989596:991369 [4] proxy.cc:1521 NCCL WARN [Proxy Service 116] Failed to execute operation Close from rank 119, retcode 3
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
117: 
117: GPU-792:989580:991359 [5] proxy.cc:1521 NCCL WARN [Proxy Service 117] Failed to execute operation Close from rank 119, retcode 3
118: 
118: GPU-792:989620:991361 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
118: 
118: GPU-792:989620:991361 [6] proxy.cc:1521 NCCL WARN [Proxy Service 118] Failed to execute operation Close from rank 119, retcode 3
119: 
119: GPU-792:989564:991360 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
119: 
119: GPU-792:989564:991360 [7] proxy.cc:1521 NCCL WARN [Proxy Service 119] Failed to execute operation Close from rank 119, retcode 3
127: 
127: GPU-332:997543:999326 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
120: 
120: GPU-332:997594:999339 [0] proxy.cc:1521 NCCL WARN [Proxy Service 120] Failed to execute operation Close from rank 127, retcode 3
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
121: 
121: GPU-332:997544:999336 [1] proxy.cc:1521 NCCL WARN [Proxy Service 121] Failed to execute operation Close from rank 127, retcode 3
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
122: 
122: GPU-332:997632:999333 [2] proxy.cc:1521 NCCL WARN [Proxy Service 122] Failed to execute operation Close from rank 127, retcode 3
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
123: 
123: GPU-332:997603:999332 [3] proxy.cc:1521 NCCL WARN [Proxy Service 123] Failed to execute operation Close from rank 127, retcode 3
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
124: 
124: GPU-332:997648:999330 [4] proxy.cc:1521 NCCL WARN [Proxy Service 124] Failed to execute operation Close from rank 127, retcode 3
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
125: 
125: GPU-332:997568:999328 [5] proxy.cc:1521 NCCL WARN [Proxy Service 125] Failed to execute operation Close from rank 127, retcode 3
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
126: 
126: GPU-332:997602:999329 [6] proxy.cc:1521 NCCL WARN [Proxy Service 126] Failed to execute operation Close from rank 127, retcode 3
127: 
127: GPU-332:997543:999326 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
127: 
127: GPU-332:997543:999326 [7] proxy.cc:1521 NCCL WARN [Proxy Service 127] Failed to execute operation Close from rank 127, retcode 3
  0: ENDING TIMING RUN AT 2024-10-11 12:07:39 AM
  0: RESULT,LLM_FINETUNING,,597,nvidia,2024-10-10 11:57:42 PM
++ date +%s
+ echo 'RUNANDTIME_STOP 1728605269'
RUNANDTIME_STOP 1728605269
+ set -e
