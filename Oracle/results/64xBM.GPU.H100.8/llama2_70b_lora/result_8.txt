+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ echo ':::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 340 8 GPU-[130,881,338,8,234,245,792,332] BM.GPU.H100.8 Cluster DGXH100_8x8x1xtp4pp1cp2'
:::DLPAL /mnt/orangefs/mlperf/llama/cont/loraubuntu.sqsh 340 8 GPU-[130,881,338,8,234,245,792,332] BM.GPU.H100.8 Cluster DGXH100_8x8x1xtp4pp1cp2
++ srun --ntasks=1 --container-name=llama2_70b_lora_340 mlperf-sysjson.sh
srun: warning: can't run 1 processes on 8 nodes, setting nnodes to 1
+ echo ':::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}'
:::SYSJSON {"submitter":"Oracle","division":"closed","status":"Available cloud","system_name":"BM.GPU.H100.8","number_of_nodes":"8","host_processors_per_node":"2","host_processor_model_name":"Intel(R) Xeon(R) Platinum 8480+","host_processor_core_count":"56","host_processor_vcpu_count":"","host_processor_frequency":"","host_processor_caches":"","host_processor_interconnect":"","host_memory_capacity":"2.0 TB","host_storage_type":"","host_storage_capacity":"","host_networking":"","host_networking_topology":"","host_memory_configuration":"","accelerators_per_node":"8","accelerator_model_name":"NVIDIA H100 80GB HBM3","accelerator_host_interconnect":"","accelerator_frequency":"","accelerator_on-chip_memories":"","accelerator_memory_configuration":"","accelerator_memory_capacity":"81559 MiB","accelerator_interconnect":"","accelerator_interconnect_topology":"","cooling":"","hw_notes":"","framework":"PyTorch NVIDIA Release 24.09","framework_name":"","other_software_stack":{"cuda_version":"12.6.1.006","cuda_driver_version":"560.35.03","nccl_version":"2.22.3","cublas_version":"12.6.3.1002","cudnn_version":"9.4.0.58","trt_version":"10.4.0.26","dali_version":"1.41.0","mofed_version":"5.4-rdmacore39.0","openmpi_version":"4.1.7","kernel_version":"Linux 5.15.0-122-generic","nvidia_kernel_driver":"550.90.12"},"operating_system":"Ubuntu 22.04.4 LTS","sw_notes":""}
+ srun --ntasks=1 --container-name=llama2_70b_lora_340 bash -c 'echo ":::GITCOMMITID ${GIT_COMMIT_ID} ${LAUNCHER_GIT_COMMIT_ID}"'
srun: warning: can't run 1 processes on 8 nodes, setting nnodes to 1
:::GITCOMMITID  
+ '[' 1 -eq 1 ']'
+ srun --ntasks=8 --mpi=pmix bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on GPU-130
Clearing cache on GPU-332
Clearing cache on GPU-234
Clearing cache on GPU-338
Clearing cache on GPU-881
Clearing cache on GPU-245
Clearing cache on GPU-792
Clearing cache on GPU-8
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ export SEED=9851
+ SEED=9851
+ set +e
++ date +%s
+ echo 'RUNANDTIME_START 1728674490'
RUNANDTIME_START 1728674490
+ srun -l --mpi=pmix --ntasks=64 --ntasks-per-node=8 --time=20 --container-name=llama2_70b_lora_340 --container-mounts=/mnt/orangefs/mlperf/llama/data_model/gov_report:/data:ro,/mnt/orangefs/mlperf/llama/data_model/model:/ckpt:ro,/mnt/orangefs/mlperf/llama/logs:/results:rw --container-env=MASTER_PORT,MASTER_ADDR slurm2pytorch ./run_and_time.sh
 0: STARTING TIMING RUN AT 2024-10-11 07:21:38 PM
 8: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
20: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
51: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
45: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 2: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
48: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
49: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
16: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
22: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
27: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
42: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
43: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
35: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
25: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
33: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
29: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 7: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
58: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
57: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 9: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
54: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
50: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
41: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
44: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
55: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
52: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
53: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
17: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
46: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
47: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
40: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
21: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 1: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
61: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
10: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
11: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
62: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 4: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
14: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
13: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
12: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
15: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
63: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
18: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
34: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
23: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
19: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
36: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
32: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
38: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
37: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
39: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 0: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 6: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 3: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
 5: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
59: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
28: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
31: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
60: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
56: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
26: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
30: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
24: num_gpus=8 num_sockets = 2 num_nodes=2 cores_per_socket=56
12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 0: 
 0: 
 0: ************** Experiment configuration ***********
 0: 
 0: model:
 0:   ub_tp_comm_overlap_cfg:
 0:     qkv_fprop:
 0:       method: ring_exchange
 0:       aggregate: 0
 0:     fc1_fprop:
 0:       method: ring_exchange
 0:       aggregate: 0
 0:     proj_dgrad:
 0:       method: ring_exchange
 0:       aggregate: 0
 0:     fc2_dgrad:
 0:       method: ring_exchange
 0:       aggregate: 0
 0:     proj_fprop:
 0:       method: pipeline
 0:       num_sm: 32
 0:       cga_size: 2
 0:       num_splits: 4
 0:       set_sm_margin: 1
 0:       atomic_gemm: 1
 0:       fp8_buf: 1
 0:     fc2_fprop:
 0:       method: pipeline
 0:       num_sm: 16
 0:       cga_size: 2
 0:       num_splits: 4
 0:       set_sm_margin: 1
 0:       atomic_gemm: 1
 0:       fp8_buf: 0
 0:     qkv_dgrad:
 0:       method: bulk
 0:       num_sm: 4
 0:       cga_size: 2
 0:       set_sm_margin: 0
 0:     fc1_dgrad:
 0:       method: ring_exchange
 0:       num_sm: 1
 0:       cga_size: 2
 0:       set_sm_margin: 1
 0:       atomic_gemm: 0
 0:       fp8_buf: 0
 0:   mcore_gpt: true
 0:   seed: 9851
 0:   tensor_model_parallel_size: 4
 0:   pipeline_model_parallel_size: 1
 0:   context_parallel_size: 2
 0:   cpu_offloading: false
 0:   dist_ckpt_load_strictness: log_all
 0:   global_batch_size: 8
 0:   micro_batch_size: 1
 0:   max_position_embeddings: 8192
 0:   encoder_seq_length: 8192
 0:   restore_from_path: /ckpt
 0:   resume_from_checkpoint: null
 0:   save_nemo_on_validation_end: false
 0:   sync_batch_comm: false
 0:   megatron_amp_O2: true
 0:   sequence_parallel: 1
 0:   activations_checkpoint_granularity: null
 0:   activations_checkpoint_method: null
 0:   activations_checkpoint_num_layers: null
 0:   activations_checkpoint_layers_per_pipeline: null
 0:   answer_only_loss: true
 0:   gradient_as_bucket_view: false
 0:   hidden_dropout: 0.0
 0:   attention_dropout: 0.0
 0:   ffn_dropout: 0.0
 0:   bias_activation_fusion: true
 0:   bias_dropout_add_fusion: false
 0:   transformer_engine: true
 0:   fp8: true
 0:   fp8_params: true
 0:   fp8_hybrid: true
 0:   fp8_amax_history_len: 32
 0:   fp8_amax_compute_algo: max
 0:   reduce_amax: false
 0:   fp8_e4m3: false
 0:   fp8_interval: 1
 0:   fp8_margin: 0
 0:   fp8_dot_product_attention: 0
 0:   activation_func_fp8_input_store: 0
 0:   apply_rope_fusion: true
 0:   disable_parameter_transpose_cache: true
 0:   ub_tp_comm_overlap: 1
 0:   tp_comm_overlap_ag: true
 0:   tp_comm_overlap_rs: true
 0:   tp_comm_overlap_rs_dgrad: false
 0:   tp_comm_overlap_disable_qkv: true
 0:   batch_p2p_comm: 'False'
 0:   virtual_pipeline_model_parallel_size: 1
 0:   sharp: true
 0:   nccl_communicator_config_path: null
 0:   peft:
 0:     peft_scheme: lora
 0:     restore_from_path: null
 0:     lora_tuning:
 0:       adapter_dim: 16
 0:       alpha: 32
 0:       adapter_dropout: 0.1
 0:       dropout_position: pre
 0:       target_modules:
 0:       - attention
 0:       column_init_method: kaiming
 0:       row_init_method: zero
 0:       layer_selection: null
 0:       weight_tying: false
 0:       position_embedding_strategy: null
 0:       a2a_experimental: 1
 0:   data:
 0:     multiprocessing_context: spawn
 0:     pin_memory: true
 0:     sample_weight: constant
 0:     validation_drop_last: false
 0:     train_ds:
 0:       file_names:
 0:       - /data/train.npy
 0:       packed_sequence: true
 0:       packed_sequence_return_cu_seqlen: false
 0:       index_mapping_dir: /results/data_index/train
 0:       global_batch_size: 8
 0:       micro_batch_size: 1
 0:       shuffle: true
 0:       num_workers: 1
 0:       memmap_workers: 2
 0:       pin_memory: true
 0:       max_seq_length: 8192
 0:       min_seq_length: 1
 0:       drop_last: true
 0:       concat_sampling_probabilities:
 0:       - 1.0
 0:       label_key: output
 0:       add_eos: true
 0:       add_sep: false
 0:       add_bos: false
 0:       truncation_field: input
 0:       prompt_template: '{input} {output}'
 0:       truncation_method: right
 0:       seed: 9851
 0:     validation_ds:
 0:       file_names:
 0:       - /data/validation.npy
 0:       packed_sequence: true
 0:       packed_sequence_return_cu_seqlen: false
 0:       index_mapping_dir: /results/data_index/val
 0:       names: null
 0:       global_batch_size: 8
 0:       micro_batch_size: 1
 0:       shuffle: false
 0:       num_workers: 1
 0:       memmap_workers: 2
 0:       pin_memory: true
 0:       max_seq_length: 8192
 0:       min_seq_length: 1
 0:       drop_last: false
 0:       label_key: output
 0:       add_eos: true
 0:       add_sep: false
 0:       add_bos: false
 0:       write_predictions_to_file: false
 0:       output_file_path_prefix: null
 0:       truncation_field: input
 0:       prompt_template: '{input} {output}'
 0:       tokens_to_generate: 32
 0:       truncation_method: right
 0:       metric:
 0:         name: loss
 0:         average: null
 0:         num_classes: null
 0:   optim:
 0:     name: mcore_distributed_optim
 0:     overlap_grad_sync: true
 0:     overlap_param_sync: true
 0:     delay_grad_reduce: true
 0:     delay_param_gather: true
 0:     average_in_collective: false
 0:     lr: 0.0004
 0:     min_lr: 0
 0:     weight_decay: 0.0001
 0:     betas:
 0:     - 0.9
 0:     - 0.999
 0:     eps: 1.0e-08
 0:     amsgrad: false
 0:     sched:
 0:       name: CosineAnnealing
 0:       warmup_ratio: 0.0
 0:       min_lr: 0.0
 0:       constant_steps: 0
 0:       monitor: val_loss
 0:       reduce_on_plateau: false
 0:   enable_cuda_graph: 1
 0:   enable_cg_fp8_weight_caching: true
 0:   custom:
 0:     warmup: true
 0:     warmup_train_steps: 5
 0:     warmup_validation_steps: 5
 0:     reset_fp8_stats_after_warmup: 1
 0: name: megatron_gpt_peft_lora_tuning
 0: trainer:
 0:   devices: 8
 0:   num_nodes: 8
 0:   accelerator: gpu
 0:   precision: bf16-mixed
 0:   max_steps: 1024
 0:   val_check_interval: 192
 0:   check_val_every_n_epoch: null
 0:   log_every_n_steps: 0
 0:   gradient_clip_val: 0.3
 0:   gradient_clip_algorithm: norm
 0:   num_sanity_val_steps: 0
 0:   max_epochs: 1000
 0:   limit_val_batches: 1.0
 0:   limit_train_batches: 1.0
 0:   limit_test_batches: 0
 0:   logger: false
 0:   enable_checkpointing: false
 0:   use_distributed_sampler: false
 0:   enable_progress_bar: false
 0: exp_manager:
 0:   log_tflops_per_sec_per_gpu: false
 0:   explicit_log_dir: null
 0:   exp_dir: /results
 0:   create_wandb_logger: false
 0:   resume_if_exists: false
 0:   resume_ignore_no_checkpoint: true
 0:   create_checkpoint_callback: false
 0:   log_global_rank_0_only: true
 0:   create_early_stopping_callback: false
 0:   create_tensorboard_logger: false
 0: 
 0: GPU available: True (cuda), used: True
 0: TPU available: False, using: 0 TPU cores
 0: HPU available: False, using: 0 HPUs
 0: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
 0: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
 0: setting number of microbatches to constant 1
54: Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
55: Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
52: Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
53: Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
18: Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
23: Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
20: Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
21: Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
16: Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
49: Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
50: Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
51: Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
48: Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
17: Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
22: Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
11: Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
13: Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
19: Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
12: Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
 8: Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
 9: Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
14: Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
10: Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
15: Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
42: Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
43: Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
41: Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
44: Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
27: Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
31: Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
30: Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
26: Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
45: Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
28: Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
40: Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
29: Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
46: Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
47: Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
39: Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
36: Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
33: Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
 4: Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
 1: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
32: Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
 2: Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
24: Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
 6: Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
25: Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
60: Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
 0: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
38: Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
37: Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
35: Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
63: Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
61: Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
62: Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
34: Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
 7: Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
 5: Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
 3: Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
58: Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
56: Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
57: Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
59: Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
 2: NCCL version 2.22.3+cuda12.6
 0: ----------------------------------------------------------------------------------------------------
 0: distributed_backend=nccl
 0: All distributed processes registered. Starting with 64 processes
 0: ----------------------------------------------------------------------------------------------------
 0: 
 1: NCCL version 2.22.3+cuda12.6
 0: The number of process groups to use SHARP with depends on the type of the network switch. Nvidia QM1 switch supports SAHRP up to 8 process groups and QM2 supports up to 256 process groups. We apply SHARP to the communications of the data-parallel domain. If the number of data-parallel process groups is larger than the max process groups that the network switch supports, the communication will fall back to non-SHARP operators. To enable SHARP, `#SBATCH_NETWORK=sharp` should be set in the sbatch script.
 3: NCCL version 2.22.3+cuda12.6
 0: NCCL version 2.22.3+cuda12.6
 0: Loading distributed checkpoint with TensorStoreLoadShardedStrategy
 0: Loading distributed checkpoint directly on the GPU
 0: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
24: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
32: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
16: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
48: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 8: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
56: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
40: make: Entering directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
16: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
32: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
24: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 0: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
48: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
 8: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
40: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
56: g++ -O3 -Wall -shared -std=c++11 -fPIC -fdiagnostics-color -I/usr/include/python3.10 -I/usr/local/lib/python3.10/dist-packages/pybind11/include helpers.cpp -o helpers.cpython-310-x86_64-linux-gnu.so
16: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
48: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
32: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 8: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 0: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
40: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
56: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
24: make: Leaving directory '/workspace/ft-llm/NeMo/nemo/collections/nlp/data/language_modeling/megatron'
 0: > building indices for blendable datasets ...
 0:  > sample ratios:
 0:    dataset 0, input: 1, achieved: 1
 0: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 2: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 3: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 4: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 5: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
49: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
32: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 6: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
16: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
24: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 7: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
33: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 1: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
34: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
35: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
50: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
25: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
37: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
51: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
26: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
38: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
53: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
27: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
39: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
54: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
28: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
36: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
55: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
29: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
40: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
48: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
30: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
17: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
52: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
31: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
56: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
18: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
19: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 8: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
20: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
21: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
22: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
23: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 9: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
10: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
11: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
12: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
13: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
14: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
15: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
57: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
58: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
59: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
41: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
60: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
42: LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
61: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
43: LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
62: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
44: LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
63: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
45: LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
46: LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
47: LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
 0: Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, use_distributed_optimizer=True, check_for_nan_in_grad=False, bucket_size=40000000, average_in_collective=False)
 0: Number of buckets for gradient all-reduce / reduce-scatter: 1
 0: Params for bucket 1 (11141120 elements):
 0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.39.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.42.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.33.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.60.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.58.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.70.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.5.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.69.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.7.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.79.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.0.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.25.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.61.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.52.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.50.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.74.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.4.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.72.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.66.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.32.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.46.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.30.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.29.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.22.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.20.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.10.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.35.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.27.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.26.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.49.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.55.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.56.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.21.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.14.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.54.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.18.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.17.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.16.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.73.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.77.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.51.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.48.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.3.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.62.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.41.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.13.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.75.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.37.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.36.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.38.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.34.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.1.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.45.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.6.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.59.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.40.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.76.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.67.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.63.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.57.self_attention.adapter_layer.lora_kqv_adapter.linear_out.weight
 0: 	module.decoder.layers.31.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.23.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.9.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.71.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.64.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.15.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.19.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.65.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.47.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.44.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.11.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.8.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.43.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.68.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.28.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.12.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.2.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: 	module.decoder.layers.78.self_attention.adapter_layer.lora_dense_attention_adapter.linear_in.weight
 0: 	module.decoder.layers.53.self_attention.adapter_layer.lora_dense_attention_adapter.linear_out.weight
 0: 	module.decoder.layers.24.self_attention.adapter_layer.lora_kqv_adapter.linear_in.weight
 0: Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0004, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0001, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_grad_reduce=True, overlap_param_gather=True, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, clip_grad=0.3, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')
 0: 
 0:   | Name         | Type | Params | Mode
 0: ---------------------------------------------
 0:   | other params | n/a  | 17.3 B | n/a 
 0: ---------------------------------------------
 0: 11.1 M    Trainable params
 0: 17.2 B    Non-trainable params
 0: 17.3 B    Total params
 0: 69,029.364Total estimated model params size (MB)
 0: 0         Modules in train mode
 0: 0         Modules in eval mode
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784342, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 332}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784342, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 333}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784342, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "llama2_70b_lora", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784342, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "SUBMISSION_ORG_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784342, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784343, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784343, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "8xSUBMISSION_PLATFORM_PLACEHOLDER", "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 334}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784343, "event_type": "POINT_IN_TIME", "key": "seed", "value": 9851, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 335}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674784343, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 341}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785003, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3901, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 346}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785031, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 173, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 350}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785031, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_factor", "value": 0.0, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 354}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785032, "event_type": "POINT_IN_TIME", "key": "opt_adamw_weight_decay", "value": 0.0001, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 358}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785032, "event_type": "POINT_IN_TIME", "key": "opt_gradient_clip_norm", "value": 0.3, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 362}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785032, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 367}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785032, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 1024, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 368}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785032, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.0004, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 369}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785032, "event_type": "POINT_IN_TIME", "key": "lora_rank", "value": 16, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 370}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674785032, "event_type": "POINT_IN_TIME", "key": "lora_alpha", "value": 32, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 371}}
32: SLURM auto-requeueing enabled. Setting signal handlers.
60: SLURM auto-requeueing enabled. Setting signal handlers.
33: SLURM auto-requeueing enabled. Setting signal handlers.
34: SLURM auto-requeueing enabled. Setting signal handlers.
35: SLURM auto-requeueing enabled. Setting signal handlers.
 8: SLURM auto-requeueing enabled. Setting signal handlers.
40: SLURM auto-requeueing enabled. Setting signal handlers.
37: SLURM auto-requeueing enabled. Setting signal handlers.
 9: SLURM auto-requeueing enabled. Setting signal handlers.
38: SLURM auto-requeueing enabled. Setting signal handlers.
10: SLURM auto-requeueing enabled. Setting signal handlers.
39: SLURM auto-requeueing enabled. Setting signal handlers.
36: SLURM auto-requeueing enabled. Setting signal handlers.
11: SLURM auto-requeueing enabled. Setting signal handlers.
 0: SLURM auto-requeueing enabled. Setting signal handlers.
12: SLURM auto-requeueing enabled. Setting signal handlers.
61: SLURM auto-requeueing enabled. Setting signal handlers.
 4: SLURM auto-requeueing enabled. Setting signal handlers.
14: SLURM auto-requeueing enabled. Setting signal handlers.
16: SLURM auto-requeueing enabled. Setting signal handlers.
28: SLURM auto-requeueing enabled. Setting signal handlers.
62: SLURM auto-requeueing enabled. Setting signal handlers.
 5: SLURM auto-requeueing enabled. Setting signal handlers.
15: SLURM auto-requeueing enabled. Setting signal handlers.
41: SLURM auto-requeueing enabled. Setting signal handlers.
48: SLURM auto-requeueing enabled. Setting signal handlers.
63: SLURM auto-requeueing enabled. Setting signal handlers.
 6: SLURM auto-requeueing enabled. Setting signal handlers.
13: SLURM auto-requeueing enabled. Setting signal handlers.
42: SLURM auto-requeueing enabled. Setting signal handlers.
56: SLURM auto-requeueing enabled. Setting signal handlers.
 7: SLURM auto-requeueing enabled. Setting signal handlers.
43: SLURM auto-requeueing enabled. Setting signal handlers.
58: SLURM auto-requeueing enabled. Setting signal handlers.
 1: SLURM auto-requeueing enabled. Setting signal handlers.
44: SLURM auto-requeueing enabled. Setting signal handlers.
49: SLURM auto-requeueing enabled. Setting signal handlers.
29: SLURM auto-requeueing enabled. Setting signal handlers.
59: SLURM auto-requeueing enabled. Setting signal handlers.
 2: SLURM auto-requeueing enabled. Setting signal handlers.
45: SLURM auto-requeueing enabled. Setting signal handlers.
50: SLURM auto-requeueing enabled. Setting signal handlers.
30: SLURM auto-requeueing enabled. Setting signal handlers.
57: SLURM auto-requeueing enabled. Setting signal handlers.
 3: SLURM auto-requeueing enabled. Setting signal handlers.
47: SLURM auto-requeueing enabled. Setting signal handlers.
17: SLURM auto-requeueing enabled. Setting signal handlers.
51: SLURM auto-requeueing enabled. Setting signal handlers.
31: SLURM auto-requeueing enabled. Setting signal handlers.
46: SLURM auto-requeueing enabled. Setting signal handlers.
18: SLURM auto-requeueing enabled. Setting signal handlers.
52: SLURM auto-requeueing enabled. Setting signal handlers.
24: SLURM auto-requeueing enabled. Setting signal handlers.
19: SLURM auto-requeueing enabled. Setting signal handlers.
53: SLURM auto-requeueing enabled. Setting signal handlers.
25: SLURM auto-requeueing enabled. Setting signal handlers.
20: SLURM auto-requeueing enabled. Setting signal handlers.
54: SLURM auto-requeueing enabled. Setting signal handlers.
26: SLURM auto-requeueing enabled. Setting signal handlers.
21: SLURM auto-requeueing enabled. Setting signal handlers.
55: SLURM auto-requeueing enabled. Setting signal handlers.
27: SLURM auto-requeueing enabled. Setting signal handlers.
22: SLURM auto-requeueing enabled. Setting signal handlers.
23: SLURM auto-requeueing enabled. Setting signal handlers.
18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
56: !!! [UB] Global ranks on node 7: [56, 57, 58, 59, 60, 61, 62, 63]
16: !!! [UB] Global ranks on node 2: [16, 17, 18, 19, 20, 21, 22, 23]
 8: !!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
 0: !!! [UB] Number of physical nodes: 8
 0: !!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
 0: !!! [UB] Create Userbuffers Communicator
 0: UB_TIMEOUT is set to 110 sec, 217800000000 cycles, freq: 1980000khz
48: !!! [UB] Global ranks on node 6: [48, 49, 50, 51, 52, 53, 54, 55]
24: !!! [UB] Global ranks on node 3: [24, 25, 26, 27, 28, 29, 30, 31]
40: !!! [UB] Global ranks on node 5: [40, 41, 42, 43, 44, 45, 46, 47]
32: !!! [UB] Global ranks on node 4: [32, 33, 34, 35, 36, 37, 38, 39]
 0: MC initialized succesfully, window size = 549755813888
 0: !!! [UBP2P] Register UBuf 1
 0: !!! [UBP2P] Register UBuf 2
 0: !!! [UBP2P] Register UBuf 3
 0: !!! [UBP2P] Register UBuf 4
 0: !!! [UBP2P] Register UBuf 5
 0: !!! [UB] Register UBuf 6
 0: !!! [UB] Register UBuf 7
 0: !!! [UB] Register UBuf 8
 0: !!! [UB] Register UBuf 9
 0: !!! [UB] Register UBuf 10
 4: NCCL version 2.22.3+cuda12.6
16: NCCL version 2.22.3+cuda12.6
20: NCCL version 2.22.3+cuda12.6
12: NCCL version 2.22.3+cuda12.6
 8: NCCL version 2.22.3+cuda12.6
32: NCCL version 2.22.3+cuda12.6
36: NCCL version 2.22.3+cuda12.6
60: NCCL version 2.22.3+cuda12.6
24: NCCL version 2.22.3+cuda12.6
56: NCCL version 2.22.3+cuda12.6
28: NCCL version 2.22.3+cuda12.6
52: NCCL version 2.22.3+cuda12.6
48: NCCL version 2.22.3+cuda12.6
40: NCCL version 2.22.3+cuda12.6
44: NCCL version 2.22.3+cuda12.6
 9: NCCL version 2.22.3+cuda12.6
11: NCCL version 2.22.3+cuda12.6
10: NCCL version 2.22.3+cuda12.6
33: NCCL version 2.22.3+cuda12.6
35: NCCL version 2.22.3+cuda12.6
34: NCCL version 2.22.3+cuda12.6
58: NCCL version 2.22.3+cuda12.6
59: NCCL version 2.22.3+cuda12.6
57: NCCL version 2.22.3+cuda12.6
26: NCCL version 2.22.3+cuda12.6
25: NCCL version 2.22.3+cuda12.6
27: NCCL version 2.22.3+cuda12.6
17: NCCL version 2.22.3+cuda12.6
18: NCCL version 2.22.3+cuda12.6
19: NCCL version 2.22.3+cuda12.6
41: NCCL version 2.22.3+cuda12.6
42: NCCL version 2.22.3+cuda12.6
43: NCCL version 2.22.3+cuda12.6
49: NCCL version 2.22.3+cuda12.6
50: NCCL version 2.22.3+cuda12.6
51: NCCL version 2.22.3+cuda12.6
 7: NCCL version 2.22.3+cuda12.6
 6: NCCL version 2.22.3+cuda12.6
 5: NCCL version 2.22.3+cuda12.6
 0: :::MLLOG {"namespace": "", "time_ms": 1728674898009, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674898010, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 271}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674898010, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 272, "samples_count": 0}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674903525, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 2.263392448425293, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 80, "lr": 0.00039990588350021866}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674909034, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.452841877937317, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 160, "lr": 0.00039962362258002984}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674914543, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3596440553665161, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 240, "lr": 0.000399153482893532}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674920062, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3988538980484009, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 320, "lr": 0.000398495906919742}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674925592, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3467204570770264, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 400, "lr": 0.0003976515135461499}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674931136, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.4362090826034546, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 480, "lr": 0.0003966210974862433}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674936686, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3921527862548828, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 560, "lr": 0.0003954056285315509}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674942242, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2682517766952515, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 640, "lr": 0.00039400625063890885}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674947801, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.288822889328003, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 720, "lr": 0.00039242428085380836}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674953361, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.280603051185608, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 800, "lr": 0.00039066120807083876}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674958921, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3063617944717407, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 880, "lr": 0.00038871869163239207}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674964478, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3513808250427246, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 960, "lr": 0.0003865985597669478}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674970035, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2237880229949951, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1040, "lr": 0.0003843028078684084}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674975594, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.377913236618042, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1120, "lr": 0.0003818335966181045}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674981152, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2907172441482544, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1200, "lr": 0.00037919324995123706}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674986713, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2680073976516724, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1280, "lr": 0.000376384252869671}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674992279, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.309521198272705, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1360, "lr": 0.00037340924910313856}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728674997838, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.341963529586792, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1440, "lr": 0.00037027103862105306}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675003398, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3539936542510986, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1520, "lr": 0.000366972574997276}}
39: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
59: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
57: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
37: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
34: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
33: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
60: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
41: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
21: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
23: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
50: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
32: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 9: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 8: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
17: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
18: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
35: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
36: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
49: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
43: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
46: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
14: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
16: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
58: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
22: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
62: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
10: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
20: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
12: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
15: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
38: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
11: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
61: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
27: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
44: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
25: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
19: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
55: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
53: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
48: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
13: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
51: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
29: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
54: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
24: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 1: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 3: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 0: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
52: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
28: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
40: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
26: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
56: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
31: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
30: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 2: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
63: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
42: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
45: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
47: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 5: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 7: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 4: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 6: `zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).
 0: :::MLLOG {"namespace": "", "time_ms": 1728675014155, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.429427521290371}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675014155, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675014155, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 1536}}
 0: setting number of microbatches to constant 1
 0: setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1728675024050, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9393479228019714, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675024050, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675024051, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 1536}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675028520, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3002268075942993, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1600, "lr": 0.00036351696263031675}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675034082, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3147269487380981, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1680, "lr": 0.00035990745382158103}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675039641, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3169348239898682, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1760, "lr": 0.0003561474457144189}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675045204, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.36916983127594, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1840, "lr": 0.0003522404770968524}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675050763, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2840144634246826, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 1920, "lr": 0.00034819022507099186}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675050770, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.381956693043707}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675050770, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675050770, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 1920}}
 0: setting number of microbatches to constant 1
 0: setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1728675060399, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9329745769500732, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675060400, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675060400, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 1920}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675065961, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2674508094787598, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2000, "lr": 0.00034400050159227635}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675071527, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.363250970840454, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2080, "lr": 0.0003396752498817946}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675077098, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3383097648620605, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2160, "lr": 0.0003352185407150632}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675082658, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.270410418510437, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2240, "lr": 0.0003306345685907553}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675087113, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.38500765737037}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675087113, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675087113, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2304}}
 0: setting number of microbatches to constant 1
 0: setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1728675096820, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9321818351745605, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675096820, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675096820, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2304}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675097928, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2975763082504272, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2320, "lr": 0.00032592764778298545}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675103493, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2292532920837402, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2400, "lr": 0.00032110220828086513}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675109062, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2992494106292725, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2480, "lr": 0.0003161627916191529}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675114611, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3743700981140137, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2560, "lr": 0.00031111404660392046}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675120151, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3674930334091187, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2640, "lr": 0.000305960724937259}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675123482, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.4130149301045}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675123482, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675123482, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 2688}}
 0: setting number of microbatches to constant 1
 0: setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1728675133151, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.9271411299705505, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675133151, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675133151, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 253, "samples_count": 2688}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675135363, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2630819082260132, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2720, "lr": 0.00030070767674514353}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675140912, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2073355913162231, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2800, "lr": 0.00029535984601266447}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675146465, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3034864664077759, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2880, "lr": 0.00028992226593092135}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675152009, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.3147730827331543, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 2960, "lr": 0.00028440005415995997}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675157552, "event_type": "POINT_IN_TIME", "key": "train_loss", "value": 1.2850666046142578, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 223, "samples_count": 3040, "lr": 0.00027879840801220965}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675159776, "event_type": "POINT_IN_TIME", "key": "tracked_stats", "value": {"throughput": 14.432721173699106}, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 287, "step": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675159777, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 236, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675159777, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 241, "samples_count": 3072}}
 0: setting number of microbatches to constant 1
 0: setting number of microbatches to constant 1
 0: :::MLLOG {"namespace": "", "time_ms": 1728675169526, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.923808217048645, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 307, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675169527, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 312, "samples_count": 3072}}
 0: :::MLLOG {"namespace": "", "time_ms": 1728675169527, "event_type": "INTERVAL_END", "key": "run_stop", "value": 0.923808217048645, "metadata": {"file": "/workspace/ft-llm/custom_callbacks.py", "lineno": 321, "samples_count": 3072, "status": "success"}}
14: [rank14]:[W1011 19:32:53.611647999 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
63: [rank63]:[W1011 19:32:53.876329048 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
52: [rank52]:[W1011 19:32:53.532043065 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 7: [rank7]:[W1011 19:32:53.301630160 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
33: [rank33]:[W1011 19:32:53.254363020 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
25: [rank25]:[W1011 19:32:53.213968096 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
16: [rank16]:[W1011 19:32:53.862181233 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
47: [rank47]:[W1011 19:32:53.880073508 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
55: [rank55]:[W1011 19:32:53.572669989 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
44: [rank44]:[W1011 19:32:53.889992933 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
39: [rank39]:[W1011 19:32:53.292184646 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
46: [rank46]:[W1011 19:32:53.901085028 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
54: [rank54]:[W1011 19:32:53.593764119 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
45: [rank45]:[W1011 19:32:53.903801788 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
17: [rank17]:[W1011 19:32:53.903106652 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
41: [rank41]:[W1011 19:32:53.909236420 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
12: [rank12]:[W1011 19:32:53.721575077 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
15: [rank15]:[W1011 19:32:53.721649132 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
23: [rank23]:[W1011 19:32:53.904334172 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
22: [rank22]:[W1011 19:32:53.905146331 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
62: [rank62]:[W1011 19:32:53.968255058 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
38: [rank38]:[W1011 19:32:53.313253148 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
61: [rank61]:[W1011 19:32:53.969308012 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
37: [rank37]:[W1011 19:32:53.314392989 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
34: [rank34]:[W1011 19:32:53.315209969 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
18: [rank18]:[W1011 19:32:53.907436948 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
53: [rank53]:[W1011 19:32:53.606444720 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 5: [rank5]:[W1011 19:32:53.376025134 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
32: [rank32]:[W1011 19:32:53.320642390 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
40: [rank40]:[W1011 19:32:53.918786219 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 0: [rank0]:[W1011 19:32:53.380774891 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 1: [rank1]:[W1011 19:32:53.381268281 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
57: [rank57]:[W1011 19:32:53.976335351 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 9: [rank9]:[W1011 19:32:53.730748554 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
20: [rank20]:[W1011 19:32:53.914240437 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
31: [rank31]:[W1011 19:32:53.275734740 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 6: [rank6]:[W1011 19:32:53.383256824 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
30: [rank30]:[W1011 19:32:53.276482464 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 2: [rank2]:[W1011 19:32:53.384240142 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
21: [rank21]:[W1011 19:32:53.916523525 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
42: [rank42]:[W1011 19:32:53.923526691 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
13: [rank13]:[W1011 19:32:53.735348619 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 3: [rank3]:[W1011 19:32:53.387753544 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
11: [rank11]:[W1011 19:32:53.737149551 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
56: [rank56]:[W1011 19:32:53.985919044 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
49: [rank49]:[W1011 19:32:53.621969470 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
35: [rank35]:[W1011 19:32:53.336643348 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
43: [rank43]:[W1011 19:32:53.934711231 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
19: [rank19]:[W1011 19:32:53.928608670 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
60: [rank60]:[W1011 19:32:53.997368148 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
36: [rank36]:[W1011 19:32:53.342412438 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 4: [rank4]:[W1011 19:32:53.402537469 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
10: [rank10]:[W1011 19:32:53.753665663 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
58: [rank58]:[W1011 19:32:53.000562869 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
51: [rank51]:[W1011 19:32:53.637387121 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
59: [rank59]:[W1011 19:32:53.001845898 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
48: [rank48]:[W1011 19:32:53.641661441 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 8: [rank8]:[W1011 19:32:53.760490216 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
50: [rank50]:[W1011 19:32:53.645081797 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
27: [rank27]:[W1011 19:32:53.310157059 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
24: [rank24]:[W1011 19:32:53.314466517 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
26: [rank26]:[W1011 19:32:53.317783805 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
29: [rank29]:[W1011 19:32:54.359723432 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
28: [rank28]:[W1011 19:32:54.376292520 ProcessGroupNCCL.cpp:1218] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 0, retcode 3
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 1, retcode 3
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 2, retcode 3
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 1, retcode 3
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 2, retcode 3
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 2, retcode 3
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 1, retcode 3
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 8, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 9, retcode 3
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 9, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 8, retcode 3
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 3, retcode 3
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 3, retcode 3
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 3, retcode 3
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 3, retcode 3
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 7, retcode 3
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 7, retcode 3
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 7, retcode 3
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 5, retcode 3
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 5, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 16, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 17, retcode 3
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 7, retcode 3
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 17, retcode 3
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 6, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 16, retcode 3
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 7, retcode 3
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 6, retcode 3
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 5, retcode 3
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 7, retcode 3
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 0: 
 0: GPU-130:1276302:1288628 [0] proxy.cc:1521 NCCL WARN [Proxy Service 0] Failed to execute operation Close from rank 4, retcode 3
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 1: 
 1: GPU-130:1276350:1288622 [1] proxy.cc:1521 NCCL WARN [Proxy Service 1] Failed to execute operation Close from rank 4, retcode 3
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 6, retcode 3
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 5, retcode 3
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 6, retcode 3
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 7, retcode 3
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 2: 
 2: GPU-130:1276257:1288623 [2] proxy.cc:1521 NCCL WARN [Proxy Service 2] Failed to execute operation Close from rank 4, retcode 3
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 5, retcode 3
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 6, retcode 3
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 7, retcode 3
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 3: 
 3: GPU-130:1276366:1288627 [3] proxy.cc:1521 NCCL WARN [Proxy Service 3] Failed to execute operation Close from rank 4, retcode 3
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 4: 
 4: GPU-130:1276258:1288626 [4] proxy.cc:1521 NCCL WARN [Proxy Service 4] Failed to execute operation Close from rank 4, retcode 3
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 5, retcode 3
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 6, retcode 3
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 5: 
 5: GPU-130:1276331:1288625 [5] proxy.cc:1521 NCCL WARN [Proxy Service 5] Failed to execute operation Close from rank 4, retcode 3
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 5, retcode 3
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 6, retcode 3
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 5, retcode 3
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 6: 
 6: GPU-130:1276272:1288624 [6] proxy.cc:1521 NCCL WARN [Proxy Service 6] Failed to execute operation Close from rank 4, retcode 3
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 6, retcode 3
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 7: 
 7: GPU-130:1276323:1288621 [7] proxy.cc:1521 NCCL WARN [Proxy Service 7] Failed to execute operation Close from rank 4, retcode 3
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 10, retcode 3
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 11, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 10, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 11, retcode 3
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 10, retcode 3
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 11, retcode 3
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 10, retcode 3
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 11, retcode 3
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 24, retcode 3
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 18, retcode 3
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 19, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 18, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 19, retcode 3
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 18, retcode 3
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 19, retcode 3
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 18, retcode 3
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 19, retcode 3
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 12, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 12, retcode 3
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 12, retcode 3
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 12, retcode 3
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 12, retcode 3
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 32, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 25, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 25, retcode 3
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 20, retcode 3
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 21, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 20, retcode 3
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 20, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 21, retcode 3
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 22, retcode 3
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 21, retcode 3
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 20, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 22, retcode 3
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 21, retcode 3
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 20, retcode 3
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 22, retcode 3
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 21, retcode 3
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 21, retcode 3
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 22, retcode 3
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 20, retcode 3
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 20, retcode 3
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 20, retcode 3
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
16: 
16: GPU-338:1865917:1868107 [0] proxy.cc:1521 NCCL WARN [Proxy Service 16] Failed to execute operation Close from rank 23, retcode 3
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 22, retcode 3
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 21, retcode 3
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 21, retcode 3
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 22, retcode 3
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
17: 
17: GPU-338:1865996:1868108 [1] proxy.cc:1521 NCCL WARN [Proxy Service 17] Failed to execute operation Close from rank 23, retcode 3
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 22, retcode 3
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
18: 
18: GPU-338:1865941:1868103 [2] proxy.cc:1521 NCCL WARN [Proxy Service 18] Failed to execute operation Close from rank 23, retcode 3
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 26, retcode 3
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 22, retcode 3
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
19: 
19: GPU-338:1866021:1868098 [3] proxy.cc:1521 NCCL WARN [Proxy Service 19] Failed to execute operation Close from rank 23, retcode 3
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
20: 
20: GPU-338:1865916:1868096 [4] proxy.cc:1521 NCCL WARN [Proxy Service 20] Failed to execute operation Close from rank 23, retcode 3
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
21: 
21: GPU-338:1865965:1868097 [5] proxy.cc:1521 NCCL WARN [Proxy Service 21] Failed to execute operation Close from rank 23, retcode 3
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
22: 
22: GPU-338:1865997:1868095 [6] proxy.cc:1521 NCCL WARN [Proxy Service 22] Failed to execute operation Close from rank 23, retcode 3
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
23: 
23: GPU-338:1865964:1868099 [7] proxy.cc:1521 NCCL WARN [Proxy Service 23] Failed to execute operation Close from rank 23, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 26, retcode 3
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 26, retcode 3
14: 
14: GPU-881:1886505:1888690 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 14, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 14, retcode 3
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 14, retcode 3
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 14, retcode 3
13: 
13: GPU-881:1886595:1888691 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 14, retcode 3
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 13, retcode 3
13: 
13: GPU-881:1886595:1888691 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
13: 
13: GPU-881:1886595:1888691 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 14, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 13, retcode 3
14: 
14: GPU-881:1886505:1888690 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
14: 
14: GPU-881:1886505:1888690 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 14, retcode 3
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 13, retcode 3
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 13, retcode 3
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 13, retcode 3
13: 
13: GPU-881:1886595:1888691 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
13: 
13: GPU-881:1886595:1888691 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 13, retcode 3
14: 
14: GPU-881:1886505:1888690 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
14: 
14: GPU-881:1886505:1888690 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 13, retcode 3
15: 
15: GPU-881:1886556:1888689 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 8: 
 8: GPU-881:1886552:1888702 [0] proxy.cc:1521 NCCL WARN [Proxy Service 8] Failed to execute operation Close from rank 15, retcode 3
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
 9: 
 9: GPU-881:1886598:1888701 [1] proxy.cc:1521 NCCL WARN [Proxy Service 9] Failed to execute operation Close from rank 15, retcode 3
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
10: 
10: GPU-881:1886549:1888699 [2] proxy.cc:1521 NCCL WARN [Proxy Service 10] Failed to execute operation Close from rank 15, retcode 3
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
11: 
11: GPU-881:1886522:1888693 [3] proxy.cc:1521 NCCL WARN [Proxy Service 11] Failed to execute operation Close from rank 15, retcode 3
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
12: 
12: GPU-881:1886608:1888694 [4] proxy.cc:1521 NCCL WARN [Proxy Service 12] Failed to execute operation Close from rank 15, retcode 3
13: 
13: GPU-881:1886595:1888691 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
13: 
13: GPU-881:1886595:1888691 [5] proxy.cc:1521 NCCL WARN [Proxy Service 13] Failed to execute operation Close from rank 15, retcode 3
14: 
14: GPU-881:1886505:1888690 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
14: 
14: GPU-881:1886505:1888690 [6] proxy.cc:1521 NCCL WARN [Proxy Service 14] Failed to execute operation Close from rank 15, retcode 3
15: 
15: GPU-881:1886556:1888689 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
15: 
15: GPU-881:1886556:1888689 [7] proxy.cc:1521 NCCL WARN [Proxy Service 15] Failed to execute operation Close from rank 15, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 33, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 33, retcode 3
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 34, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 34, retcode 3
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 34, retcode 3
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 27, retcode 3
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 29, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 27, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 29, retcode 3
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 27, retcode 3
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 28, retcode 3
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 29, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 28, retcode 3
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 27, retcode 3
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 28, retcode 3
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 29, retcode 3
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 27, retcode 3
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 28, retcode 3
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 28, retcode 3
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 27, retcode 3
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 29, retcode 3
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 29, retcode 3
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 28, retcode 3
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 48, retcode 3
31: 
31: GPU-8:1597422:1609314 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 31, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 31, retcode 3
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 31, retcode 3
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 31, retcode 3
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 31, retcode 3
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 31, retcode 3
30: 
30: GPU-8:1597500:1609315 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
30: 
30: GPU-8:1597500:1609315 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
30: 
30: GPU-8:1597500:1609315 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 31, retcode 3
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
24: 
24: GPU-8:1597466:1609310 [0] proxy.cc:1521 NCCL WARN [Proxy Service 24] Failed to execute operation Close from rank 30, retcode 3
31: 
31: GPU-8:1597422:1609314 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
31: 
31: GPU-8:1597422:1609314 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 31, retcode 3
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
25: 
25: GPU-8:1597346:1609311 [1] proxy.cc:1521 NCCL WARN [Proxy Service 25] Failed to execute operation Close from rank 30, retcode 3
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
26: 
26: GPU-8:1597464:1609312 [2] proxy.cc:1521 NCCL WARN [Proxy Service 26] Failed to execute operation Close from rank 30, retcode 3
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
27: 
27: GPU-8:1597323:1609317 [3] proxy.cc:1521 NCCL WARN [Proxy Service 27] Failed to execute operation Close from rank 30, retcode 3
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
28: 
28: GPU-8:1597395:1609316 [4] proxy.cc:1521 NCCL WARN [Proxy Service 28] Failed to execute operation Close from rank 30, retcode 3
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
29: 
29: GPU-8:1597297:1609313 [5] proxy.cc:1521 NCCL WARN [Proxy Service 29] Failed to execute operation Close from rank 30, retcode 3
30: 
30: GPU-8:1597500:1609315 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
30: 
30: GPU-8:1597500:1609315 [6] proxy.cc:1521 NCCL WARN [Proxy Service 30] Failed to execute operation Close from rank 30, retcode 3
31: 
31: GPU-8:1597422:1609314 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
31: 
31: GPU-8:1597422:1609314 [7] proxy.cc:1521 NCCL WARN [Proxy Service 31] Failed to execute operation Close from rank 30, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 51, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 51, retcode 3
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 37, retcode 3
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 36, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 37, retcode 3
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 35, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 36, retcode 3
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 35, retcode 3
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 37, retcode 3
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 35, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 35, retcode 3
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 36, retcode 3
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 37, retcode 3
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 37, retcode 3
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 36, retcode 3
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 35, retcode 3
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 35, retcode 3
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 36, retcode 3
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 37, retcode 3
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 36, retcode 3
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 56, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 43, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 43, retcode 3
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 41, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 41, retcode 3
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 50, retcode 3
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 50, retcode 3
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 50, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 50, retcode 3
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 49, retcode 3
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 49, retcode 3
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 49, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 49, retcode 3
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 40, retcode 3
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 40, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 0, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 40, retcode 3
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 63, retcode 3
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 59, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 63, retcode 3
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 63, retcode 3
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 58, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 63, retcode 3
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 59, retcode 3
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 59, retcode 3
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 60, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 59, retcode 3
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 58, retcode 3
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 60, retcode 3
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 58, retcode 3
39: 
39: GPU-234:1909941:1911721 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 63, retcode 3
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 3, res=3, closed=0
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 59, retcode 3
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 60, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 60, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 58, retcode 3
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 58, retcode 3
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 60, retcode 3
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 39, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 39, retcode 3
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 39, retcode 3
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 39, retcode 3
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 39, retcode 3
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 39, retcode 3
38: 
38: GPU-234:1909957:1911719 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
38: 
38: GPU-234:1909957:1911719 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
38: 
38: GPU-234:1909957:1911719 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 39, retcode 3
39: 
39: GPU-234:1909941:1911721 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
39: 
39: GPU-234:1909941:1911721 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 39, retcode 3
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
32: 
32: GPU-234:1909925:1911711 [0] proxy.cc:1521 NCCL WARN [Proxy Service 32] Failed to execute operation Close from rank 38, retcode 3
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
33: 
33: GPU-234:1909981:1911713 [1] proxy.cc:1521 NCCL WARN [Proxy Service 33] Failed to execute operation Close from rank 38, retcode 3
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
34: 
34: GPU-234:1910019:1911712 [2] proxy.cc:1521 NCCL WARN [Proxy Service 34] Failed to execute operation Close from rank 38, retcode 3
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
35: 
35: GPU-234:1909980:1911717 [3] proxy.cc:1521 NCCL WARN [Proxy Service 35] Failed to execute operation Close from rank 38, retcode 3
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
36: 
36: GPU-234:1910020:1911718 [4] proxy.cc:1521 NCCL WARN [Proxy Service 36] Failed to execute operation Close from rank 38, retcode 3
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
37: 
37: GPU-234:1910021:1911720 [5] proxy.cc:1521 NCCL WARN [Proxy Service 37] Failed to execute operation Close from rank 38, retcode 3
38: 
38: GPU-234:1909957:1911719 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
38: 
38: GPU-234:1909957:1911719 [6] proxy.cc:1521 NCCL WARN [Proxy Service 38] Failed to execute operation Close from rank 38, retcode 3
39: 
39: GPU-234:1909941:1911721 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
39: 
39: GPU-234:1909941:1911721 [7] proxy.cc:1521 NCCL WARN [Proxy Service 39] Failed to execute operation Close from rank 38, retcode 3
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 47, retcode 3
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 47, retcode 3
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 44, retcode 3
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 45, retcode 3
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 44, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 47, retcode 3
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 46, retcode 3
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 46, retcode 3
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
42: 
42: GPU-245:1885846:1887987 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
42: 
42: GPU-245:1885846:1887987 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 45, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 44, retcode 3
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 44, retcode 3
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 45, retcode 3
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 47, retcode 3
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 47, retcode 3
42: 
42: GPU-245:1885846:1887987 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 46, retcode 3
42: 
42: GPU-245:1885846:1887987 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 46, retcode 3
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 46, retcode 3
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 44, retcode 3
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 47, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 45, retcode 3
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 45, retcode 3
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 45, retcode 3
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 44, retcode 3
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 46, retcode 3
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 47, retcode 3
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 44, retcode 3
42: 
42: GPU-245:1885846:1887987 [2] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 46, retcode 3
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 45, retcode 3
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 46, retcode 3
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 45, retcode 3
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
40: 
40: GPU-245:1885892:1887988 [0] proxy.cc:1521 NCCL WARN [Proxy Service 40] Failed to execute operation Close from rank 42, retcode 3
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
41: 
41: GPU-245:1885883:1887989 [1] proxy.cc:1521 NCCL WARN [Proxy Service 41] Failed to execute operation Close from rank 42, retcode 3
42: 
42: GPU-245:1885846:1887987 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
42: 
42: GPU-245:1885846:1887987 [2] proxy.cc:1521 NCCL WARN [Proxy Service 42] Failed to execute operation Close from rank 42, retcode 3
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
43: 
43: GPU-245:1885821:1887994 [3] proxy.cc:1521 NCCL WARN [Proxy Service 43] Failed to execute operation Close from rank 42, retcode 3
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
44: 
44: GPU-245:1885798:1887991 [4] proxy.cc:1521 NCCL WARN [Proxy Service 44] Failed to execute operation Close from rank 42, retcode 3
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
45: 
45: GPU-245:1885822:1887992 [5] proxy.cc:1521 NCCL WARN [Proxy Service 45] Failed to execute operation Close from rank 42, retcode 3
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
46: 
46: GPU-245:1885881:1887993 [6] proxy.cc:1521 NCCL WARN [Proxy Service 46] Failed to execute operation Close from rank 42, retcode 3
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 2, res=3, closed=0
47: 
47: GPU-245:1885886:1887990 [7] proxy.cc:1521 NCCL WARN [Proxy Service 47] Failed to execute operation Close from rank 42, retcode 3
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 52, retcode 3
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 52, retcode 3
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 52, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 52, retcode 3
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 4, res=3, closed=0
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 52, retcode 3
57: 
57: GPU-332:1888393:1890172 [1] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 57, retcode 3
57: 
57: GPU-332:1888393:1890172 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
57: 
57: GPU-332:1888393:1890172 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 57, retcode 3
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 57, retcode 3
61: 
61: GPU-332:1888417:1890160 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 57, retcode 3
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 61, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 57, retcode 3
57: 
57: GPU-332:1888393:1890172 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
57: 
57: GPU-332:1888393:1890172 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 61, retcode 3
61: 
61: GPU-332:1888417:1890160 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
61: 
61: GPU-332:1888417:1890160 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 57, retcode 3
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 61, retcode 3
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 61, retcode 3
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 1, res=3, closed=0
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 57, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 61, retcode 3
61: 
61: GPU-332:1888417:1890160 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
61: 
61: GPU-332:1888417:1890160 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 61, retcode 3
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 61, retcode 3
54: 
54: GPU-792:1875453:1877210 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 54, retcode 3
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 54, retcode 3
53: 
53: GPU-792:1875489:1877209 [5] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 54, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 54, retcode 3
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 53, retcode 3
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 54, retcode 3
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 53, retcode 3
53: 
53: GPU-792:1875489:1877209 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
53: 
53: GPU-792:1875489:1877209 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 54, retcode 3
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 53, retcode 3
54: 
54: GPU-792:1875453:1877210 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
54: 
54: GPU-792:1875453:1877210 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 54, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 53, retcode 3
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 53, retcode 3
53: 
53: GPU-792:1875489:1877209 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
53: 
53: GPU-792:1875489:1877209 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 53, retcode 3
54: 
54: GPU-792:1875453:1877210 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 5, res=3, closed=0
54: 
54: GPU-792:1875453:1877210 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 53, retcode 3
55: 
55: GPU-792:1875529:1877208 [7] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
48: 
48: GPU-792:1875421:1877206 [0] proxy.cc:1521 NCCL WARN [Proxy Service 48] Failed to execute operation Close from rank 55, retcode 3
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
49: 
49: GPU-792:1875469:1877205 [1] proxy.cc:1521 NCCL WARN [Proxy Service 49] Failed to execute operation Close from rank 55, retcode 3
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
50: 
50: GPU-792:1875501:1877207 [2] proxy.cc:1521 NCCL WARN [Proxy Service 50] Failed to execute operation Close from rank 55, retcode 3
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
51: 
51: GPU-792:1875437:1877212 [3] proxy.cc:1521 NCCL WARN [Proxy Service 51] Failed to execute operation Close from rank 55, retcode 3
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
52: 
52: GPU-792:1875524:1877211 [4] proxy.cc:1521 NCCL WARN [Proxy Service 52] Failed to execute operation Close from rank 55, retcode 3
53: 
53: GPU-792:1875489:1877209 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
53: 
53: GPU-792:1875489:1877209 [5] proxy.cc:1521 NCCL WARN [Proxy Service 53] Failed to execute operation Close from rank 55, retcode 3
54: 
54: GPU-792:1875453:1877210 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
54: 
54: GPU-792:1875453:1877210 [6] proxy.cc:1521 NCCL WARN [Proxy Service 54] Failed to execute operation Close from rank 55, retcode 3
55: 
55: GPU-792:1875529:1877208 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 7, res=3, closed=0
55: 
55: GPU-792:1875529:1877208 [7] proxy.cc:1521 NCCL WARN [Proxy Service 55] Failed to execute operation Close from rank 55, retcode 3
62: 
62: GPU-332:1888416:1890164 [6] proxy.cc:1458 NCCL WARN [Service thread] Accept failed Resource temporarily unavailable
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
56: 
56: GPU-332:1888441:1890171 [0] proxy.cc:1521 NCCL WARN [Proxy Service 56] Failed to execute operation Close from rank 62, retcode 3
57: 
57: GPU-332:1888393:1890172 [1] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
57: 
57: GPU-332:1888393:1890172 [1] proxy.cc:1521 NCCL WARN [Proxy Service 57] Failed to execute operation Close from rank 62, retcode 3
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
58: 
58: GPU-332:1888377:1890161 [2] proxy.cc:1521 NCCL WARN [Proxy Service 58] Failed to execute operation Close from rank 62, retcode 3
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
59: 
59: GPU-332:1888467:1890162 [3] proxy.cc:1521 NCCL WARN [Proxy Service 59] Failed to execute operation Close from rank 62, retcode 3
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
60: 
60: GPU-332:1888481:1890163 [4] proxy.cc:1521 NCCL WARN [Proxy Service 60] Failed to execute operation Close from rank 62, retcode 3
61: 
61: GPU-332:1888417:1890160 [5] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
61: 
61: GPU-332:1888417:1890160 [5] proxy.cc:1521 NCCL WARN [Proxy Service 61] Failed to execute operation Close from rank 62, retcode 3
62: 
62: GPU-332:1888416:1890164 [6] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
62: 
62: GPU-332:1888416:1890164 [6] proxy.cc:1521 NCCL WARN [Proxy Service 62] Failed to execute operation Close from rank 62, retcode 3
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1497 NCCL WARN [Service thread] Could not receive type from localRank 6, res=3, closed=0
63: 
63: GPU-332:1888470:1890159 [7] proxy.cc:1521 NCCL WARN [Proxy Service 63] Failed to execute operation Close from rank 62, retcode 3
 0: ENDING TIMING RUN AT 2024-10-11 07:33:09 PM
 0: RESULT,LLM_FINETUNING,,691,nvidia,2024-10-11 07:21:38 PM
++ date +%s
+ echo 'RUNANDTIME_STOP 1728675199'
RUNANDTIME_STOP 1728675199
+ set -e
